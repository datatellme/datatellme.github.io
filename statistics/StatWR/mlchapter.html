<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>9 Predictive modelling and machine learning | R 통계분석</title>
  <meta name="description" content="9 Predictive modelling and machine learning | R 통계분석" />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="9 Predictive modelling and machine learning | R 통계분석" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="9 Predictive modelling and machine learning | R 통계분석" />
  
  
  

<meta name="author" content="psnam" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="regression.html"/>
<link rel="next" href="advancedchapter.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="preamble.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="https://datatellme.github.io">Data Tell Me Something</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>안녕하세요</a></li>
<li class="chapter" data-level="1" data-path="Introduction.html"><a href="Introduction.html"><i class="fa fa-check"></i><b>1</b> 들어가며</a>
<ul>
<li class="chapter" data-level="1.1" data-path="Introduction.html"><a href="Introduction.html#whatR"><i class="fa fa-check"></i><b>1.1</b> R이란?</a></li>
<li class="chapter" data-level="1.2" data-path="Introduction.html"><a href="Introduction.html#Rinstallation"><i class="fa fa-check"></i><b>1.2</b> R 설치하기</a></li>
<li class="chapter" data-level="1.3" data-path="Introduction.html"><a href="Introduction.html#RStudioinstallation"><i class="fa fa-check"></i><b>1.3</b> RStudio 설치하기</a></li>
<li class="chapter" data-level="1.4" data-path="Introduction.html"><a href="Introduction.html#LaTeXinstallation"><i class="fa fa-check"></i><b>1.4</b> 레이텍 설치하기</a></li>
<li class="chapter" data-level="1.5" data-path="Introduction.html"><a href="Introduction.html#packageinstallation"><i class="fa fa-check"></i><b>1.5</b> Packages 설치하기</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="thebasics.html"><a href="thebasics.html"><i class="fa fa-check"></i><b>2</b> 기본</a>
<ul>
<li class="chapter" data-level="2.1" data-path="thebasics.html"><a href="thebasics.html#rstudio-살펴보기"><i class="fa fa-check"></i><b>2.1</b> RStudio 살펴보기</a></li>
<li class="chapter" data-level="2.2" data-path="thebasics.html"><a href="thebasics.html#runningcode"><i class="fa fa-check"></i><b>2.2</b> R 코드 실행하기</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="thebasics.html"><a href="thebasics.html#r-스크립트"><i class="fa fa-check"></i><b>2.2.1</b> R 스크립트</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="thebasics.html"><a href="thebasics.html#varsandfuncs"><i class="fa fa-check"></i><b>2.3</b> 변수와 함수</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="thebasics.html"><a href="thebasics.html#데이터-저장"><i class="fa fa-check"></i><b>2.3.1</b> 데이터 저장</a></li>
<li class="chapter" data-level="2.3.2" data-path="thebasics.html"><a href="thebasics.html#변수-이름"><i class="fa fa-check"></i><b>2.3.2</b> 변수 이름</a></li>
<li class="chapter" data-level="2.3.3" data-path="thebasics.html"><a href="thebasics.html#벡터와-데이터프레임"><i class="fa fa-check"></i><b>2.3.3</b> 벡터와 데이터프레임</a></li>
<li class="chapter" data-level="2.3.4" data-path="thebasics.html"><a href="thebasics.html#함수"><i class="fa fa-check"></i><b>2.3.4</b> 함수</a></li>
<li class="chapter" data-level="2.3.5" data-path="thebasics.html"><a href="thebasics.html#maths"><i class="fa fa-check"></i><b>2.3.5</b> 수학연산</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="thebasics.html"><a href="thebasics.html#descstats"><i class="fa fa-check"></i><b>2.4</b> 기술통계</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="thebasics.html"><a href="thebasics.html#숫자-데이터"><i class="fa fa-check"></i><b>2.4.1</b> 숫자 데이터</a></li>
<li class="chapter" data-level="2.4.2" data-path="thebasics.html"><a href="thebasics.html#catdata1"><i class="fa fa-check"></i><b>2.4.2</b> 명목 데이터</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="thebasics.html"><a href="thebasics.html#그래프와-숫자-데이터"><i class="fa fa-check"></i><b>2.5</b> 그래프와 숫자 데이터</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="thebasics.html"><a href="thebasics.html#나의-그래프"><i class="fa fa-check"></i><b>2.5.1</b> 나의 그래프</a></li>
<li class="chapter" data-level="2.5.2" data-path="thebasics.html"><a href="thebasics.html#색과-형태-및-축"><i class="fa fa-check"></i><b>2.5.2</b> 색과 형태 및 축</a></li>
<li class="chapter" data-level="2.5.3" data-path="thebasics.html"><a href="thebasics.html#축의-한계와-스케일"><i class="fa fa-check"></i><b>2.5.3</b> 축의 한계와 스케일</a></li>
<li class="chapter" data-level="2.5.4" data-path="thebasics.html"><a href="thebasics.html#comparinggroups"><i class="fa fa-check"></i><b>2.5.4</b> 그룹 비교</a></li>
<li class="chapter" data-level="2.5.5" data-path="thebasics.html"><a href="thebasics.html#박스-그래풀럿"><i class="fa fa-check"></i><b>2.5.5</b> 박스 그래풀럿</a></li>
<li class="chapter" data-level="2.5.6" data-path="thebasics.html"><a href="thebasics.html#히스토그램"><i class="fa fa-check"></i><b>2.5.6</b> 히스토그램</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="thebasics.html"><a href="thebasics.html#catdata2"><i class="fa fa-check"></i><b>2.6</b> 그래프와 명목 데인터</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="thebasics.html"><a href="thebasics.html#막대-그래프"><i class="fa fa-check"></i><b>2.6.1</b> 막대 그래프</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="thebasics.html"><a href="thebasics.html#그래프-저장"><i class="fa fa-check"></i><b>2.7</b> 그래프 저장</a></li>
<li class="chapter" data-level="2.8" data-path="thebasics.html"><a href="thebasics.html#troubleshooting"><i class="fa fa-check"></i><b>2.8</b> 오류 문제 해결</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="datachapter.html"><a href="datachapter.html"><i class="fa fa-check"></i><b>3</b> Transforming, summarising, and analysing data</a>
<ul>
<li class="chapter" data-level="3.1" data-path="datachapter.html"><a href="datachapter.html#data-frames-and-data-types"><i class="fa fa-check"></i><b>3.1</b> Data frames and data types</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="datachapter.html"><a href="datachapter.html#types-and-structures"><i class="fa fa-check"></i><b>3.1.1</b> Types and structures</a></li>
<li class="chapter" data-level="3.1.2" data-path="datachapter.html"><a href="datachapter.html#typesoftables"><i class="fa fa-check"></i><b>3.1.2</b> Types of tables</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="datachapter.html"><a href="datachapter.html#findingpoints"><i class="fa fa-check"></i><b>3.2</b> Vectors in data frames</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="datachapter.html"><a href="datachapter.html#accessingelements"><i class="fa fa-check"></i><b>3.2.1</b> Accessing vectors and elements</a></li>
<li class="chapter" data-level="3.2.2" data-path="datachapter.html"><a href="datachapter.html#use-your-dollars"><i class="fa fa-check"></i><b>3.2.2</b> Use your dollars</a></li>
<li class="chapter" data-level="3.2.3" data-path="datachapter.html"><a href="datachapter.html#conditionsintro"><i class="fa fa-check"></i><b>3.2.3</b> Using conditions</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="datachapter.html"><a href="datachapter.html#paths"><i class="fa fa-check"></i><b>3.3</b> Importing data</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="datachapter.html"><a href="datachapter.html#importing-csv-files"><i class="fa fa-check"></i><b>3.3.1</b> Importing csv files</a></li>
<li class="chapter" data-level="3.3.2" data-path="datachapter.html"><a href="datachapter.html#file-paths"><i class="fa fa-check"></i><b>3.3.2</b> File paths</a></li>
<li class="chapter" data-level="3.3.3" data-path="datachapter.html"><a href="datachapter.html#importing-excel-files"><i class="fa fa-check"></i><b>3.3.3</b> Importing Excel files</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="datachapter.html"><a href="datachapter.html#saving-and-exporting-your-data"><i class="fa fa-check"></i><b>3.4</b> Saving and exporting your data</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="datachapter.html"><a href="datachapter.html#exporting-data"><i class="fa fa-check"></i><b>3.4.1</b> Exporting data</a></li>
<li class="chapter" data-level="3.4.2" data-path="datachapter.html"><a href="datachapter.html#saving-and-loading-r-data"><i class="fa fa-check"></i><b>3.4.2</b> Saving and loading R data</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="datachapter.html"><a href="datachapter.html#rstudio-projects"><i class="fa fa-check"></i><b>3.5</b> RStudio projects</a></li>
<li class="chapter" data-level="3.6" data-path="datachapter.html"><a href="datachapter.html#firstttest"><i class="fa fa-check"></i><b>3.6</b> Running a t-test</a></li>
<li class="chapter" data-level="3.7" data-path="datachapter.html"><a href="datachapter.html#firstlm"><i class="fa fa-check"></i><b>3.7</b> Fitting a linear regression model</a></li>
<li class="chapter" data-level="3.8" data-path="datachapter.html"><a href="datachapter.html#grouped"><i class="fa fa-check"></i><b>3.8</b> Grouped summaries</a></li>
<li class="chapter" data-level="3.9" data-path="datachapter.html"><a href="datachapter.html#pipes"><i class="fa fa-check"></i><b>3.9</b> Using <code>%&gt;%</code> pipes</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="datachapter.html"><a href="datachapter.html#ceci-nest-pas-une-pipe"><i class="fa fa-check"></i><b>3.9.1</b> <em>Ceci n’est pas une pipe</em></a></li>
<li class="chapter" data-level="3.9.2" data-path="datachapter.html"><a href="datachapter.html#aliases-and-placeholders"><i class="fa fa-check"></i><b>3.9.2</b> Aliases and placeholders</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="datachapter.html"><a href="datachapter.html#flavours-of-r-base-and-tidyverse"><i class="fa fa-check"></i><b>3.10</b> Flavours of R: base and tidyverse</a></li>
<li class="chapter" data-level="3.11" data-path="datachapter.html"><a href="datachapter.html#ethicalguidelines"><i class="fa fa-check"></i><b>3.11</b> Ethics and good statistical practice</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="eda.html"><a href="eda.html"><i class="fa fa-check"></i><b>4</b> Exploratory data analysis and unsupervised learning</a>
<ul>
<li class="chapter" data-level="4.1" data-path="eda.html"><a href="eda.html#rmarkdown"><i class="fa fa-check"></i><b>4.1</b> Reports with R Markdown</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="eda.html"><a href="eda.html#a-first-example"><i class="fa fa-check"></i><b>4.1.1</b> A first example</a></li>
<li class="chapter" data-level="4.1.2" data-path="eda.html"><a href="eda.html#formatting-text"><i class="fa fa-check"></i><b>4.1.2</b> Formatting text</a></li>
<li class="chapter" data-level="4.1.3" data-path="eda.html"><a href="eda.html#lists-tables-and-images"><i class="fa fa-check"></i><b>4.1.3</b> Lists, tables, and images</a></li>
<li class="chapter" data-level="4.1.4" data-path="eda.html"><a href="eda.html#code-chunks"><i class="fa fa-check"></i><b>4.1.4</b> Code chunks</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="eda.html"><a href="eda.html#themes"><i class="fa fa-check"></i><b>4.2</b> Customising <code>ggplot2</code> plots</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="eda.html"><a href="eda.html#using-themes"><i class="fa fa-check"></i><b>4.2.1</b> Using themes</a></li>
<li class="chapter" data-level="4.2.2" data-path="eda.html"><a href="eda.html#colourpalettes"><i class="fa fa-check"></i><b>4.2.2</b> Colour palettes</a></li>
<li class="chapter" data-level="4.2.3" data-path="eda.html"><a href="eda.html#theme-settings"><i class="fa fa-check"></i><b>4.2.3</b> Theme settings</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="eda.html"><a href="eda.html#exploring-distributions"><i class="fa fa-check"></i><b>4.3</b> Exploring distributions</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="eda.html"><a href="eda.html#density-plots-and-frequency-polygons"><i class="fa fa-check"></i><b>4.3.1</b> Density plots and frequency polygons</a></li>
<li class="chapter" data-level="4.3.2" data-path="eda.html"><a href="eda.html#asking-questions"><i class="fa fa-check"></i><b>4.3.2</b> Asking questions</a></li>
<li class="chapter" data-level="4.3.3" data-path="eda.html"><a href="eda.html#violin-plots"><i class="fa fa-check"></i><b>4.3.3</b> Violin plots</a></li>
<li class="chapter" data-level="4.3.4" data-path="eda.html"><a href="eda.html#patchwork"><i class="fa fa-check"></i><b>4.3.4</b> Combine multiple plots into a single graphic</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="eda.html"><a href="eda.html#outliers-and-missing-data"><i class="fa fa-check"></i><b>4.4</b> Outliers and missing data</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="eda.html"><a href="eda.html#detecting-outliers"><i class="fa fa-check"></i><b>4.4.1</b> Detecting outliers</a></li>
<li class="chapter" data-level="4.4.2" data-path="eda.html"><a href="eda.html#labelling-outliers"><i class="fa fa-check"></i><b>4.4.2</b> Labelling outliers</a></li>
<li class="chapter" data-level="4.4.3" data-path="eda.html"><a href="eda.html#missing-data"><i class="fa fa-check"></i><b>4.4.3</b> Missing data</a></li>
<li class="chapter" data-level="4.4.4" data-path="eda.html"><a href="eda.html#exploring-data"><i class="fa fa-check"></i><b>4.4.4</b> Exploring data</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="eda.html"><a href="eda.html#trends-in-scatterplots"><i class="fa fa-check"></i><b>4.5</b> Trends in scatterplots</a></li>
<li class="chapter" data-level="4.6" data-path="eda.html"><a href="eda.html#tsplots"><i class="fa fa-check"></i><b>4.6</b> Exploring time series</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="eda.html"><a href="eda.html#annotations-and-reference-lines"><i class="fa fa-check"></i><b>4.6.1</b> Annotations and reference lines</a></li>
<li class="chapter" data-level="4.6.2" data-path="eda.html"><a href="eda.html#longitudinal-data"><i class="fa fa-check"></i><b>4.6.2</b> Longitudinal data</a></li>
<li class="chapter" data-level="4.6.3" data-path="eda.html"><a href="eda.html#path-plots"><i class="fa fa-check"></i><b>4.6.3</b> Path plots</a></li>
<li class="chapter" data-level="4.6.4" data-path="eda.html"><a href="eda.html#spaghetti-plots"><i class="fa fa-check"></i><b>4.6.4</b> Spaghetti plots</a></li>
<li class="chapter" data-level="4.6.5" data-path="eda.html"><a href="eda.html#stlsec"><i class="fa fa-check"></i><b>4.6.5</b> Seasonal plots and decompositions</a></li>
<li class="chapter" data-level="4.6.6" data-path="eda.html"><a href="eda.html#detecting-changepoints"><i class="fa fa-check"></i><b>4.6.6</b> Detecting changepoints</a></li>
<li class="chapter" data-level="4.6.7" data-path="eda.html"><a href="eda.html#interactivets"><i class="fa fa-check"></i><b>4.6.7</b> Interactive time series plots</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="eda.html"><a href="eda.html#polar"><i class="fa fa-check"></i><b>4.7</b> Using polar coordinates</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="eda.html"><a href="eda.html#visualising-periodic-data"><i class="fa fa-check"></i><b>4.7.1</b> Visualising periodic data</a></li>
<li class="chapter" data-level="4.7.2" data-path="eda.html"><a href="eda.html#pie-charts"><i class="fa fa-check"></i><b>4.7.2</b> Pie charts</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="eda.html"><a href="eda.html#visualising-multiple-variables"><i class="fa fa-check"></i><b>4.8</b> Visualising multiple variables</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="eda.html"><a href="eda.html#scatterplotmatrix"><i class="fa fa-check"></i><b>4.8.1</b> Scatterplot matrices</a></li>
<li class="chapter" data-level="4.8.2" data-path="eda.html"><a href="eda.html#d-scatterplots"><i class="fa fa-check"></i><b>4.8.2</b> 3D scatterplots</a></li>
<li class="chapter" data-level="4.8.3" data-path="eda.html"><a href="eda.html#correlograms"><i class="fa fa-check"></i><b>4.8.3</b> Correlograms</a></li>
<li class="chapter" data-level="4.8.4" data-path="eda.html"><a href="eda.html#adding-more-variables-to-scatterplots"><i class="fa fa-check"></i><b>4.8.4</b> Adding more variables to scatterplots</a></li>
<li class="chapter" data-level="4.8.5" data-path="eda.html"><a href="eda.html#overplotting"><i class="fa fa-check"></i><b>4.8.5</b> Overplotting</a></li>
<li class="chapter" data-level="4.8.6" data-path="eda.html"><a href="eda.html#categorical-data"><i class="fa fa-check"></i><b>4.8.6</b> Categorical data</a></li>
<li class="chapter" data-level="4.8.7" data-path="eda.html"><a href="eda.html#putting-it-all-together"><i class="fa fa-check"></i><b>4.8.7</b> Putting it all together</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="eda.html"><a href="eda.html#pca"><i class="fa fa-check"></i><b>4.9</b> Principal component analysis</a></li>
<li class="chapter" data-level="4.10" data-path="eda.html"><a href="eda.html#clustering"><i class="fa fa-check"></i><b>4.10</b> Cluster analysis</a>
<ul>
<li class="chapter" data-level="4.10.1" data-path="eda.html"><a href="eda.html#hierarchical-clustering"><i class="fa fa-check"></i><b>4.10.1</b> Hierarchical clustering</a></li>
<li class="chapter" data-level="4.10.2" data-path="eda.html"><a href="eda.html#heatmaps-and-clustering-variables"><i class="fa fa-check"></i><b>4.10.2</b> Heatmaps and clustering variables</a></li>
<li class="chapter" data-level="4.10.3" data-path="eda.html"><a href="eda.html#centroid"><i class="fa fa-check"></i><b>4.10.3</b> Centroid-based clustering</a></li>
<li class="chapter" data-level="4.10.4" data-path="eda.html"><a href="eda.html#fuzzy-clustering"><i class="fa fa-check"></i><b>4.10.4</b> Fuzzy clustering</a></li>
<li class="chapter" data-level="4.10.5" data-path="eda.html"><a href="eda.html#modelbasedclustering"><i class="fa fa-check"></i><b>4.10.5</b> Model-based clustering</a></li>
<li class="chapter" data-level="4.10.6" data-path="eda.html"><a href="eda.html#comparing-clusters"><i class="fa fa-check"></i><b>4.10.6</b> Comparing clusters</a></li>
</ul></li>
<li class="chapter" data-level="4.11" data-path="eda.html"><a href="eda.html#exploratory-factor-analysis"><i class="fa fa-check"></i><b>4.11</b> Exploratory factor analysis</a>
<ul>
<li class="chapter" data-level="4.11.1" data-path="eda.html"><a href="eda.html#factor-analysis"><i class="fa fa-check"></i><b>4.11.1</b> Factor analysis</a></li>
<li class="chapter" data-level="4.11.2" data-path="eda.html"><a href="eda.html#lca"><i class="fa fa-check"></i><b>4.11.2</b> Latent class analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="messychapter.html"><a href="messychapter.html"><i class="fa fa-check"></i><b>5</b> Dealing with messy data</a>
<ul>
<li class="chapter" data-level="5.1" data-path="messychapter.html"><a href="messychapter.html#coercion"><i class="fa fa-check"></i><b>5.1</b> Changing data types</a></li>
<li class="chapter" data-level="5.2" data-path="messychapter.html"><a href="messychapter.html#lists2"><i class="fa fa-check"></i><b>5.2</b> Working with lists</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="messychapter.html"><a href="messychapter.html#splitvector"><i class="fa fa-check"></i><b>5.2.1</b> Splitting vectors into lists</a></li>
<li class="chapter" data-level="5.2.2" data-path="messychapter.html"><a href="messychapter.html#collapsing-lists-into-vectors"><i class="fa fa-check"></i><b>5.2.2</b> Collapsing lists into vectors</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="messychapter.html"><a href="messychapter.html#working-with-numbers"><i class="fa fa-check"></i><b>5.3</b> Working with numbers</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="messychapter.html"><a href="messychapter.html#rounding-numbers"><i class="fa fa-check"></i><b>5.3.1</b> Rounding numbers</a></li>
<li class="chapter" data-level="5.3.2" data-path="messychapter.html"><a href="messychapter.html#sums-and-means-in-data-frames"><i class="fa fa-check"></i><b>5.3.2</b> Sums and means in data frames</a></li>
<li class="chapter" data-level="5.3.3" data-path="messychapter.html"><a href="messychapter.html#rle"><i class="fa fa-check"></i><b>5.3.3</b> Summaries of series of numbers</a></li>
<li class="chapter" data-level="5.3.4" data-path="messychapter.html"><a href="messychapter.html#scientific-notation-1e-03"><i class="fa fa-check"></i><b>5.3.4</b> Scientific notation <code>1e-03</code></a></li>
<li class="chapter" data-level="5.3.5" data-path="messychapter.html"><a href="messychapter.html#floatingpoints"><i class="fa fa-check"></i><b>5.3.5</b> Floating point arithmetics</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="messychapter.html"><a href="messychapter.html#factors"><i class="fa fa-check"></i><b>5.4</b> Working with factors</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="messychapter.html"><a href="messychapter.html#creating-factors"><i class="fa fa-check"></i><b>5.4.1</b> Creating factors</a></li>
<li class="chapter" data-level="5.4.2" data-path="messychapter.html"><a href="messychapter.html#factorlevels"><i class="fa fa-check"></i><b>5.4.2</b> Changing factor levels</a></li>
<li class="chapter" data-level="5.4.3" data-path="messychapter.html"><a href="messychapter.html#changing-the-order-of-levels"><i class="fa fa-check"></i><b>5.4.3</b> Changing the order of levels</a></li>
<li class="chapter" data-level="5.4.4" data-path="messychapter.html"><a href="messychapter.html#combining-levels"><i class="fa fa-check"></i><b>5.4.4</b> Combining levels</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="messychapter.html"><a href="messychapter.html#strings"><i class="fa fa-check"></i><b>5.5</b> Working with strings</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="messychapter.html"><a href="messychapter.html#concatenating-strings"><i class="fa fa-check"></i><b>5.5.1</b> Concatenating strings</a></li>
<li class="chapter" data-level="5.5.2" data-path="messychapter.html"><a href="messychapter.html#changing-case"><i class="fa fa-check"></i><b>5.5.2</b> Changing case</a></li>
<li class="chapter" data-level="5.5.3" data-path="messychapter.html"><a href="messychapter.html#regexp"><i class="fa fa-check"></i><b>5.5.3</b> Finding patterns using regular expressions</a></li>
<li class="chapter" data-level="5.5.4" data-path="messychapter.html"><a href="messychapter.html#sub"><i class="fa fa-check"></i><b>5.5.4</b> Substitution</a></li>
<li class="chapter" data-level="5.5.5" data-path="messychapter.html"><a href="messychapter.html#splitting-strings"><i class="fa fa-check"></i><b>5.5.5</b> Splitting strings</a></li>
<li class="chapter" data-level="5.5.6" data-path="messychapter.html"><a href="messychapter.html#variable-names"><i class="fa fa-check"></i><b>5.5.6</b> Variable names</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="messychapter.html"><a href="messychapter.html#datetime"><i class="fa fa-check"></i><b>5.6</b> Working with dates and times</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="messychapter.html"><a href="messychapter.html#date-formats"><i class="fa fa-check"></i><b>5.6.1</b> Date formats</a></li>
<li class="chapter" data-level="5.6.2" data-path="messychapter.html"><a href="messychapter.html#plotting-with-dates"><i class="fa fa-check"></i><b>5.6.2</b> Plotting with dates</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="messychapter.html"><a href="messychapter.html#data-manipulation-with-data.table-dplyr-and-tidyr"><i class="fa fa-check"></i><b>5.7</b> Data manipulation with <code>data.table</code>, <code>dplyr</code>, and <code>tidyr</code></a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="messychapter.html"><a href="messychapter.html#dtbasics"><i class="fa fa-check"></i><b>5.7.1</b> <code>data.table</code> and tidyverse syntax basics</a></li>
<li class="chapter" data-level="5.7.2" data-path="messychapter.html"><a href="messychapter.html#modifyvar"><i class="fa fa-check"></i><b>5.7.2</b> Modifying a variable</a></li>
<li class="chapter" data-level="5.7.3" data-path="messychapter.html"><a href="messychapter.html#computing-a-new-variable-based-on-existing-variables"><i class="fa fa-check"></i><b>5.7.3</b> Computing a new variable based on existing variables</a></li>
<li class="chapter" data-level="5.7.4" data-path="messychapter.html"><a href="messychapter.html#renaming-a-variable"><i class="fa fa-check"></i><b>5.7.4</b> Renaming a variable</a></li>
<li class="chapter" data-level="5.7.5" data-path="messychapter.html"><a href="messychapter.html#removing-a-variable"><i class="fa fa-check"></i><b>5.7.5</b> Removing a variable</a></li>
<li class="chapter" data-level="5.7.6" data-path="messychapter.html"><a href="messychapter.html#recodedplyr"><i class="fa fa-check"></i><b>5.7.6</b> Recoding <code>factor</code> levels</a></li>
<li class="chapter" data-level="5.7.7" data-path="messychapter.html"><a href="messychapter.html#grouped2"><i class="fa fa-check"></i><b>5.7.7</b> Grouped summaries</a></li>
<li class="chapter" data-level="5.7.8" data-path="messychapter.html"><a href="messychapter.html#fillNA"><i class="fa fa-check"></i><b>5.7.8</b> Filling in missing values</a></li>
<li class="chapter" data-level="5.7.9" data-path="messychapter.html"><a href="messychapter.html#chaining-commands-together"><i class="fa fa-check"></i><b>5.7.9</b> Chaining commands together</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="messychapter.html"><a href="messychapter.html#filtering"><i class="fa fa-check"></i><b>5.8</b> Filtering: select rows</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="messychapter.html"><a href="messychapter.html#filtering-using-row-numbers"><i class="fa fa-check"></i><b>5.8.1</b> Filtering using row numbers</a></li>
<li class="chapter" data-level="5.8.2" data-path="messychapter.html"><a href="messychapter.html#conditions2"><i class="fa fa-check"></i><b>5.8.2</b> Filtering using conditions</a></li>
<li class="chapter" data-level="5.8.3" data-path="messychapter.html"><a href="messychapter.html#selecting-rows-at-random"><i class="fa fa-check"></i><b>5.8.3</b> Selecting rows at random</a></li>
<li class="chapter" data-level="5.8.4" data-path="messychapter.html"><a href="messychapter.html#using-regular-expressions-to-select-rows"><i class="fa fa-check"></i><b>5.8.4</b> Using regular expressions to select rows</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="messychapter.html"><a href="messychapter.html#subsetting-select-columns"><i class="fa fa-check"></i><b>5.9</b> Subsetting: select columns</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="messychapter.html"><a href="messychapter.html#selecting-a-single-column"><i class="fa fa-check"></i><b>5.9.1</b> Selecting a single column</a></li>
<li class="chapter" data-level="5.9.2" data-path="messychapter.html"><a href="messychapter.html#selecting-multiple-columns"><i class="fa fa-check"></i><b>5.9.2</b> Selecting multiple columns</a></li>
<li class="chapter" data-level="5.9.3" data-path="messychapter.html"><a href="messychapter.html#using-regular-expressions-to-select-columns"><i class="fa fa-check"></i><b>5.9.3</b> Using regular expressions to select columns</a></li>
<li class="chapter" data-level="5.9.4" data-path="messychapter.html"><a href="messychapter.html#subsetusingcn"><i class="fa fa-check"></i><b>5.9.4</b> Subsetting using column numbers</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="messychapter.html"><a href="messychapter.html#sorting"><i class="fa fa-check"></i><b>5.10</b> Sorting</a>
<ul>
<li class="chapter" data-level="5.10.1" data-path="messychapter.html"><a href="messychapter.html#changing-the-column-order"><i class="fa fa-check"></i><b>5.10.1</b> Changing the column order</a></li>
<li class="chapter" data-level="5.10.2" data-path="messychapter.html"><a href="messychapter.html#changing-the-row-order"><i class="fa fa-check"></i><b>5.10.2</b> Changing the row order</a></li>
</ul></li>
<li class="chapter" data-level="5.11" data-path="messychapter.html"><a href="messychapter.html#reshaping"><i class="fa fa-check"></i><b>5.11</b> Reshaping data</a>
<ul>
<li class="chapter" data-level="5.11.1" data-path="messychapter.html"><a href="messychapter.html#from-long-to-wide"><i class="fa fa-check"></i><b>5.11.1</b> From long to wide</a></li>
<li class="chapter" data-level="5.11.2" data-path="messychapter.html"><a href="messychapter.html#from-wide-to-long"><i class="fa fa-check"></i><b>5.11.2</b> From wide to long</a></li>
<li class="chapter" data-level="5.11.3" data-path="messychapter.html"><a href="messychapter.html#splittingcolumns"><i class="fa fa-check"></i><b>5.11.3</b> Splitting columns</a></li>
<li class="chapter" data-level="5.11.4" data-path="messychapter.html"><a href="messychapter.html#merging-columns"><i class="fa fa-check"></i><b>5.11.4</b> Merging columns</a></li>
</ul></li>
<li class="chapter" data-level="5.12" data-path="messychapter.html"><a href="messychapter.html#mergedata"><i class="fa fa-check"></i><b>5.12</b> Merging data from multiple tables</a>
<ul>
<li class="chapter" data-level="5.12.1" data-path="messychapter.html"><a href="messychapter.html#binds"><i class="fa fa-check"></i><b>5.12.1</b> Binds</a></li>
<li class="chapter" data-level="5.12.2" data-path="messychapter.html"><a href="messychapter.html#merging-tables-using-keys"><i class="fa fa-check"></i><b>5.12.2</b> Merging tables using keys</a></li>
<li class="chapter" data-level="5.12.3" data-path="messychapter.html"><a href="messychapter.html#inner-and-outer-joins"><i class="fa fa-check"></i><b>5.12.3</b> Inner and outer joins</a></li>
<li class="chapter" data-level="5.12.4" data-path="messychapter.html"><a href="messychapter.html#semijoins"><i class="fa fa-check"></i><b>5.12.4</b> Semijoins and antijoins</a></li>
</ul></li>
<li class="chapter" data-level="5.13" data-path="messychapter.html"><a href="messychapter.html#scraping-data-from-websites"><i class="fa fa-check"></i><b>5.13</b> Scraping data from websites</a></li>
<li class="chapter" data-level="5.14" data-path="messychapter.html"><a href="messychapter.html#commontasks"><i class="fa fa-check"></i><b>5.14</b> Other commons tasks</a>
<ul>
<li class="chapter" data-level="5.14.1" data-path="messychapter.html"><a href="messychapter.html#deleting-variables"><i class="fa fa-check"></i><b>5.14.1</b> Deleting variables</a></li>
<li class="chapter" data-level="5.14.2" data-path="messychapter.html"><a href="messychapter.html#foreign"><i class="fa fa-check"></i><b>5.14.2</b> Importing data from other statistical packages</a></li>
<li class="chapter" data-level="5.14.3" data-path="messychapter.html"><a href="messychapter.html#importing-data-from-databases"><i class="fa fa-check"></i><b>5.14.3</b> Importing data from databases</a></li>
<li class="chapter" data-level="5.14.4" data-path="messychapter.html"><a href="messychapter.html#importing-data-from-json-files"><i class="fa fa-check"></i><b>5.14.4</b> Importing data from JSON files</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="progchapter.html"><a href="progchapter.html"><i class="fa fa-check"></i><b>6</b> R programming</a>
<ul>
<li class="chapter" data-level="6.1" data-path="progchapter.html"><a href="progchapter.html#functions"><i class="fa fa-check"></i><b>6.1</b> Functions</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="progchapter.html"><a href="progchapter.html#createfunctions"><i class="fa fa-check"></i><b>6.1.1</b> Creating functions</a></li>
<li class="chapter" data-level="6.1.2" data-path="progchapter.html"><a href="progchapter.html#local-and-global-variables"><i class="fa fa-check"></i><b>6.1.2</b> Local and global variables</a></li>
<li class="chapter" data-level="6.1.3" data-path="progchapter.html"><a href="progchapter.html#willfunctionwork"><i class="fa fa-check"></i><b>6.1.3</b> Will your function work?</a></li>
<li class="chapter" data-level="6.1.4" data-path="progchapter.html"><a href="progchapter.html#more-on-arguments"><i class="fa fa-check"></i><b>6.1.4</b> More on arguments</a></li>
<li class="chapter" data-level="6.1.5" data-path="progchapter.html"><a href="progchapter.html#namespaces"><i class="fa fa-check"></i><b>6.1.5</b> Namespaces</a></li>
<li class="chapter" data-level="6.1.6" data-path="progchapter.html"><a href="progchapter.html#sourcing-other-scripts"><i class="fa fa-check"></i><b>6.1.6</b> Sourcing other scripts</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="progchapter.html"><a href="progchapter.html#morepipes"><i class="fa fa-check"></i><b>6.2</b> More on pipes</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="progchapter.html"><a href="progchapter.html#ce-ne-sont-pas-non-plus-des-pipes"><i class="fa fa-check"></i><b>6.2.1</b> <em>Ce ne sont pas non plus des pipes</em></a></li>
<li class="chapter" data-level="6.2.2" data-path="progchapter.html"><a href="progchapter.html#writing-functions-with-pipes"><i class="fa fa-check"></i><b>6.2.2</b> Writing functions with pipes</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="progchapter.html"><a href="progchapter.html#conditions"><i class="fa fa-check"></i><b>6.3</b> Checking conditions</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="progchapter.html"><a href="progchapter.html#if-and-else"><i class="fa fa-check"></i><b>6.3.1</b> <code>if</code> and <code>else</code></a></li>
<li class="chapter" data-level="6.3.2" data-path="progchapter.html"><a href="progchapter.html#section"><i class="fa fa-check"></i><b>6.3.2</b> <code>&amp;</code> &amp; <code>&amp;&amp;</code></a></li>
<li class="chapter" data-level="6.3.3" data-path="progchapter.html"><a href="progchapter.html#ifelse"><i class="fa fa-check"></i><b>6.3.3</b> <code>ifelse</code></a></li>
<li class="chapter" data-level="6.3.4" data-path="progchapter.html"><a href="progchapter.html#switch"><i class="fa fa-check"></i><b>6.3.4</b> <code>switch</code></a></li>
<li class="chapter" data-level="6.3.5" data-path="progchapter.html"><a href="progchapter.html#failing-gracefully"><i class="fa fa-check"></i><b>6.3.5</b> Failing gracefully</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="progchapter.html"><a href="progchapter.html#loopsection"><i class="fa fa-check"></i><b>6.4</b> Iteration using loops</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="progchapter.html"><a href="progchapter.html#forloops"><i class="fa fa-check"></i><b>6.4.1</b> <code>for</code> loops</a></li>
<li class="chapter" data-level="6.4.2" data-path="progchapter.html"><a href="progchapter.html#nestedloops"><i class="fa fa-check"></i><b>6.4.2</b> Loops within loops</a></li>
<li class="chapter" data-level="6.4.3" data-path="progchapter.html"><a href="progchapter.html#beepr"><i class="fa fa-check"></i><b>6.4.3</b> Keeping track of what’s happening</a></li>
<li class="chapter" data-level="6.4.4" data-path="progchapter.html"><a href="progchapter.html#lists"><i class="fa fa-check"></i><b>6.4.4</b> Loops and lists</a></li>
<li class="chapter" data-level="6.4.5" data-path="progchapter.html"><a href="progchapter.html#whileloop"><i class="fa fa-check"></i><b>6.4.5</b> <code>while</code> loops</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="progchapter.html"><a href="progchapter.html#vectorloops"><i class="fa fa-check"></i><b>6.5</b> Iteration using vectorisation and functionals</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="progchapter.html"><a href="progchapter.html#a-first-example-with-apply"><i class="fa fa-check"></i><b>6.5.1</b> A first example with <code>apply</code></a></li>
<li class="chapter" data-level="6.5.2" data-path="progchapter.html"><a href="progchapter.html#variations-on-a-theme"><i class="fa fa-check"></i><b>6.5.2</b> Variations on a theme</a></li>
<li class="chapter" data-level="6.5.3" data-path="progchapter.html"><a href="progchapter.html#purrr"><i class="fa fa-check"></i><b>6.5.3</b> <code>purrr</code></a></li>
<li class="chapter" data-level="6.5.4" data-path="progchapter.html"><a href="progchapter.html#specialised-functions"><i class="fa fa-check"></i><b>6.5.4</b> Specialised functions</a></li>
<li class="chapter" data-level="6.5.5" data-path="progchapter.html"><a href="progchapter.html#exploring-data-with-functionals"><i class="fa fa-check"></i><b>6.5.5</b> Exploring data with functionals</a></li>
<li class="chapter" data-level="6.5.6" data-path="progchapter.html"><a href="progchapter.html#keep-calm-and-carry-on"><i class="fa fa-check"></i><b>6.5.6</b> Keep calm and carry on</a></li>
<li class="chapter" data-level="6.5.7" data-path="progchapter.html"><a href="progchapter.html#multiiter"><i class="fa fa-check"></i><b>6.5.7</b> Iterating over multiple variables</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="progchapter.html"><a href="progchapter.html#measureperformance"><i class="fa fa-check"></i><b>6.6</b> Measuring code performance</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="progchapter.html"><a href="progchapter.html#timing-functions"><i class="fa fa-check"></i><b>6.6.1</b> Timing functions</a></li>
<li class="chapter" data-level="6.6.2" data-path="progchapter.html"><a href="progchapter.html#measuring-memory-usage---and-a-note-on-compilation"><i class="fa fa-check"></i><b>6.6.2</b> Measuring memory usage - and a note on compilation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="modchapter.html"><a href="modchapter.html"><i class="fa fa-check"></i><b>7</b> Modern classical statistics</a>
<ul>
<li class="chapter" data-level="7.1" data-path="modchapter.html"><a href="modchapter.html#simulation"><i class="fa fa-check"></i><b>7.1</b> Simulation and distributions</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="modchapter.html"><a href="modchapter.html#generating-random-numbers"><i class="fa fa-check"></i><b>7.1.1</b> Generating random numbers</a></li>
<li class="chapter" data-level="7.1.2" data-path="modchapter.html"><a href="modchapter.html#distfunctions"><i class="fa fa-check"></i><b>7.1.2</b> Some common distributions</a></li>
<li class="chapter" data-level="7.1.3" data-path="modchapter.html"><a href="modchapter.html#assessing-distributional-assumptions"><i class="fa fa-check"></i><b>7.1.3</b> Assessing distributional assumptions</a></li>
<li class="chapter" data-level="7.1.4" data-path="modchapter.html"><a href="modchapter.html#monte-carlo-integration"><i class="fa fa-check"></i><b>7.1.4</b> Monte Carlo integration</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="modchapter.html"><a href="modchapter.html#ttest"><i class="fa fa-check"></i><b>7.2</b> Student’s t-test revisited</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="modchapter.html"><a href="modchapter.html#oldschoolt"><i class="fa fa-check"></i><b>7.2.1</b> The old-school t-test</a></li>
<li class="chapter" data-level="7.2.2" data-path="modchapter.html"><a href="modchapter.html#permutation-tests"><i class="fa fa-check"></i><b>7.2.2</b> Permutation tests</a></li>
<li class="chapter" data-level="7.2.3" data-path="modchapter.html"><a href="modchapter.html#the-bootstrap"><i class="fa fa-check"></i><b>7.2.3</b> The bootstrap</a></li>
<li class="chapter" data-level="7.2.4" data-path="modchapter.html"><a href="modchapter.html#saving-the-output"><i class="fa fa-check"></i><b>7.2.4</b> Saving the output</a></li>
<li class="chapter" data-level="7.2.5" data-path="modchapter.html"><a href="modchapter.html#multipletesting"><i class="fa fa-check"></i><b>7.2.5</b> Multiple testing</a></li>
<li class="chapter" data-level="7.2.6" data-path="modchapter.html"><a href="modchapter.html#hotellingst2"><i class="fa fa-check"></i><b>7.2.6</b> Multivariate testing with Hotelling’s <span class="math inline">\(T^2\)</span></a></li>
<li class="chapter" data-level="7.2.7" data-path="modchapter.html"><a href="modchapter.html#sample-size-computations-for-the-t-test"><i class="fa fa-check"></i><b>7.2.7</b> Sample size computations for the t-test</a></li>
<li class="chapter" data-level="7.2.8" data-path="modchapter.html"><a href="modchapter.html#a-bayesian-approach"><i class="fa fa-check"></i><b>7.2.8</b> A Bayesian approach</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="modchapter.html"><a href="modchapter.html#other-common-hypothesis-tests-and-confidence-intervals"><i class="fa fa-check"></i><b>7.3</b> Other common hypothesis tests and confidence intervals</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="modchapter.html"><a href="modchapter.html#nonparametric-tests-of-location"><i class="fa fa-check"></i><b>7.3.1</b> Nonparametric tests of location</a></li>
<li class="chapter" data-level="7.3.2" data-path="modchapter.html"><a href="modchapter.html#tests-for-correlation"><i class="fa fa-check"></i><b>7.3.2</b> Tests for correlation</a></li>
<li class="chapter" data-level="7.3.3" data-path="modchapter.html"><a href="modchapter.html#chi2-tests"><i class="fa fa-check"></i><b>7.3.3</b> <span class="math inline">\(\chi^2\)</span>-tests</a></li>
<li class="chapter" data-level="7.3.4" data-path="modchapter.html"><a href="modchapter.html#confprop"><i class="fa fa-check"></i><b>7.3.4</b> Confidence intervals for proportions</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="modchapter.html"><a href="modchapter.html#ethicsinference"><i class="fa fa-check"></i><b>7.4</b> Ethical issues in statistical inference</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="modchapter.html"><a href="modchapter.html#p-hacking-and-the-file-drawer-problem"><i class="fa fa-check"></i><b>7.4.1</b> p-hacking and the file-drawer problem</a></li>
<li class="chapter" data-level="7.4.2" data-path="modchapter.html"><a href="modchapter.html#reproducibility"><i class="fa fa-check"></i><b>7.4.2</b> Reproducibility</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="modchapter.html"><a href="modchapter.html#simeval"><i class="fa fa-check"></i><b>7.5</b> Evaluating statistical methods using simulation</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="modchapter.html"><a href="modchapter.html#comparing-estimators"><i class="fa fa-check"></i><b>7.5.1</b> Comparing estimators</a></li>
<li class="chapter" data-level="7.5.2" data-path="modchapter.html"><a href="modchapter.html#simtypeI"><i class="fa fa-check"></i><b>7.5.2</b> Type I error rate of hypothesis tests</a></li>
<li class="chapter" data-level="7.5.3" data-path="modchapter.html"><a href="modchapter.html#simpower"><i class="fa fa-check"></i><b>7.5.3</b> Power of hypothesis tests</a></li>
<li class="chapter" data-level="7.5.4" data-path="modchapter.html"><a href="modchapter.html#powerlocation"><i class="fa fa-check"></i><b>7.5.4</b> Power of some tests of location</a></li>
<li class="chapter" data-level="7.5.5" data-path="modchapter.html"><a href="modchapter.html#simadvice"><i class="fa fa-check"></i><b>7.5.5</b> Some advice on simulation studies</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="modchapter.html"><a href="modchapter.html#sscus"><i class="fa fa-check"></i><b>7.6</b> Sample size computations using simulation</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="modchapter.html"><a href="modchapter.html#simcorpower"><i class="fa fa-check"></i><b>7.6.1</b> Writing your own simulation</a></li>
<li class="chapter" data-level="7.6.2" data-path="modchapter.html"><a href="modchapter.html#the-wilcoxon-mann-whitney-test"><i class="fa fa-check"></i><b>7.6.2</b> The Wilcoxon-Mann-Whitney test</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="modchapter.html"><a href="modchapter.html#bootstrap"><i class="fa fa-check"></i><b>7.7</b> Bootstrapping</a>
<ul>
<li class="chapter" data-level="7.7.1" data-path="modchapter.html"><a href="modchapter.html#a-general-approach"><i class="fa fa-check"></i><b>7.7.1</b> A general approach</a></li>
<li class="chapter" data-level="7.7.2" data-path="modchapter.html"><a href="modchapter.html#bootstrap-confidence-intervals"><i class="fa fa-check"></i><b>7.7.2</b> Bootstrap confidence intervals</a></li>
<li class="chapter" data-level="7.7.3" data-path="modchapter.html"><a href="modchapter.html#intervalinversion"><i class="fa fa-check"></i><b>7.7.3</b> Bootstrap hypothesis tests</a></li>
<li class="chapter" data-level="7.7.4" data-path="modchapter.html"><a href="modchapter.html#parametricbootstrap"><i class="fa fa-check"></i><b>7.7.4</b> The parametric bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="modchapter.html"><a href="modchapter.html#reporting-statistical-results"><i class="fa fa-check"></i><b>7.8</b> Reporting statistical results</a>
<ul>
<li class="chapter" data-level="7.8.1" data-path="modchapter.html"><a href="modchapter.html#what-should-you-include"><i class="fa fa-check"></i><b>7.8.1</b> What should you include?</a></li>
<li class="chapter" data-level="7.8.2" data-path="modchapter.html"><a href="modchapter.html#citing-r-packages"><i class="fa fa-check"></i><b>7.8.2</b> Citing R packages</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>8</b> Regression models</a>
<ul>
<li class="chapter" data-level="8.1" data-path="regression.html"><a href="regression.html#linearmodels"><i class="fa fa-check"></i><b>8.1</b> Linear models</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="regression.html"><a href="regression.html#fitting-linear-models"><i class="fa fa-check"></i><b>8.1.1</b> Fitting linear models</a></li>
<li class="chapter" data-level="8.1.2" data-path="regression.html"><a href="regression.html#interactions-and-polynomial-terms"><i class="fa fa-check"></i><b>8.1.2</b> Interactions and polynomial terms</a></li>
<li class="chapter" data-level="8.1.3" data-path="regression.html"><a href="regression.html#dummy-variables"><i class="fa fa-check"></i><b>8.1.3</b> Dummy variables</a></li>
<li class="chapter" data-level="8.1.4" data-path="regression.html"><a href="regression.html#model-diagnostics"><i class="fa fa-check"></i><b>8.1.4</b> Model diagnostics</a></li>
<li class="chapter" data-level="8.1.5" data-path="regression.html"><a href="regression.html#transformations"><i class="fa fa-check"></i><b>8.1.5</b> Transformations</a></li>
<li class="chapter" data-level="8.1.6" data-path="regression.html"><a href="regression.html#alternatives-to-lm"><i class="fa fa-check"></i><b>8.1.6</b> Alternatives to <code>lm</code></a></li>
<li class="chapter" data-level="8.1.7" data-path="regression.html"><a href="regression.html#regbootstrap"><i class="fa fa-check"></i><b>8.1.7</b> Bootstrap confidence intervals for regression coefficients</a></li>
<li class="chapter" data-level="8.1.8" data-path="regression.html"><a href="regression.html#broom"><i class="fa fa-check"></i><b>8.1.8</b> Alternative summaries with <code>broom</code></a></li>
<li class="chapter" data-level="8.1.9" data-path="regression.html"><a href="regression.html#stepwise"><i class="fa fa-check"></i><b>8.1.9</b> Variable selection</a></li>
<li class="chapter" data-level="8.1.10" data-path="regression.html"><a href="regression.html#prediction"><i class="fa fa-check"></i><b>8.1.10</b> Prediction</a></li>
<li class="chapter" data-level="8.1.11" data-path="regression.html"><a href="regression.html#multipredict"><i class="fa fa-check"></i><b>8.1.11</b> Prediction for multiple datasets</a></li>
<li class="chapter" data-level="8.1.12" data-path="regression.html"><a href="regression.html#ANOVA"><i class="fa fa-check"></i><b>8.1.12</b> ANOVA</a></li>
<li class="chapter" data-level="8.1.13" data-path="regression.html"><a href="regression.html#bayeslm"><i class="fa fa-check"></i><b>8.1.13</b> Bayesian estimation of linear models</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="regression.html"><a href="regression.html#ethical-issues-in-regression-modelling"><i class="fa fa-check"></i><b>8.2</b> Ethical issues in regression modelling</a></li>
<li class="chapter" data-level="8.3" data-path="regression.html"><a href="regression.html#generalised-linear-models"><i class="fa fa-check"></i><b>8.3</b> Generalised linear models</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="regression.html"><a href="regression.html#logreg"><i class="fa fa-check"></i><b>8.3.1</b> Modelling proportions: Logistic regression</a></li>
<li class="chapter" data-level="8.3.2" data-path="regression.html"><a href="regression.html#bootstrap-confidence-intervals-1"><i class="fa fa-check"></i><b>8.3.2</b> Bootstrap confidence intervals</a></li>
<li class="chapter" data-level="8.3.3" data-path="regression.html"><a href="regression.html#model-diagnostics-1"><i class="fa fa-check"></i><b>8.3.3</b> Model diagnostics</a></li>
<li class="chapter" data-level="8.3.4" data-path="regression.html"><a href="regression.html#prediction-1"><i class="fa fa-check"></i><b>8.3.4</b> Prediction</a></li>
<li class="chapter" data-level="8.3.5" data-path="regression.html"><a href="regression.html#modelling-count-data"><i class="fa fa-check"></i><b>8.3.5</b> Modelling count data</a></li>
<li class="chapter" data-level="8.3.6" data-path="regression.html"><a href="regression.html#modelling-rates"><i class="fa fa-check"></i><b>8.3.6</b> Modelling rates</a></li>
<li class="chapter" data-level="8.3.7" data-path="regression.html"><a href="regression.html#bayesian-estimation-of-generalised-linear-models"><i class="fa fa-check"></i><b>8.3.7</b> Bayesian estimation of generalised linear models</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="regression.html"><a href="regression.html#mixedmodels"><i class="fa fa-check"></i><b>8.4</b> Mixed models</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="regression.html"><a href="regression.html#fitting-a-linear-mixed-model"><i class="fa fa-check"></i><b>8.4.1</b> Fitting a linear mixed model</a></li>
<li class="chapter" data-level="8.4.2" data-path="regression.html"><a href="regression.html#model-diagnostics-2"><i class="fa fa-check"></i><b>8.4.2</b> Model diagnostics</a></li>
<li class="chapter" data-level="8.4.3" data-path="regression.html"><a href="regression.html#lmmbootstrap"><i class="fa fa-check"></i><b>8.4.3</b> Bootstrapping</a></li>
<li class="chapter" data-level="8.4.4" data-path="regression.html"><a href="regression.html#nested-random-effects-and-multilevelhierarchical-models"><i class="fa fa-check"></i><b>8.4.4</b> Nested random effects and multilevel/hierarchical models</a></li>
<li class="chapter" data-level="8.4.5" data-path="regression.html"><a href="regression.html#anova-with-random-effects"><i class="fa fa-check"></i><b>8.4.5</b> ANOVA with random effects</a></li>
<li class="chapter" data-level="8.4.6" data-path="regression.html"><a href="regression.html#generalised-linear-mixed-models"><i class="fa fa-check"></i><b>8.4.6</b> Generalised linear mixed models</a></li>
<li class="chapter" data-level="8.4.7" data-path="regression.html"><a href="regression.html#bayesian-estimation-of-mixed-models"><i class="fa fa-check"></i><b>8.4.7</b> Bayesian estimation of mixed models</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="regression.html"><a href="regression.html#survival"><i class="fa fa-check"></i><b>8.5</b> Survival analysis</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="regression.html"><a href="regression.html#comparing-groups"><i class="fa fa-check"></i><b>8.5.1</b> Comparing groups</a></li>
<li class="chapter" data-level="8.5.2" data-path="regression.html"><a href="regression.html#the-cox-proportional-hazards-model"><i class="fa fa-check"></i><b>8.5.2</b> The Cox proportional hazards model</a></li>
<li class="chapter" data-level="8.5.3" data-path="regression.html"><a href="regression.html#accelerated-failure-time-models"><i class="fa fa-check"></i><b>8.5.3</b> Accelerated failure time models</a></li>
<li class="chapter" data-level="8.5.4" data-path="regression.html"><a href="regression.html#bayesian-survival-analysis"><i class="fa fa-check"></i><b>8.5.4</b> Bayesian survival analysis</a></li>
<li class="chapter" data-level="8.5.5" data-path="regression.html"><a href="regression.html#multivariate-survival-analysis"><i class="fa fa-check"></i><b>8.5.5</b> Multivariate survival analysis</a></li>
<li class="chapter" data-level="8.5.6" data-path="regression.html"><a href="regression.html#power-estimates-for-the-logrank-test"><i class="fa fa-check"></i><b>8.5.6</b> Power estimates for the logrank test</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="regression.html"><a href="regression.html#left-censored-data-and-nondetects"><i class="fa fa-check"></i><b>8.6</b> Left-censored data and nondetects</a>
<ul>
<li class="chapter" data-level="8.6.1" data-path="regression.html"><a href="regression.html#estimation"><i class="fa fa-check"></i><b>8.6.1</b> Estimation</a></li>
<li class="chapter" data-level="8.6.2" data-path="regression.html"><a href="regression.html#tests-of-means"><i class="fa fa-check"></i><b>8.6.2</b> Tests of means</a></li>
<li class="chapter" data-level="8.6.3" data-path="regression.html"><a href="regression.html#censored-regression"><i class="fa fa-check"></i><b>8.6.3</b> Censored regression</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="regression.html"><a href="regression.html#creating-matched-samples"><i class="fa fa-check"></i><b>8.7</b> Creating matched samples</a>
<ul>
<li class="chapter" data-level="8.7.1" data-path="regression.html"><a href="regression.html#propensity-score-matching"><i class="fa fa-check"></i><b>8.7.1</b> Propensity score matching</a></li>
<li class="chapter" data-level="8.7.2" data-path="regression.html"><a href="regression.html#stepwise-matching"><i class="fa fa-check"></i><b>8.7.2</b> Stepwise matching</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="mlchapter.html"><a href="mlchapter.html"><i class="fa fa-check"></i><b>9</b> Predictive modelling and machine learning</a>
<ul>
<li class="chapter" data-level="9.1" data-path="mlchapter.html"><a href="mlchapter.html#evaluating-predictive-models"><i class="fa fa-check"></i><b>9.1</b> Evaluating predictive models</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="mlchapter.html"><a href="mlchapter.html#evaluating-regression-models"><i class="fa fa-check"></i><b>9.1.1</b> Evaluating regression models</a></li>
<li class="chapter" data-level="9.1.2" data-path="mlchapter.html"><a href="mlchapter.html#test-training-splits"><i class="fa fa-check"></i><b>9.1.2</b> Test-training splits</a></li>
<li class="chapter" data-level="9.1.3" data-path="mlchapter.html"><a href="mlchapter.html#loocv"><i class="fa fa-check"></i><b>9.1.3</b> Leave-one-out cross-validation and <code>caret</code></a></li>
<li class="chapter" data-level="9.1.4" data-path="mlchapter.html"><a href="mlchapter.html#kfoldcv"><i class="fa fa-check"></i><b>9.1.4</b> k-fold cross-validation</a></li>
<li class="chapter" data-level="9.1.5" data-path="mlchapter.html"><a href="mlchapter.html#twinned-observations"><i class="fa fa-check"></i><b>9.1.5</b> Twinned observations</a></li>
<li class="chapter" data-level="9.1.6" data-path="mlchapter.html"><a href="mlchapter.html#bootstrapping"><i class="fa fa-check"></i><b>9.1.6</b> Bootstrapping</a></li>
<li class="chapter" data-level="9.1.7" data-path="mlchapter.html"><a href="mlchapter.html#classifieraccuracy"><i class="fa fa-check"></i><b>9.1.7</b> Evaluating classification models</a></li>
<li class="chapter" data-level="9.1.8" data-path="mlchapter.html"><a href="mlchapter.html#decisionboundaries"><i class="fa fa-check"></i><b>9.1.8</b> Visualising decision boundaries</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="mlchapter.html"><a href="mlchapter.html#ethical-issues-in-predictive-modelling"><i class="fa fa-check"></i><b>9.2</b> Ethical issues in predictive modelling</a></li>
<li class="chapter" data-level="9.3" data-path="mlchapter.html"><a href="mlchapter.html#modellingchallenges"><i class="fa fa-check"></i><b>9.3</b> Challenges in predictive modelling</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="mlchapter.html"><a href="mlchapter.html#imbalanced"><i class="fa fa-check"></i><b>9.3.1</b> Handling class imbalance</a></li>
<li class="chapter" data-level="9.3.2" data-path="mlchapter.html"><a href="mlchapter.html#varimportance"><i class="fa fa-check"></i><b>9.3.2</b> Assessing variable importance</a></li>
<li class="chapter" data-level="9.3.3" data-path="mlchapter.html"><a href="mlchapter.html#extrapolation"><i class="fa fa-check"></i><b>9.3.3</b> Extrapolation</a></li>
<li class="chapter" data-level="9.3.4" data-path="mlchapter.html"><a href="mlchapter.html#missing-data-and-imputation"><i class="fa fa-check"></i><b>9.3.4</b> Missing data and imputation</a></li>
<li class="chapter" data-level="9.3.5" data-path="mlchapter.html"><a href="mlchapter.html#endless-waiting"><i class="fa fa-check"></i><b>9.3.5</b> Endless waiting</a></li>
<li class="chapter" data-level="9.3.6" data-path="mlchapter.html"><a href="mlchapter.html#overfittingtestset"><i class="fa fa-check"></i><b>9.3.6</b> Overfitting to the test set</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="mlchapter.html"><a href="mlchapter.html#regularised"><i class="fa fa-check"></i><b>9.4</b> Regularised regression models</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="mlchapter.html"><a href="mlchapter.html#ridge-regression"><i class="fa fa-check"></i><b>9.4.1</b> Ridge regression</a></li>
<li class="chapter" data-level="9.4.2" data-path="mlchapter.html"><a href="mlchapter.html#lasso"><i class="fa fa-check"></i><b>9.4.2</b> The lasso</a></li>
<li class="chapter" data-level="9.4.3" data-path="mlchapter.html"><a href="mlchapter.html#elastic-net"><i class="fa fa-check"></i><b>9.4.3</b> Elastic net</a></li>
<li class="chapter" data-level="9.4.4" data-path="mlchapter.html"><a href="mlchapter.html#choosing-the-best-model"><i class="fa fa-check"></i><b>9.4.4</b> Choosing the best model</a></li>
<li class="chapter" data-level="9.4.5" data-path="mlchapter.html"><a href="mlchapter.html#regularised-mixed-models"><i class="fa fa-check"></i><b>9.4.5</b> Regularised mixed models</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="mlchapter.html"><a href="mlchapter.html#mlmethods"><i class="fa fa-check"></i><b>9.5</b> Machine learning models</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="mlchapter.html"><a href="mlchapter.html#decisiontrees"><i class="fa fa-check"></i><b>9.5.1</b> Decision trees</a></li>
<li class="chapter" data-level="9.5.2" data-path="mlchapter.html"><a href="mlchapter.html#randomforests"><i class="fa fa-check"></i><b>9.5.2</b> Random forests</a></li>
<li class="chapter" data-level="9.5.3" data-path="mlchapter.html"><a href="mlchapter.html#boosted-trees"><i class="fa fa-check"></i><b>9.5.3</b> Boosted trees</a></li>
<li class="chapter" data-level="9.5.4" data-path="mlchapter.html"><a href="mlchapter.html#model-trees"><i class="fa fa-check"></i><b>9.5.4</b> Model trees</a></li>
<li class="chapter" data-level="9.5.5" data-path="mlchapter.html"><a href="mlchapter.html#discriminant-analysis"><i class="fa fa-check"></i><b>9.5.5</b> Discriminant analysis</a></li>
<li class="chapter" data-level="9.5.6" data-path="mlchapter.html"><a href="mlchapter.html#support-vector-machines"><i class="fa fa-check"></i><b>9.5.6</b> Support vector machines</a></li>
<li class="chapter" data-level="9.5.7" data-path="mlchapter.html"><a href="mlchapter.html#nearest-neighbours-classifiers"><i class="fa fa-check"></i><b>9.5.7</b> Nearest neighbours classifiers</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="mlchapter.html"><a href="mlchapter.html#tsforecast"><i class="fa fa-check"></i><b>9.6</b> Forecasting time series</a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="mlchapter.html"><a href="mlchapter.html#decomposition"><i class="fa fa-check"></i><b>9.6.1</b> Decomposition</a></li>
<li class="chapter" data-level="9.6.2" data-path="mlchapter.html"><a href="mlchapter.html#forecasting-using-arima-models"><i class="fa fa-check"></i><b>9.6.2</b> Forecasting using ARIMA models</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="mlchapter.html"><a href="mlchapter.html#deploying-models"><i class="fa fa-check"></i><b>9.7</b> Deploying models</a>
<ul>
<li class="chapter" data-level="9.7.1" data-path="mlchapter.html"><a href="mlchapter.html#creating-apis-with-plumber"><i class="fa fa-check"></i><b>9.7.1</b> Creating APIs with <code>plumber</code></a></li>
<li class="chapter" data-level="9.7.2" data-path="mlchapter.html"><a href="mlchapter.html#different-types-of-output"><i class="fa fa-check"></i><b>9.7.2</b> Different types of output</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="advancedchapter.html"><a href="advancedchapter.html"><i class="fa fa-check"></i><b>10</b> Advanced topics</a>
<ul>
<li class="chapter" data-level="10.1" data-path="advancedchapter.html"><a href="advancedchapter.html#moreonpackages"><i class="fa fa-check"></i><b>10.1</b> More on packages</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="advancedchapter.html"><a href="advancedchapter.html#loading-and-auto-installing-packages"><i class="fa fa-check"></i><b>10.1.1</b> Loading and auto-installing packages</a></li>
<li class="chapter" data-level="10.1.2" data-path="advancedchapter.html"><a href="advancedchapter.html#updating-r-and-your-packages"><i class="fa fa-check"></i><b>10.1.2</b> Updating R and your packages</a></li>
<li class="chapter" data-level="10.1.3" data-path="advancedchapter.html"><a href="advancedchapter.html#alternative-repositories"><i class="fa fa-check"></i><b>10.1.3</b> Alternative repositories</a></li>
<li class="chapter" data-level="10.1.4" data-path="advancedchapter.html"><a href="advancedchapter.html#removing-packages"><i class="fa fa-check"></i><b>10.1.4</b> Removing packages</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="advancedchapter.html"><a href="advancedchapter.html#parallel"><i class="fa fa-check"></i><b>10.2</b> Speeding up computations with parallelisation</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="advancedchapter.html"><a href="advancedchapter.html#parallelising-for-loops"><i class="fa fa-check"></i><b>10.2.1</b> Parallelising <code>for</code> loops</a></li>
<li class="chapter" data-level="10.2.2" data-path="advancedchapter.html"><a href="advancedchapter.html#parallelising-functionals"><i class="fa fa-check"></i><b>10.2.2</b> Parallelising functionals</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="advancedchapter.html"><a href="advancedchapter.html#linearalgebra"><i class="fa fa-check"></i><b>10.3</b> Linear algebra and matrices</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="advancedchapter.html"><a href="advancedchapter.html#creating-matrices"><i class="fa fa-check"></i><b>10.3.1</b> Creating matrices</a></li>
<li class="chapter" data-level="10.3.2" data-path="advancedchapter.html"><a href="advancedchapter.html#sparse-matrices"><i class="fa fa-check"></i><b>10.3.2</b> Sparse matrices</a></li>
<li class="chapter" data-level="10.3.3" data-path="advancedchapter.html"><a href="advancedchapter.html#matrix-operations"><i class="fa fa-check"></i><b>10.3.3</b> Matrix operations</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="advancedchapter.html"><a href="advancedchapter.html#integration"><i class="fa fa-check"></i><b>10.4</b> Integration with other programming languages</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="advancedchapter.html"><a href="advancedchapter.html#integration-with-c"><i class="fa fa-check"></i><b>10.4.1</b> Integration with C++</a></li>
<li class="chapter" data-level="10.4.2" data-path="advancedchapter.html"><a href="advancedchapter.html#integration-with-python"><i class="fa fa-check"></i><b>10.4.2</b> Integration with Python</a></li>
<li class="chapter" data-level="10.4.3" data-path="advancedchapter.html"><a href="advancedchapter.html#integration-with-tensorflow-and-pytorch"><i class="fa fa-check"></i><b>10.4.3</b> Integration with Tensorflow and PyTorch</a></li>
<li class="chapter" data-level="10.4.4" data-path="advancedchapter.html"><a href="advancedchapter.html#integration-with-spark"><i class="fa fa-check"></i><b>10.4.4</b> Integration with Spark</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="errorchapter.html"><a href="errorchapter.html"><i class="fa fa-check"></i><b>11</b> Debugging</a>
<ul>
<li class="chapter" data-level="11.1" data-path="errorchapter.html"><a href="errorchapter.html#debugging"><i class="fa fa-check"></i><b>11.1</b> Debugging</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="errorchapter.html"><a href="errorchapter.html#find-out-where-the-error-occured-with-traceback"><i class="fa fa-check"></i><b>11.1.1</b> Find out where the error occured with <code>traceback</code></a></li>
<li class="chapter" data-level="11.1.2" data-path="errorchapter.html"><a href="errorchapter.html#interactive-debugging-of-functions-with-debug"><i class="fa fa-check"></i><b>11.1.2</b> Interactive debugging of functions with <code>debug</code></a></li>
<li class="chapter" data-level="11.1.3" data-path="errorchapter.html"><a href="errorchapter.html#investigate-the-environment-with-recover"><i class="fa fa-check"></i><b>11.1.3</b> Investigate the environment with <code>recover</code></a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="errorchapter.html"><a href="errorchapter.html#commonerrors"><i class="fa fa-check"></i><b>11.2</b> Common error messages</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="errorchapter.html"><a href="errorchapter.html#section-1"><i class="fa fa-check"></i><b>11.2.1</b> <code>+</code></a></li>
<li class="chapter" data-level="11.2.2" data-path="errorchapter.html"><a href="errorchapter.html#could-not-find-function"><i class="fa fa-check"></i><b>11.2.2</b> <code>could not find function</code></a></li>
<li class="chapter" data-level="11.2.3" data-path="errorchapter.html"><a href="errorchapter.html#object-not-found"><i class="fa fa-check"></i><b>11.2.3</b> <code>object not found</code></a></li>
<li class="chapter" data-level="11.2.4" data-path="errorchapter.html"><a href="errorchapter.html#cannot-open-the-connection-and-no-such-file-or-directory"><i class="fa fa-check"></i><b>11.2.4</b> <code>cannot open the connection</code> and <code>No such file or directory</code></a></li>
<li class="chapter" data-level="11.2.5" data-path="errorchapter.html"><a href="errorchapter.html#invalid-description-argument"><i class="fa fa-check"></i><b>11.2.5</b> <code>invalid 'description' argument</code></a></li>
<li class="chapter" data-level="11.2.6" data-path="errorchapter.html"><a href="errorchapter.html#missing-value-where-truefalse-needed"><i class="fa fa-check"></i><b>11.2.6</b> <code>missing value where TRUE/FALSE needed</code></a></li>
<li class="chapter" data-level="11.2.7" data-path="errorchapter.html"><a href="errorchapter.html#unexpected-in-..."><i class="fa fa-check"></i><b>11.2.7</b> <code>unexpected '=' in ...</code></a></li>
<li class="chapter" data-level="11.2.8" data-path="errorchapter.html"><a href="errorchapter.html#attempt-to-apply-non-function"><i class="fa fa-check"></i><b>11.2.8</b> <code>attempt to apply non-function</code></a></li>
<li class="chapter" data-level="11.2.9" data-path="errorchapter.html"><a href="errorchapter.html#undefined-columns-selected"><i class="fa fa-check"></i><b>11.2.9</b> <code>undefined columns selected</code></a></li>
<li class="chapter" data-level="11.2.10" data-path="errorchapter.html"><a href="errorchapter.html#subscript-out-of-bounds"><i class="fa fa-check"></i><b>11.2.10</b> <code>subscript out of bounds</code></a></li>
<li class="chapter" data-level="11.2.11" data-path="errorchapter.html"><a href="errorchapter.html#object-of-type-closure-is-not-subsettable"><i class="fa fa-check"></i><b>11.2.11</b> <code>Object of type ‘closure’ is not subsettable</code></a></li>
<li class="chapter" data-level="11.2.12" data-path="errorchapter.html"><a href="errorchapter.html#operator-is-invalid-for-atomic-vectors"><i class="fa fa-check"></i><b>11.2.12</b> <code>$ operator is invalid for atomic vectors</code></a></li>
<li class="chapter" data-level="11.2.13" data-path="errorchapter.html"><a href="errorchapter.html#list-object-cannot-be-coerced-to-type-double"><i class="fa fa-check"></i><b>11.2.13</b> <code>(list) object cannot be coerced to type ‘double’</code></a></li>
<li class="chapter" data-level="11.2.14" data-path="errorchapter.html"><a href="errorchapter.html#arguments-imply-differing-number-of-rows"><i class="fa fa-check"></i><b>11.2.14</b> <code>arguments imply differing number of rows</code></a></li>
<li class="chapter" data-level="11.2.15" data-path="errorchapter.html"><a href="errorchapter.html#non-numeric-argument-to-a-binary-operator"><i class="fa fa-check"></i><b>11.2.15</b> <code>non-numeric argument to a binary operator</code></a></li>
<li class="chapter" data-level="11.2.16" data-path="errorchapter.html"><a href="errorchapter.html#non-numeric-argument-to-mathematical-function"><i class="fa fa-check"></i><b>11.2.16</b> <code>non-numeric argument to mathematical function</code></a></li>
<li class="chapter" data-level="11.2.17" data-path="errorchapter.html"><a href="errorchapter.html#cannot-allocate-vector-of-size-..."><i class="fa fa-check"></i><b>11.2.17</b> <code>cannot allocate vector of size ...</code></a></li>
<li class="chapter" data-level="11.2.18" data-path="errorchapter.html"><a href="errorchapter.html#error-in-plot.new-figure-margins-too-large"><i class="fa fa-check"></i><b>11.2.18</b> <code>Error in plot.new() : figure margins too large</code></a></li>
<li class="chapter" data-level="11.2.19" data-path="errorchapter.html"><a href="errorchapter.html#error-in-.call.graphicsc_palette2-.callc_palette2-null-invalid-graphics-state"><i class="fa fa-check"></i><b>11.2.19</b> <code>Error in .Call.graphics(C_palette2, .Call(C_palette2, NULL)) : invalid graphics state</code></a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="errorchapter.html"><a href="errorchapter.html#commonwarnings"><i class="fa fa-check"></i><b>11.3</b> Common warning messages</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="errorchapter.html"><a href="errorchapter.html#replacement-has-...-rows-..."><i class="fa fa-check"></i><b>11.3.1</b> <code>replacement has ... rows ...</code></a></li>
<li class="chapter" data-level="11.3.2" data-path="errorchapter.html"><a href="errorchapter.html#iferror"><i class="fa fa-check"></i><b>11.3.2</b> <code>the condition has length &gt; 1 and only the first element will be used</code></a></li>
<li class="chapter" data-level="11.3.3" data-path="errorchapter.html"><a href="errorchapter.html#number-of-items-to-replace-is-not-a-multiple-of-replacement-length"><i class="fa fa-check"></i><b>11.3.3</b> <code>number of items to replace is not a multiple of replacement length</code></a></li>
<li class="chapter" data-level="11.3.4" data-path="errorchapter.html"><a href="errorchapter.html#longer-object-length-is-not-a-multiple-of-shorter-object-length"><i class="fa fa-check"></i><b>11.3.4</b> <code>longer object length is not a multiple of shorter object length</code></a></li>
<li class="chapter" data-level="11.3.5" data-path="errorchapter.html"><a href="errorchapter.html#nas-introduced-by-coercion"><i class="fa fa-check"></i><b>11.3.5</b> <code>NAs introduced by coercion</code></a></li>
<li class="chapter" data-level="11.3.6" data-path="errorchapter.html"><a href="errorchapter.html#package-is-not-available-for-r-version-4.x.x"><i class="fa fa-check"></i><b>11.3.6</b> <code>package is not available (for R version 4.x.x)</code></a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="errorchapter.html"><a href="errorchapter.html#installationmessages"><i class="fa fa-check"></i><b>11.4</b> Messages printed when installing <code>ggplot2</code></a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="mathschap.html"><a href="mathschap.html"><i class="fa fa-check"></i><b>12</b> Mathematical appendix</a>
<ul>
<li class="chapter" data-level="12.1" data-path="mathschap.html"><a href="mathschap.html#bootstrapcimaths"><i class="fa fa-check"></i><b>12.1</b> Bootstrap confidence intervals</a></li>
<li class="chapter" data-level="12.2" data-path="mathschap.html"><a href="mathschap.html#confintequal"><i class="fa fa-check"></i><b>12.2</b> The equivalence between confidence intervals and hypothesis tests</a></li>
<li class="chapter" data-level="12.3" data-path="mathschap.html"><a href="mathschap.html#twotypesofpvalues"><i class="fa fa-check"></i><b>12.3</b> Two types of p-values</a></li>
<li class="chapter" data-level="12.4" data-path="mathschap.html"><a href="mathschap.html#deviance"><i class="fa fa-check"></i><b>12.4</b> Deviance tests</a></li>
<li class="chapter" data-level="12.5" data-path="mathschap.html"><a href="mathschap.html#regreg"><i class="fa fa-check"></i><b>12.5</b> Regularised regression</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="solutions.html"><a href="solutions.html"><i class="fa fa-check"></i><b>13</b> Solutions to exercises</a>
<ul>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#solutionsch2"><i class="fa fa-check"></i>Chapter 2</a></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#solutionsch3"><i class="fa fa-check"></i>Chapter 3</a></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#solutionsch4"><i class="fa fa-check"></i>Chapter 4</a></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#ch5solutions"><i class="fa fa-check"></i>Chapter 5</a></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#ch6solutions"><i class="fa fa-check"></i>Chapter 6</a></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#ch7solutions"><i class="fa fa-check"></i>Chapter 7</a></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#ch7bsolutions"><i class="fa fa-check"></i>Chapter 8</a></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#ch8solutions"><i class="fa fa-check"></i>Chapter 9</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>Bibliography</a>
<ul>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html#references1"><i class="fa fa-check"></i>Further reading</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html#references2"><i class="fa fa-check"></i>Online resources</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html#references3"><i class="fa fa-check"></i>References</a></li>
</ul></li>
<li class="divider"></li>
<li>&copy; 2023 psnam</li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">R 통계분석</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mlchapter" class="section level1 hasAnchor" number="9">
<h1><span class="header-section-number">9</span> Predictive modelling and machine learning<a href="mlchapter.html#mlchapter" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In predictive modelling, we fit statistical models that use historical data to make predictions about future (or unknown) outcomes. This practice is a cornerstone of modern statistics, and includes methods ranging from classical parametric linear regression to black-box machine learning models.</p>
<p>After reading this chapter, you will be able to use R to:</p>
<ul>
<li>Fit predictive models for regression and classification,</li>
<li>Evaluate predictive models,</li>
<li>Use cross-validation and the bootstrap for out-of-sample evaluations,</li>
<li>Handle imbalanced classes in classification problems,</li>
<li>Fit regularised (and possibly also generalised) linear models, e.g. using the lasso,</li>
<li>Fit a number of machine learning models, including kNN, decision trees, random forests, and boosted trees.</li>
<li>Make forecasts based on time series data.</li>
</ul>
<div id="evaluating-predictive-models" class="section level2 hasAnchor" number="9.1">
<h2><span class="header-section-number">9.1</span> Evaluating predictive models<a href="mlchapter.html#evaluating-predictive-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In many ways, modern predictive modelling differs from the more traditional inference problems that we studied in the previous chapter. The goal of predictive modelling is (usually) not to test whether some variable affects another or to study causal relationships. Instead, our only goal is to make good predictions. It is little surprise then that the tools we use to evaluate predictive models differ from those used to evaluate models used for other purposes, like hypothesis testing. In this section, we will have a look at how to evaluate predictive models.</p>
<p>The terminology used in predictive modelling differs a little from that used in traditional statistics. For instance, explanatory variables are often called <em>features</em> or <em>predictors</em>, and predictive modelling is often referred to as <em>supervised learning</em>. We will stick with the terms used in Section <a href="modchapter.html#modchapter">7</a>, to keep the terminology consistent within the book.</p>
<p>Predictive models can be divided into two categories:</p>
<ul>
<li><em>Regression</em>, where we want to make predictions for a numeric variable,</li>
<li><em>Classification</em>, where we want to make predictions for a categorical variable.</li>
</ul>
<p>There are many similarities between these two, but we need to use different measures when evaluating their predictive performance. Let’s start with models for numeric predictions, i.e. regression models.</p>
<div id="evaluating-regression-models" class="section level3 hasAnchor" number="9.1.1">
<h3><span class="header-section-number">9.1.1</span> Evaluating regression models<a href="mlchapter.html#evaluating-regression-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s return to the <code>mtcars</code> data that we studied in Section <a href="regression.html#linearmodels">8.1</a>. There, we fitted a linear model to explain the fuel consumption of cars:</p>
<div class="sourceCode" id="cb956"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb956-1"><a href="mlchapter.html#cb956-1" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> ., <span class="at">data =</span> mtcars)</span></code></pre></div>
<p>(Recall that the formula <code>mpg ~ .</code> means that all variables in the dataset, except <code>mpg</code>, are used as explanatory variables in the model.)</p>
<p>A number of measures of how well the model fits the data have been proposed. Without going into details (it will soon be apparent why), we can mention examples like the coefficient of determination <span class="math inline">\(R^2\)</span>, and information criteria like <span class="math inline">\(AIC\)</span> and <span class="math inline">\(BIC\)</span>. All of these are straightforward to compute for our model:</p>
<div class="sourceCode" id="cb957"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb957-1"><a href="mlchapter.html#cb957-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)<span class="sc">$</span>r.squared     <span class="co"># R^2</span></span>
<span id="cb957-2"><a href="mlchapter.html#cb957-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)<span class="sc">$</span>adj.r.squared <span class="co"># Adjusted R^2</span></span>
<span id="cb957-3"><a href="mlchapter.html#cb957-3" aria-hidden="true" tabindex="-1"></a><span class="fu">AIC</span>(m)                   <span class="co"># AIC</span></span>
<span id="cb957-4"><a href="mlchapter.html#cb957-4" aria-hidden="true" tabindex="-1"></a><span class="fu">BIC</span>(m)                   <span class="co"># BIC</span></span></code></pre></div>
<p><span class="math inline">\(R^2\)</span> is a popular tool for assessing model fit, with values close to 1 indicating a good fit and values close to 0 indicating a poor fit (i.e. that most of the variation in the data isn’t accounted for).</p>
<p>It is nice if our model fits the data well, but what really matters in predictive modelling is how close the predictions from the model are to the truth. We therefore need ways to measure the distance between predicted values and observed values - ways to measure the size of the average prediction error. A common measure is the root-mean-square error (RMSE). Given <span class="math inline">\(n\)</span> observations <span class="math inline">\(y_1,y_2,\ldots,y_n\)</span> for which our model makes the predictions <span class="math inline">\(\hat{y}_1,\ldots,\hat{y}_n\)</span>, this is defined as
<span class="math display">\[RMSE = \sqrt{\frac{\sum_{i=1}^n(\hat{y}_i-y_i)^2}{n}},\]</span>
that is, as the named implies, the square root of the mean of the squared errors <span class="math inline">\((\hat{y}_i-y_i)^2\)</span>.</p>
<p>Another common measure is the mean absolute error (MAE):</p>
<p><span class="math display">\[MAE = \frac{\sum_{i=1}^n|\hat{y}_i-y_i|}{n}.\]</span></p>
<p>Let’s compare the predicted values <span class="math inline">\(\hat{y}_i\)</span> to the observed values <span class="math inline">\(y_i\)</span> for our <code>mtcars</code> model <code>m</code>:</p>
<div class="sourceCode" id="cb958"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb958-1"><a href="mlchapter.html#cb958-1" aria-hidden="true" tabindex="-1"></a>rmse <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((<span class="fu">predict</span>(m) <span class="sc">-</span> mtcars<span class="sc">$</span>mpg)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb958-2"><a href="mlchapter.html#cb958-2" aria-hidden="true" tabindex="-1"></a>mae <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">abs</span>(<span class="fu">predict</span>(m) <span class="sc">-</span> mtcars<span class="sc">$</span>mpg))</span>
<span id="cb958-3"><a href="mlchapter.html#cb958-3" aria-hidden="true" tabindex="-1"></a>rmse; mae</span></code></pre></div>
<p>There is a problem with this computation, and it is a big one. What we just computed was the difference between predicted values and observed values <em>for the sample that was used to fit the model</em>. This doesn’t necessarily tell us anything about how well the model will fare when used to make predictions about new observations. It is, for instance, entirely possible that our model has <em>overfitted</em> to the sample, and essentially has learned the examples therein by heart, ignoring the general patterns that we were trying to model. This would lead to a small <span class="math inline">\(RMSE\)</span> and <span class="math inline">\(MAE\)</span>, and a high <span class="math inline">\(R^2\)</span>, but would render the model useless for predictive purposes.</p>
<p>All the computations that we’ve just done - <span class="math inline">\(R^2\)</span>, <span class="math inline">\(AIC\)</span>, <span class="math inline">\(BIC\)</span>, <span class="math inline">\(RMSE\)</span> and <span class="math inline">\(MAE\)</span> - were examples of <em>in-sample evaluations</em> of our model. There are a number of problems associated with in-sample evaluations, all of which have been known for a long time - see e.g. Picard &amp; Cook (1984). In general, they tend to be overly optimistic and overestimate how well the model will perform for new data. It is about time that we got rid of them for good.</p>
<p>A fundamental principle of predictive modelling is that the model chiefly should be judged on how well it makes predictions for new data. To evaluate its performance, we therefore need to carry out some form of <em>out-of-sample evaluation</em>, i.e. to use the model to make predictions for new data (that weren’t used to fit the model). We can then compare those predictions to the actual observed values for those data, and e.g. compute the <span class="math inline">\(RMSE\)</span> or <span class="math inline">\(MAE\)</span> to measure the size of the average prediction error. Out-of-sample evaluations, when done right, are less overoptimistic than in-sample evaluations, and are also better in the sense that they actually measure the right thing.</p>
<p><span class="math display">\[\sim\]</span></p>
<div class="exercise">
<p><span id="exr:ch8exc1" class="exercise"><strong>Exercise 9.1  </strong></span>To see that a high <span class="math inline">\(R^2\)</span> and low p-values say very little about the predictive performance of a model, consider the following dataset with 30 randomly generated observations of four variables:</p>
</div>
<div class="sourceCode" id="cb959"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb959-1"><a href="mlchapter.html#cb959-1" aria-hidden="true" tabindex="-1"></a>exdata <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x1 =</span> <span class="fu">c</span>(<span class="fl">0.87</span>, <span class="sc">-</span><span class="fl">1.03</span>, <span class="fl">0.02</span>, <span class="sc">-</span><span class="fl">0.25</span>, <span class="sc">-</span><span class="fl">1.09</span>, <span class="fl">0.74</span>,</span>
<span id="cb959-2"><a href="mlchapter.html#cb959-2" aria-hidden="true" tabindex="-1"></a>          <span class="fl">0.09</span>, <span class="sc">-</span><span class="fl">1.64</span>, <span class="sc">-</span><span class="fl">0.32</span>, <span class="sc">-</span><span class="fl">0.33</span>, <span class="fl">1.40</span>, <span class="fl">0.29</span>, <span class="sc">-</span><span class="fl">0.71</span>, <span class="fl">1.36</span>, <span class="fl">0.64</span>,</span>
<span id="cb959-3"><a href="mlchapter.html#cb959-3" aria-hidden="true" tabindex="-1"></a>          <span class="sc">-</span><span class="fl">0.78</span>, <span class="sc">-</span><span class="fl">0.58</span>, <span class="fl">0.67</span>, <span class="sc">-</span><span class="fl">0.90</span>, <span class="sc">-</span><span class="fl">1.52</span>, <span class="sc">-</span><span class="fl">0.11</span>, <span class="sc">-</span><span class="fl">0.65</span>, <span class="fl">0.04</span>,</span>
<span id="cb959-4"><a href="mlchapter.html#cb959-4" aria-hidden="true" tabindex="-1"></a>          <span class="sc">-</span><span class="fl">0.72</span>, <span class="fl">1.71</span>, <span class="sc">-</span><span class="fl">1.58</span>, <span class="sc">-</span><span class="fl">1.76</span>, <span class="fl">2.10</span>, <span class="fl">0.81</span>, <span class="sc">-</span><span class="fl">0.30</span>),</span>
<span id="cb959-5"><a href="mlchapter.html#cb959-5" aria-hidden="true" tabindex="-1"></a>          <span class="at">x2 =</span> <span class="fu">c</span>(<span class="fl">1.38</span>, <span class="fl">0.14</span>, <span class="fl">1.46</span>, <span class="fl">0.27</span>, <span class="sc">-</span><span class="fl">1.02</span>, <span class="sc">-</span><span class="fl">1.94</span>, <span class="fl">0.12</span>, <span class="sc">-</span><span class="fl">0.64</span>,</span>
<span id="cb959-6"><a href="mlchapter.html#cb959-6" aria-hidden="true" tabindex="-1"></a>          <span class="fl">0.64</span>, <span class="sc">-</span><span class="fl">0.39</span>, <span class="fl">0.28</span>, <span class="fl">0.50</span>, <span class="sc">-</span><span class="fl">1.29</span>, <span class="fl">0.52</span>, <span class="fl">0.28</span>, <span class="fl">0.23</span>, <span class="fl">0.05</span>,</span>
<span id="cb959-7"><a href="mlchapter.html#cb959-7" aria-hidden="true" tabindex="-1"></a>          <span class="fl">3.10</span>, <span class="fl">0.84</span>, <span class="sc">-</span><span class="fl">0.66</span>, <span class="sc">-</span><span class="fl">1.35</span>, <span class="sc">-</span><span class="fl">0.06</span>, <span class="sc">-</span><span class="fl">0.66</span>, <span class="fl">0.40</span>, <span class="sc">-</span><span class="fl">0.23</span>,</span>
<span id="cb959-8"><a href="mlchapter.html#cb959-8" aria-hidden="true" tabindex="-1"></a>          <span class="sc">-</span><span class="fl">0.97</span>, <span class="sc">-</span><span class="fl">0.78</span>, <span class="fl">0.38</span>, <span class="fl">0.49</span>, <span class="fl">0.21</span>),</span>
<span id="cb959-9"><a href="mlchapter.html#cb959-9" aria-hidden="true" tabindex="-1"></a>          <span class="at">x3 =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>,</span>
<span id="cb959-10"><a href="mlchapter.html#cb959-10" aria-hidden="true" tabindex="-1"></a>          <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>),</span>
<span id="cb959-11"><a href="mlchapter.html#cb959-11" aria-hidden="true" tabindex="-1"></a>          <span class="at">y =</span> <span class="fu">c</span>(<span class="fl">3.47</span>, <span class="sc">-</span><span class="fl">0.80</span>, <span class="fl">4.57</span>, <span class="fl">0.16</span>, <span class="sc">-</span><span class="fl">1.77</span>, <span class="sc">-</span><span class="fl">6.84</span>, <span class="fl">1.28</span>, <span class="sc">-</span><span class="fl">0.52</span>,</span>
<span id="cb959-12"><a href="mlchapter.html#cb959-12" aria-hidden="true" tabindex="-1"></a>          <span class="fl">1.00</span>, <span class="sc">-</span><span class="fl">2.50</span>, <span class="sc">-</span><span class="fl">1.99</span>, <span class="fl">1.13</span>, <span class="sc">-</span><span class="fl">4.26</span>, <span class="fl">1.16</span>, <span class="sc">-</span><span class="fl">0.69</span>, <span class="fl">0.89</span>, <span class="sc">-</span><span class="fl">1.01</span>,</span>
<span id="cb959-13"><a href="mlchapter.html#cb959-13" aria-hidden="true" tabindex="-1"></a>          <span class="fl">7.56</span>, <span class="fl">2.33</span>, <span class="fl">0.36</span>, <span class="sc">-</span><span class="fl">1.11</span>, <span class="sc">-</span><span class="fl">0.53</span>, <span class="sc">-</span><span class="fl">1.44</span>, <span class="sc">-</span><span class="fl">0.43</span>, <span class="fl">0.69</span>, <span class="sc">-</span><span class="fl">2.30</span>,</span>
<span id="cb959-14"><a href="mlchapter.html#cb959-14" aria-hidden="true" tabindex="-1"></a>          <span class="sc">-</span><span class="fl">3.55</span>, <span class="fl">0.99</span>, <span class="sc">-</span><span class="fl">0.50</span>, <span class="sc">-</span><span class="fl">1.67</span>))</span></code></pre></div>
<ol style="list-style-type: decimal">
<li>The true relationship between the variables, used to generate the <code>y</code> variables, is <span class="math inline">\(y = 2x_1-x_2+x_3\cdot x_2\)</span>. Plot the <code>y</code> values in the data against this expected value. Does a linear model seem appropriate?</li>
<li>Fit a linear regression model with <code>x1</code>, <code>x2</code> and <code>x3</code> as explanatory variables (without any interactions) using the first 20 observations of the data. Do the p-values and <span class="math inline">\(R^2\)</span> indicate a good fit?</li>
<li>Make predictions for the remaining 10 observations. Are the predictions accurate?</li>
<li>A common (mal)practice is to remove explanatory variables that aren’t significant from a linear model (see Section <a href="regression.html#stepwise">8.1.9</a> for some comments on this). Remove any variables from the regression model with a p-value above 0.05, and refit the model using the first 20 observations. Do the p-values and <span class="math inline">\(R^2\)</span> indicate a good fit? Do the predictions for the remaining 10 observations improve?</li>
<li>Finally, fit a model with <code>x1</code>, <code>x2</code> and <code>x3*x2</code> as explanatory variables (i.e. a correctly specified model) to the first 20 observations. Do the predictions for the remaining 10 observations improve?</li>
</ol>
<p><a href="solutions.html#ch8solutions1">(Click here to go to the solution.)</a></p>
</div>
<div id="test-training-splits" class="section level3 hasAnchor" number="9.1.2">
<h3><span class="header-section-number">9.1.2</span> Test-training splits<a href="mlchapter.html#test-training-splits" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In some cases, our data is naturally separated into two sets, one of which can be used to fit a model and the other to evaluate it. A common example of this is when data has been collected during two distinct time periods, and the older data is used to fit a model that is evaluated on the newer data, to see if historical data can be used to predict the future.</p>
<p>In most cases though, we don’t have that luxury. A popular alternative is to artificially create two sets by randomly withdrawing a part of the data, 10 % or 20 % say, which can be used for evaluation. In machine learning lingo, model fitting is known as <em>training</em> and model evaluation as <em>testing</em>. The set used for training (fitting) the model is therefore often referred to as the <em>training data</em>, and the set used for testing (evaluating) the model is known as the <em>test data</em>.</p>
<p>Let’s try this out with the <code>mtcars</code> data. We’ll use 80 % of the data for fitting our model and 20 % for evaluating it.</p>
<div class="sourceCode" id="cb960"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb960-1"><a href="mlchapter.html#cb960-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the sizes of the test and training samples.</span></span>
<span id="cb960-2"><a href="mlchapter.html#cb960-2" aria-hidden="true" tabindex="-1"></a><span class="co"># We use 20 % of the data for testing:</span></span>
<span id="cb960-3"><a href="mlchapter.html#cb960-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(mtcars)</span>
<span id="cb960-4"><a href="mlchapter.html#cb960-4" aria-hidden="true" tabindex="-1"></a>ntest <span class="ot">&lt;-</span> <span class="fu">round</span>(<span class="fl">0.2</span><span class="sc">*</span>n)</span>
<span id="cb960-5"><a href="mlchapter.html#cb960-5" aria-hidden="true" tabindex="-1"></a>ntrain <span class="ot">&lt;-</span> n <span class="sc">-</span> ntest</span>
<span id="cb960-6"><a href="mlchapter.html#cb960-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb960-7"><a href="mlchapter.html#cb960-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into two sets:</span></span>
<span id="cb960-8"><a href="mlchapter.html#cb960-8" aria-hidden="true" tabindex="-1"></a>train_rows <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n, ntrain)</span>
<span id="cb960-9"><a href="mlchapter.html#cb960-9" aria-hidden="true" tabindex="-1"></a>mtcars_train <span class="ot">&lt;-</span> mtcars[train_rows,]</span>
<span id="cb960-10"><a href="mlchapter.html#cb960-10" aria-hidden="true" tabindex="-1"></a>mtcars_test <span class="ot">&lt;-</span> mtcars[<span class="sc">-</span>train_rows,]</span></code></pre></div>
<p>In this case, our training set consists of 26 observations and our test set of 6 observations. Let’s fit the model using the training set and use the test set for evaluation:</p>
<div class="sourceCode" id="cb961"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb961-1"><a href="mlchapter.html#cb961-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit model to training set:</span></span>
<span id="cb961-2"><a href="mlchapter.html#cb961-2" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> ., <span class="at">data =</span> mtcars_train)</span>
<span id="cb961-3"><a href="mlchapter.html#cb961-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb961-4"><a href="mlchapter.html#cb961-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate on test set:</span></span>
<span id="cb961-5"><a href="mlchapter.html#cb961-5" aria-hidden="true" tabindex="-1"></a>rmse <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((<span class="fu">predict</span>(m, mtcars_test) <span class="sc">-</span> mtcars_test<span class="sc">$</span>mpg)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb961-6"><a href="mlchapter.html#cb961-6" aria-hidden="true" tabindex="-1"></a>mae <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">abs</span>(<span class="fu">predict</span>(m, mtcars_test) <span class="sc">-</span> mtcars_test<span class="sc">$</span>mpg))</span>
<span id="cb961-7"><a href="mlchapter.html#cb961-7" aria-hidden="true" tabindex="-1"></a>rmse; mae</span></code></pre></div>
<p>Because of the small sample sizes here, the results can vary a lot if you rerun the two code chunks above several times (try it!). When I ran them ten times, the <span class="math inline">\(RMSE\)</span> varied between 1.8 and 7.6 - quite a difference on the scale of <code>mpg</code>! This problem is usually not as pronounced if you have larger sample sizes, but even for fairly large datasets, there can be a lot of variability depending on how the data happens to be split. It is not uncommon to get a “lucky” or “unlucky” test set that either overestimates or underestimates the model’s performance.</p>
<p>In general, I’d therefore recommend that you only use test-training splits of your data as a last resort (and only use it with sample sizes of 10,000 or more). Better tools are available in the form of the bootstrap and its darling cousin, cross-validation.</p>
</div>
<div id="loocv" class="section level3 hasAnchor" number="9.1.3">
<h3><span class="header-section-number">9.1.3</span> Leave-one-out cross-validation and <code>caret</code><a href="mlchapter.html#loocv" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The idea behind cross-validation is similar to that behind test-training splitting of the data. We partition the data into several sets, and use one of them for evaluation. The key difference is that we in a cross-validation partition the data into more than two sets, and use all of them (one-by-one) for evaluation.</p>
<p>To begin with, we split the data into <span class="math inline">\(k\)</span> sets, where <span class="math inline">\(k\)</span> is equal to or less than the number of observations <span class="math inline">\(n\)</span>. We then put the first set aside, to use for evaluation, and fit the model to the remaining <span class="math inline">\(k-1\)</span> sets. The model predictions are then evaluated on the first set. Next, we put the first set back among the others and remove the second set to use that for evaluation. And so on. This means that we fit <span class="math inline">\(k\)</span> models to <span class="math inline">\(k\)</span> different (albeit similar) training sets, and evaluate them on <span class="math inline">\(k\)</span> test sets (none of which are used for fitting the model that is evaluated on them).</p>
<p>The most basic form of cross-validation is leave-one-out cross-validation (LOOCV), where <span class="math inline">\(k=n\)</span> so that each observation is its own set. For each observation, we fit a model using all other observations, and then compare the prediction of that model to the actual value of the observation. We can do this using a <code>for</code> loop (Section <a href="progchapter.html#forloops">6.4.1</a>) as follows:</p>
<div class="sourceCode" id="cb962"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb962-1"><a href="mlchapter.html#cb962-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Leave-one-out cross-validation:</span></span>
<span id="cb962-2"><a href="mlchapter.html#cb962-2" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="st">&quot;numeric&quot;</span>, <span class="fu">nrow</span>(mtcars))</span>
<span id="cb962-3"><a href="mlchapter.html#cb962-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(mtcars))</span>
<span id="cb962-4"><a href="mlchapter.html#cb962-4" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb962-5"><a href="mlchapter.html#cb962-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Fit model to all observations except observation i:</span></span>
<span id="cb962-6"><a href="mlchapter.html#cb962-6" aria-hidden="true" tabindex="-1"></a>    m <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> ., <span class="at">data =</span> mtcars[<span class="sc">-</span>i,])</span>
<span id="cb962-7"><a href="mlchapter.html#cb962-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb962-8"><a href="mlchapter.html#cb962-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Make a prediction for observation i:</span></span>
<span id="cb962-9"><a href="mlchapter.html#cb962-9" aria-hidden="true" tabindex="-1"></a>    pred[i] <span class="ot">&lt;-</span> <span class="fu">predict</span>(m, mtcars[i,])</span>
<span id="cb962-10"><a href="mlchapter.html#cb962-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb962-11"><a href="mlchapter.html#cb962-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb962-12"><a href="mlchapter.html#cb962-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate predictions:</span></span>
<span id="cb962-13"><a href="mlchapter.html#cb962-13" aria-hidden="true" tabindex="-1"></a>rmse <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((pred <span class="sc">-</span> mtcars<span class="sc">$</span>mpg)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb962-14"><a href="mlchapter.html#cb962-14" aria-hidden="true" tabindex="-1"></a>mae <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">abs</span>(pred <span class="sc">-</span> mtcars<span class="sc">$</span>mpg))</span>
<span id="cb962-15"><a href="mlchapter.html#cb962-15" aria-hidden="true" tabindex="-1"></a>rmse; mae</span></code></pre></div>
<p>We will use cross-validation a lot, and so it is nice not to have to write a lot of code each time we want to do it. To that end, we’ll install the <code>caret</code> package, which not only lets us do cross-validation, but also acts as a wrapper for a large number of packages for predictive models. That means that we won’t have to learn a ton of functions to be able to fit different types of models. Instead, we just have to learn a few functions from <code>caret</code>. Let’s install the package and some of the packages it needs to function fully:</p>
<div class="sourceCode" id="cb963"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb963-1"><a href="mlchapter.html#cb963-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;caret&quot;</span>, <span class="at">dependencies =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p>Now, let’s see how we can use <code>caret</code> to fit a linear regression model and evaluate it using cross-validation. The two main functions used for this are <code>trainControl</code>, which we use to say that we want to perform a leave-one-out cross-validation (<code>method = "LOOCV"</code>) and <code>train</code>, where we state the model formula and specify that we want to use <code>lm</code> for fitting the model:</p>
<div class="sourceCode" id="cb964"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb964-1"><a href="mlchapter.html#cb964-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb964-2"><a href="mlchapter.html#cb964-2" aria-hidden="true" tabindex="-1"></a>tc <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;LOOCV&quot;</span>)</span>
<span id="cb964-3"><a href="mlchapter.html#cb964-3" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">train</span>(mpg <span class="sc">~</span> .,</span>
<span id="cb964-4"><a href="mlchapter.html#cb964-4" aria-hidden="true" tabindex="-1"></a>           <span class="at">data =</span> mtcars,</span>
<span id="cb964-5"><a href="mlchapter.html#cb964-5" aria-hidden="true" tabindex="-1"></a>           <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>,</span>
<span id="cb964-6"><a href="mlchapter.html#cb964-6" aria-hidden="true" tabindex="-1"></a>           <span class="at">trControl =</span> tc)</span></code></pre></div>
<p><code>train</code> has now done several things in parallel. First of all, it has fitted a linear model to the entire dataset. To see the results of the linear model we can use <code>summary</code>, just as if we’d fitted it with <code>lm</code>:</p>
<div class="sourceCode" id="cb965"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb965-1"><a href="mlchapter.html#cb965-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code></pre></div>
<p>Many, but not all, functions that we would apply to an object fitted using <code>lm</code> still work fine with a linear model fitted using <code>train</code>, including <code>predict</code>. Others, like <code>coef</code> and <code>confint</code> no longer work (or work differently) - but that is not that big a problem. We only use <code>train</code> when we are fitting a linear regression model with the intent of using it for prediction - and in such cases, we are typically not interested in the values of the model coefficients or their confidence intervals. If we need them, we can always refit the model using <code>lm</code>.</p>
<p>What makes <code>train</code> great is that <code>m</code> also contains information about the predictive performance of the model, computed, in this case, using leave-one-out cross-validation:</p>
<div class="sourceCode" id="cb966"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb966-1"><a href="mlchapter.html#cb966-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print a summary of the cross-validation:</span></span>
<span id="cb966-2"><a href="mlchapter.html#cb966-2" aria-hidden="true" tabindex="-1"></a>m</span>
<span id="cb966-3"><a href="mlchapter.html#cb966-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb966-4"><a href="mlchapter.html#cb966-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract the measures:</span></span>
<span id="cb966-5"><a href="mlchapter.html#cb966-5" aria-hidden="true" tabindex="-1"></a>m<span class="sc">$</span>results</span></code></pre></div>
<p><span class="math display">\[\sim\]</span></p>
<div class="exercise">
<p><span id="exr:ch8exc2" class="exercise"><strong>Exercise 9.2  </strong></span><a href="http://www.modernstatisticswithr.com/data.zip">Download the <code>estates.xlsx</code> data from the book’s web page</a>. It describes the selling prices (in thousands of SEK) of houses in and near Uppsala, Sweden, along with a number of variables describing the location, size, and standard of the house.</p>
<p>Fit a linear regression model to the data, with <code>selling_price</code> as the response variable and the remaining variables as explanatory variables. Perform an out-of-sample evaluation of your model. What are the <span class="math inline">\(RMSE\)</span> and <span class="math inline">\(MAE\)</span>? Do the prediction errors seem acceptable?</p>
</div>
<p><a href="solutions.html#ch8solutions2">(Click here to go to the solution.)</a></p>
</div>
<div id="kfoldcv" class="section level3 hasAnchor" number="9.1.4">
<h3><span class="header-section-number">9.1.4</span> k-fold cross-validation<a href="mlchapter.html#kfoldcv" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>LOOCV is a very good way of performing out-of-sample evaluation of your model. It can however become overoptimistic if you have “twinned” or duplicated data in your sample, i.e. observations that are identical or nearly identical (in which case the model for all intents and purposes already has “seen” the observation for which it is making a prediction). It can also be quite slow if you have a large dataset, as you need to fit <span class="math inline">\(n\)</span> different models, each using a lot of data.</p>
<p>A much faster option is <span class="math inline">\(k\)</span>-fold cross-validation, which is the name for cross-validation where <span class="math inline">\(k\)</span> is lower than <span class="math inline">\(n\)</span> - usually much lower, with <span class="math inline">\(k=10\)</span> being a common choice. To run a 10 fold cross-validation with <code>caret</code>, we change the arguments of <code>trainControl</code>, and then run <code>train</code> exactly as before:</p>
<div class="sourceCode" id="cb967"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb967-1"><a href="mlchapter.html#cb967-1" aria-hidden="true" tabindex="-1"></a>tc <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span> , <span class="at">number =</span> <span class="dv">10</span>)</span>
<span id="cb967-2"><a href="mlchapter.html#cb967-2" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">train</span>(mpg <span class="sc">~</span> .,</span>
<span id="cb967-3"><a href="mlchapter.html#cb967-3" aria-hidden="true" tabindex="-1"></a>           <span class="at">data =</span> mtcars,</span>
<span id="cb967-4"><a href="mlchapter.html#cb967-4" aria-hidden="true" tabindex="-1"></a>           <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>,</span>
<span id="cb967-5"><a href="mlchapter.html#cb967-5" aria-hidden="true" tabindex="-1"></a>           <span class="at">trControl =</span> tc)</span>
<span id="cb967-6"><a href="mlchapter.html#cb967-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb967-7"><a href="mlchapter.html#cb967-7" aria-hidden="true" tabindex="-1"></a>m</span></code></pre></div>
<p>Like with test-training splitting, the results from a <span class="math inline">\(k\)</span>-fold cross-validation will vary each time it is run (unless <span class="math inline">\(k=n\)</span>). To reduce the variance of the estimates of the prediction error, we can repeat the cross-validation procedure multiple times, and average the errors from all runs. This is known as a repeated <span class="math inline">\(k\)</span>-fold cross-validation. To run 100 10-fold cross-validations, we change the settings in <code>trainControl</code> as follows:</p>
<div class="sourceCode" id="cb968"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb968-1"><a href="mlchapter.html#cb968-1" aria-hidden="true" tabindex="-1"></a>tc <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;repeatedcv&quot;</span>,</span>
<span id="cb968-2"><a href="mlchapter.html#cb968-2" aria-hidden="true" tabindex="-1"></a>                   <span class="at">number =</span> <span class="dv">10</span>, <span class="at">repeats =</span> <span class="dv">100</span>)</span>
<span id="cb968-3"><a href="mlchapter.html#cb968-3" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">train</span>(mpg <span class="sc">~</span> .,</span>
<span id="cb968-4"><a href="mlchapter.html#cb968-4" aria-hidden="true" tabindex="-1"></a>           <span class="at">data =</span> mtcars,</span>
<span id="cb968-5"><a href="mlchapter.html#cb968-5" aria-hidden="true" tabindex="-1"></a>           <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>,</span>
<span id="cb968-6"><a href="mlchapter.html#cb968-6" aria-hidden="true" tabindex="-1"></a>           <span class="at">trControl =</span> tc)</span>
<span id="cb968-7"><a href="mlchapter.html#cb968-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb968-8"><a href="mlchapter.html#cb968-8" aria-hidden="true" tabindex="-1"></a>m</span></code></pre></div>
<p>Repeated <span class="math inline">\(k\)</span>-fold cross-validations are more computer-intensive than simple <span class="math inline">\(k\)</span>-fold cross-validations, but in return the estimates of the average prediction error are much more stable.</p>
<p>Which type of cross-validation to use for different problems remains an open question. Several studies (e.g. Zhang &amp; Yang (2015), and the references therein) indicate that in most settings larger <span class="math inline">\(k\)</span> is better (with LOOCV being the best), but there are exceptions to this rule - e.g. when you have a lot of twinned data. This is in contrast to an older belief that a high <span class="math inline">\(k\)</span> leads to estimates with high variances, tracing its roots back to a largely unsubstantiated claim in Efron (1983), which you still can see repeated in many books. When <span class="math inline">\(n\)</span> is very large, the difference between different <span class="math inline">\(k\)</span> is typically negligible.</p>
<p>A downside to <span class="math inline">\(k\)</span>-fold cross-validation is that the model is fitted using <span class="math inline">\(\frac{k-1}{k}n\)</span> observations instead of <span class="math inline">\(n\)</span>. If <span class="math inline">\(n\)</span> is small, this can lead to models that are noticeably worse than the model fitted using <span class="math inline">\(n\)</span> observations. LOOCV is the best choice in such cases, as it uses <span class="math inline">\(n-1\)</span> observations (so, almost <span class="math inline">\(n\)</span>) when fitting the models. On the other hand, there is also the computational aspect - LOOCV is simply not computationally feasible for large datasets with numerically complex models. In summary, my recommendation is to use LOOCV when possible, particularly for smaller datasets, and to use repeated 10-fold cross-validation otherwise. For very large datasets, or toy examples, you can resort to a simple 10-fold cross-validation (which still is a better option than test-training splitting).</p>
<p><span class="math display">\[\sim\]</span></p>
<div class="exercise">
<p><span id="exr:ch8exc3" class="exercise"><strong>Exercise 9.3  </strong></span>Return to the <code>estates.xlsx</code> data from the previous exercise. Refit your linear model, but this time:</p>
<ol style="list-style-type: decimal">
<li><p>Use 10-fold cross-validation for the evaluation. Run it several times and check the MAE. How much does the MAE vary between runs?</p></li>
<li><p>Run repeated 10-fold cross-validations a few times. How much does the MAE vary between runs?</p></li>
</ol>
</div>
<p><a href="solutions.html#ch8solutions3">(Click here to go to the solution.)</a></p>
</div>
<div id="twinned-observations" class="section level3 hasAnchor" number="9.1.5">
<h3><span class="header-section-number">9.1.5</span> Twinned observations<a href="mlchapter.html#twinned-observations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If you want to use LOOCV but are concerned about twinned observations, you can use <code>duplicated</code>, which returns a <code>logical</code> vector showing which rows are duplicates of previous rows. It will however not find near-duplicates. Let’s try it on the <code>diamonds</code> data from <code>ggplot2</code>:</p>
<div class="sourceCode" id="cb969"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb969-1"><a href="mlchapter.html#cb969-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb969-2"><a href="mlchapter.html#cb969-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Are there twinned observations?</span></span>
<span id="cb969-3"><a href="mlchapter.html#cb969-3" aria-hidden="true" tabindex="-1"></a><span class="fu">duplicated</span>(diamonds)</span>
<span id="cb969-4"><a href="mlchapter.html#cb969-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb969-5"><a href="mlchapter.html#cb969-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Count the number of duplicates:</span></span>
<span id="cb969-6"><a href="mlchapter.html#cb969-6" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">duplicated</span>(diamonds))</span>
<span id="cb969-7"><a href="mlchapter.html#cb969-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb969-8"><a href="mlchapter.html#cb969-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the duplicates:</span></span>
<span id="cb969-9"><a href="mlchapter.html#cb969-9" aria-hidden="true" tabindex="-1"></a>diamonds[<span class="fu">which</span>(<span class="fu">duplicated</span>(diamonds)),]</span></code></pre></div>
<p>If you plan on using LOOCV, you may want to remove duplicates. We saw how to do this in Section <a href="messychapter.html#conditions2">5.8.2</a>:</p>
<table>
<div class="column-left">
<p>With <code>data.table</code>:</p>
<div class="sourceCode" id="cb970"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb970-1"><a href="mlchapter.html#cb970-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(data.table)</span>
<span id="cb970-2"><a href="mlchapter.html#cb970-2" aria-hidden="true" tabindex="-1"></a>diamonds <span class="ot">&lt;-</span> <span class="fu">as.data.table</span>(diamonds)</span>
<span id="cb970-3"><a href="mlchapter.html#cb970-3" aria-hidden="true" tabindex="-1"></a><span class="fu">unique</span>(diamonds)</span></code></pre></div>
</div>
<div class="column-right">
<p>With <code>dplyr</code>:</p>
<div class="sourceCode" id="cb971"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb971-1"><a href="mlchapter.html#cb971-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb971-2"><a href="mlchapter.html#cb971-2" aria-hidden="true" tabindex="-1"></a>diamonds <span class="sc">%&gt;%</span> distinct</span></code></pre></div>
</div>
</table>
</div>
<div id="bootstrapping" class="section level3 hasAnchor" number="9.1.6">
<h3><span class="header-section-number">9.1.6</span> Bootstrapping<a href="mlchapter.html#bootstrapping" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>An alternative to cross-validation is to draw bootstrap samples, some of which are used to fit models, and some to evaluate them. This has the benefit that the models are fitted to <span class="math inline">\(n\)</span> observations instead of <span class="math inline">\(\frac{k-1}{k}n\)</span> observations. This is in fact the default method in <code>trainControl</code>. To use it for our <code>mtcars</code> model, with 999 bootstrap samples, we run the following:</p>
<div class="sourceCode" id="cb972"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb972-1"><a href="mlchapter.html#cb972-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb972-2"><a href="mlchapter.html#cb972-2" aria-hidden="true" tabindex="-1"></a>tc <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;boot&quot;</span>,</span>
<span id="cb972-3"><a href="mlchapter.html#cb972-3" aria-hidden="true" tabindex="-1"></a>                   <span class="at">number =</span> <span class="dv">999</span>)</span>
<span id="cb972-4"><a href="mlchapter.html#cb972-4" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">train</span>(mpg <span class="sc">~</span> .,</span>
<span id="cb972-5"><a href="mlchapter.html#cb972-5" aria-hidden="true" tabindex="-1"></a>           <span class="at">data =</span> mtcars,</span>
<span id="cb972-6"><a href="mlchapter.html#cb972-6" aria-hidden="true" tabindex="-1"></a>           <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>,</span>
<span id="cb972-7"><a href="mlchapter.html#cb972-7" aria-hidden="true" tabindex="-1"></a>           <span class="at">trControl =</span> tc)</span>
<span id="cb972-8"><a href="mlchapter.html#cb972-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb972-9"><a href="mlchapter.html#cb972-9" aria-hidden="true" tabindex="-1"></a>m</span>
<span id="cb972-10"><a href="mlchapter.html#cb972-10" aria-hidden="true" tabindex="-1"></a>m<span class="sc">$</span>results</span></code></pre></div>
<p><span class="math display">\[\sim\]</span></p>
<div class="exercise">
<p><span id="exr:ch8exc4" class="exercise"><strong>Exercise 9.4  </strong></span>Return to the <code>estates.xlsx</code> data from the previous exercise. Refit your linear model, but this time use the bootstrap to evaluate the model. Run it several times and check the MAE. How much does the MAE vary between runs?</p>
</div>
<p><a href="solutions.html#ch8solutions4">(Click here to go to the solution.)</a></p>
</div>
<div id="classifieraccuracy" class="section level3 hasAnchor" number="9.1.7">
<h3><span class="header-section-number">9.1.7</span> Evaluating classification models<a href="mlchapter.html#classifieraccuracy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Classification models, or classifiers, differ from regression model in that they aim to predict which <em>class</em> (category) an observation belongs to, rather than to predict a number. Because the target variable, the class, is categorical, it would make little sense to use measures like <span class="math inline">\(RMSE\)</span> and <span class="math inline">\(MAE\)</span> to evaluate the performance of a classifier. Instead, we will use other measures that are better suited to this type of problem.</p>
<p>To begin with, though, we’ll revisit the <code>wine</code> data that we studied in Section <a href="regression.html#logreg">8.3.1</a>. It contains characteristics of wines that belong to either of two classes: white and red. Let’s create the dataset:</p>
<div class="sourceCode" id="cb973"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb973-1"><a href="mlchapter.html#cb973-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import data about white and red wines:</span></span>
<span id="cb973-2"><a href="mlchapter.html#cb973-2" aria-hidden="true" tabindex="-1"></a>white <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;https://tinyurl.com/winedata1&quot;</span>,</span>
<span id="cb973-3"><a href="mlchapter.html#cb973-3" aria-hidden="true" tabindex="-1"></a>                  <span class="at">sep =</span> <span class="st">&quot;;&quot;</span>)</span>
<span id="cb973-4"><a href="mlchapter.html#cb973-4" aria-hidden="true" tabindex="-1"></a>red <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;https://tinyurl.com/winedata2&quot;</span>,</span>
<span id="cb973-5"><a href="mlchapter.html#cb973-5" aria-hidden="true" tabindex="-1"></a>                  <span class="at">sep =</span> <span class="st">&quot;;&quot;</span>)</span>
<span id="cb973-6"><a href="mlchapter.html#cb973-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb973-7"><a href="mlchapter.html#cb973-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Add a type variable:</span></span>
<span id="cb973-8"><a href="mlchapter.html#cb973-8" aria-hidden="true" tabindex="-1"></a>white<span class="sc">$</span>type <span class="ot">&lt;-</span> <span class="st">&quot;white&quot;</span></span>
<span id="cb973-9"><a href="mlchapter.html#cb973-9" aria-hidden="true" tabindex="-1"></a>red<span class="sc">$</span>type <span class="ot">&lt;-</span> <span class="st">&quot;red&quot;</span></span>
<span id="cb973-10"><a href="mlchapter.html#cb973-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb973-11"><a href="mlchapter.html#cb973-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Merge the datasets:</span></span>
<span id="cb973-12"><a href="mlchapter.html#cb973-12" aria-hidden="true" tabindex="-1"></a>wine <span class="ot">&lt;-</span> <span class="fu">rbind</span>(white, red)</span>
<span id="cb973-13"><a href="mlchapter.html#cb973-13" aria-hidden="true" tabindex="-1"></a>wine<span class="sc">$</span>type <span class="ot">&lt;-</span> <span class="fu">factor</span>(wine<span class="sc">$</span>type)</span>
<span id="cb973-14"><a href="mlchapter.html#cb973-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb973-15"><a href="mlchapter.html#cb973-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Check the result:</span></span>
<span id="cb973-16"><a href="mlchapter.html#cb973-16" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(wine)</span></code></pre></div>
<p>In Section <a href="regression.html#logreg">8.3.1</a>, we fitted a logistic regression model to the data using <code>glm</code>:</p>
<div class="sourceCode" id="cb974"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb974-1"><a href="mlchapter.html#cb974-1" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">glm</span>(type <span class="sc">~</span> pH <span class="sc">+</span> alcohol, <span class="at">data =</span> wine, <span class="at">family =</span> binomial)</span>
<span id="cb974-2"><a href="mlchapter.html#cb974-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code></pre></div>
<p>Logistic regression models are regression models, because they give us a numeric output: class probabilities. These probabilities can however be used for classification - we can for instance classify a wine as being red if the predicted probability that it is red is at least 0.5. We can therefore use logistic regression as a classifier, and refer to it as such, although we should bear in mind that it actually is more than that<a href="#fn58" class="footnote-ref" id="fnref58"><sup>58</sup></a>.</p>
<p>We can use <code>caret</code> and <code>train</code> to fit the same a logistic regression model, and use cross-validation or the bootstrap to evaluate it. We should supply the arguments <code>method = "glm"</code> and <code>family = "binomial"</code> to <code>train</code> to specify that we want a logistic regression model. Let’s do that, and run a repeated 10-fold cross-validation of the model - this takes longer to run than our <code>mtcars</code> example because the dataset is larger:</p>
<div class="sourceCode" id="cb975"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb975-1"><a href="mlchapter.html#cb975-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb975-2"><a href="mlchapter.html#cb975-2" aria-hidden="true" tabindex="-1"></a>tc <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;repeatedcv&quot;</span>,</span>
<span id="cb975-3"><a href="mlchapter.html#cb975-3" aria-hidden="true" tabindex="-1"></a>                   <span class="at">number =</span> <span class="dv">10</span>, <span class="at">repeats =</span> <span class="dv">100</span>)</span>
<span id="cb975-4"><a href="mlchapter.html#cb975-4" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">train</span>(type <span class="sc">~</span> pH <span class="sc">+</span> alcohol,</span>
<span id="cb975-5"><a href="mlchapter.html#cb975-5" aria-hidden="true" tabindex="-1"></a>           <span class="at">data =</span> wine,</span>
<span id="cb975-6"><a href="mlchapter.html#cb975-6" aria-hidden="true" tabindex="-1"></a>           <span class="at">trControl =</span> tc,</span>
<span id="cb975-7"><a href="mlchapter.html#cb975-7" aria-hidden="true" tabindex="-1"></a>           <span class="at">method =</span> <span class="st">&quot;glm&quot;</span>,</span>
<span id="cb975-8"><a href="mlchapter.html#cb975-8" aria-hidden="true" tabindex="-1"></a>           <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb975-9"><a href="mlchapter.html#cb975-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb975-10"><a href="mlchapter.html#cb975-10" aria-hidden="true" tabindex="-1"></a>m</span></code></pre></div>
<p>The summary reports two figures from the cross-validation:</p>
<ul>
<li><em>Accuracy</em>: the proportion of correctly classified observations,</li>
<li><em>Cohen’s kappa</em>: a measure combining the observed accuracy with the accuracy expected under random guessing (which is related to the balance between the two classes in the sample).</li>
</ul>
<p>We mentioned a little earlier that we can use logistic regression for classification by, for instance, classifying a wine as being red if the predicted probability that it is red is at least 0.5. It is of course possible to use another threshold as well, and classify wines as being red if the probability is at least 0.2, or 0.3333, or 0.62. When setting this threshold, there is a tradeoff between the occurrence of what is known as <em>false negatives</em> and <em>false positives</em>. Imagine that we have two classes (white and red), and that we label one of them as negative (white) and one as positive (red). Then:</p>
<ul>
<li>A <em>false negative</em> is a positive (red) observation incorrectly classified as negative (white),</li>
<li>A <em>false positive</em> is a negative (white) observation incorrectly classified as positive (red).</li>
</ul>
<p>In the <code>wine</code> example, there is little difference between these types of errors. But in other examples, the distinction is an important one. Imagine for instance that we, based on some data, want to classify patients as being sick (positive) or healthy (negative). In that case it might be much worse to get a false negative (the patient won’t get the treatment that they need) than a false positive (which just means that the patient will have to run a few more tests). For any given threshold, we can compute two measures of the frequency of these types of errors:</p>
<ul>
<li><em>Sensitivity</em> or <em>true positive rate</em>: the proportion of positive observations that are correctly classified as being positive,</li>
<li><em>Specificity</em> or <em>true negative rate</em>: the proportion of negative observations that are correctly classified as being negative.</li>
</ul>
<p>If we increase the threshold for at what probability a wine is classified as being red (positive), then the sensitivity will increase, but the specificity will decrease. And if we lower the threshold, the sensitivity will decrease while the specificity increases.</p>
<p>It would make sense to try several different thresholds, to see for which threshold we get a good compromise between sensitivity and specificity. We will use the <code>MLeval</code> package to visualise the result of this comparison, so let’s install that:</p>
<div class="sourceCode" id="cb976"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb976-1"><a href="mlchapter.html#cb976-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;MLeval&quot;</span>)</span></code></pre></div>
<p>Sensitivity and specificity are usually visualised using receiver operation characteristic curves, or <em>ROC curves</em> for short. We’ll plot such a curve for our <code>wine</code> model. The function <code>evalm</code> from <code>MLeval</code> can be used to collect the data that we need from the cross-validations of a model <code>m</code> created using <code>train</code>. To use it, we need to set <code>savePredictions = TRUE</code> and <code>classProbs = TRUE</code> in <code>trainControl</code>:</p>
<div class="sourceCode" id="cb977"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb977-1"><a href="mlchapter.html#cb977-1" aria-hidden="true" tabindex="-1"></a>tc <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;repeatedcv&quot;</span>,</span>
<span id="cb977-2"><a href="mlchapter.html#cb977-2" aria-hidden="true" tabindex="-1"></a>                   <span class="at">number =</span> <span class="dv">10</span>, <span class="at">repeats =</span> <span class="dv">100</span>,</span>
<span id="cb977-3"><a href="mlchapter.html#cb977-3" aria-hidden="true" tabindex="-1"></a>                   <span class="at">savePredictions =</span> <span class="cn">TRUE</span>,</span>
<span id="cb977-4"><a href="mlchapter.html#cb977-4" aria-hidden="true" tabindex="-1"></a>                   <span class="at">classProbs =</span> <span class="cn">TRUE</span>)</span>
<span id="cb977-5"><a href="mlchapter.html#cb977-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb977-6"><a href="mlchapter.html#cb977-6" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">train</span>(type <span class="sc">~</span> pH <span class="sc">+</span> alcohol,</span>
<span id="cb977-7"><a href="mlchapter.html#cb977-7" aria-hidden="true" tabindex="-1"></a>           <span class="at">data =</span> wine,</span>
<span id="cb977-8"><a href="mlchapter.html#cb977-8" aria-hidden="true" tabindex="-1"></a>           <span class="at">trControl =</span> tc,</span>
<span id="cb977-9"><a href="mlchapter.html#cb977-9" aria-hidden="true" tabindex="-1"></a>           <span class="at">method =</span> <span class="st">&quot;glm&quot;</span>,</span>
<span id="cb977-10"><a href="mlchapter.html#cb977-10" aria-hidden="true" tabindex="-1"></a>           <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb977-11"><a href="mlchapter.html#cb977-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb977-12"><a href="mlchapter.html#cb977-12" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MLeval)</span>
<span id="cb977-13"><a href="mlchapter.html#cb977-13" aria-hidden="true" tabindex="-1"></a>plots <span class="ot">&lt;-</span> <span class="fu">evalm</span>(m)</span>
<span id="cb977-14"><a href="mlchapter.html#cb977-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb977-15"><a href="mlchapter.html#cb977-15" aria-hidden="true" tabindex="-1"></a><span class="co"># ROC:</span></span>
<span id="cb977-16"><a href="mlchapter.html#cb977-16" aria-hidden="true" tabindex="-1"></a>plots<span class="sc">$</span>roc</span></code></pre></div>
<p>The x-axis shows the <em>false positive rate</em> of the classifier (which is 1 minus the specificity - we’d like this to be as low as possible) and the y-axis shows the corresponding sensitivity of the classifier (we’d like this to be as high as possible). The red line shows the false positive rate and sensitivity of our classifier, which each point on the line corresponding to a different threshold. The grey line shows the performance of a classifier that is no better than random guessing - ideally, we want the red line to be much higher than that.</p>
<p>The beauty of the ROC curve is that it gives us a visual summary of how the classifier performs for all possible thresholds. It is instrumental if we want to compare two or more classifiers, as you will do in Exercise <a href="mlchapter.html#exr:ch8exc5">9.5</a>.</p>
<p>The legend shows a summary measure, <span class="math inline">\(AUC\)</span>, the area under the ROC curve. An <span class="math inline">\(AUC\)</span> of 0.5 means that the classifier is no better than random guessing, and an <span class="math inline">\(AUC\)</span> of 1 means that the model always makes correct predictions for all thresholds. Getting an <span class="math inline">\(AUC\)</span> that is lower than 0.5, meaning that the classifier is <em>worse</em> than random guessing, is exceedingly rare, and can be a sign of some error in the model fitting.</p>
<p><code>evalm</code> also computes a 95 % confidence interval for the <span class="math inline">\(AUC\)</span>, which can be obtained as follows:</p>
<div class="sourceCode" id="cb978"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb978-1"><a href="mlchapter.html#cb978-1" aria-hidden="true" tabindex="-1"></a>plots<span class="sc">$</span>optres[[<span class="dv">1</span>]][<span class="dv">13</span>,]</span></code></pre></div>
<p>Another very important plot provided by <code>evalm</code> is the <em>calibration curve</em>. It shows how well-calibrated the model is. If the model is well-calibrated, then the predicted probabilities should be close to the true frequencies. As an example, this means that among wines for which the predicted probability of the wine being red is about 20 %, 20 % should actually be red. For a well-calibrated model, the red curve should closely follow the grey line in the plot:</p>
<div class="sourceCode" id="cb979"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb979-1"><a href="mlchapter.html#cb979-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calibration curve:</span></span>
<span id="cb979-2"><a href="mlchapter.html#cb979-2" aria-hidden="true" tabindex="-1"></a>plots<span class="sc">$</span>cc</span></code></pre></div>
<p>Our model doesn’t appear to be that well-calibrated, meaning that we can’t really trust its predicted probabilities.</p>
<p>If we just want to quickly print the <span class="math inline">\(AUC\)</span> without plotting the ROC curves, we can set <code>summaryFunction = twoClassSummary</code> in <code>trainControl</code>, after which the <span class="math inline">\(AUC\)</span> will be printed instead of accuracy and Cohen’s kappa (although it is erroneously called ROC instead of <span class="math inline">\(AUC\)</span>). The sensitivity and specificity for the 0.5 threshold are also printed:</p>
<div class="sourceCode" id="cb980"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb980-1"><a href="mlchapter.html#cb980-1" aria-hidden="true" tabindex="-1"></a>tc <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;repeatedcv&quot;</span>,</span>
<span id="cb980-2"><a href="mlchapter.html#cb980-2" aria-hidden="true" tabindex="-1"></a>                   <span class="at">number =</span> <span class="dv">10</span>, <span class="at">repeats =</span> <span class="dv">100</span>,</span>
<span id="cb980-3"><a href="mlchapter.html#cb980-3" aria-hidden="true" tabindex="-1"></a>                   <span class="at">summaryFunction =</span> twoClassSummary,</span>
<span id="cb980-4"><a href="mlchapter.html#cb980-4" aria-hidden="true" tabindex="-1"></a>                   <span class="at">savePredictions =</span> <span class="cn">TRUE</span>,</span>
<span id="cb980-5"><a href="mlchapter.html#cb980-5" aria-hidden="true" tabindex="-1"></a>                   <span class="at">classProbs =</span> <span class="cn">TRUE</span>)</span>
<span id="cb980-6"><a href="mlchapter.html#cb980-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb980-7"><a href="mlchapter.html#cb980-7" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">train</span>(type <span class="sc">~</span> pH <span class="sc">+</span> alcohol,</span>
<span id="cb980-8"><a href="mlchapter.html#cb980-8" aria-hidden="true" tabindex="-1"></a>           <span class="at">data =</span> wine,</span>
<span id="cb980-9"><a href="mlchapter.html#cb980-9" aria-hidden="true" tabindex="-1"></a>           <span class="at">trControl =</span> tc,</span>
<span id="cb980-10"><a href="mlchapter.html#cb980-10" aria-hidden="true" tabindex="-1"></a>           <span class="at">method =</span> <span class="st">&quot;glm&quot;</span>,</span>
<span id="cb980-11"><a href="mlchapter.html#cb980-11" aria-hidden="true" tabindex="-1"></a>           <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>,</span>
<span id="cb980-12"><a href="mlchapter.html#cb980-12" aria-hidden="true" tabindex="-1"></a>           <span class="at">metric =</span> <span class="st">&quot;ROC&quot;</span>)</span>
<span id="cb980-13"><a href="mlchapter.html#cb980-13" aria-hidden="true" tabindex="-1"></a>m</span></code></pre></div>
<p><span class="math display">\[\sim\]</span></p>
<div class="exercise">
<p><span id="exr:ch8exc5" class="exercise"><strong>Exercise 9.5  </strong></span>Fit a second logistic regression model, <code>m2</code>, to the <code>wine</code> data, that also includes <code>fixed.acidity</code> and <code>residual.sugar</code> as explanatory variables. You can then run</p>
</div>
<div class="sourceCode" id="cb981"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb981-1"><a href="mlchapter.html#cb981-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MLeval)</span>
<span id="cb981-2"><a href="mlchapter.html#cb981-2" aria-hidden="true" tabindex="-1"></a>plots <span class="ot">&lt;-</span> <span class="fu">evalm</span>(<span class="fu">list</span>(m, m2),</span>
<span id="cb981-3"><a href="mlchapter.html#cb981-3" aria-hidden="true" tabindex="-1"></a>               <span class="at">gnames =</span> <span class="fu">c</span>(<span class="st">&quot;Model 1&quot;</span>, <span class="st">&quot;Model 2&quot;</span>))</span></code></pre></div>
<p>to create ROC curves and calibration plots for both models. Compare their curves. Is the new model better than the simpler model?</p>
<p><a href="solutions.html#ch8solutions5">(Click here to go to the solution.)</a></p>
</div>
<div id="decisionboundaries" class="section level3 hasAnchor" number="9.1.8">
<h3><span class="header-section-number">9.1.8</span> Visualising decision boundaries<a href="mlchapter.html#decisionboundaries" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For models with two explanatory variables, the <em>decision boundaries</em> of a classifier can easily be visualised. These show the different regions of the sample space that the classifier associates with the different classes. Let’s look at an example of this using the model <code>m</code> fitted to the <code>wine</code> data at the end of the previous section. We’ll create a grid of points using <code>expand.grid</code> and make predictions for each of them (i.e. classify each of them). We can then use <code>geom_contour</code> to draw the decision boundaries:</p>
<div class="sourceCode" id="cb982"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb982-1"><a href="mlchapter.html#cb982-1" aria-hidden="true" tabindex="-1"></a>contour_data <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(</span>
<span id="cb982-2"><a href="mlchapter.html#cb982-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">pH =</span> <span class="fu">seq</span>(<span class="fu">min</span>(wine<span class="sc">$</span>pH), <span class="fu">max</span>(wine<span class="sc">$</span>pH), <span class="at">length =</span> <span class="dv">500</span>),</span>
<span id="cb982-3"><a href="mlchapter.html#cb982-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">alcohol =</span> <span class="fu">seq</span>(<span class="fu">min</span>(wine<span class="sc">$</span>alcohol), <span class="fu">max</span>(wine<span class="sc">$</span>alcohol), <span class="at">length =</span> <span class="dv">500</span>))</span>
<span id="cb982-4"><a href="mlchapter.html#cb982-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb982-5"><a href="mlchapter.html#cb982-5" aria-hidden="true" tabindex="-1"></a>predictions <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(contour_data,</span>
<span id="cb982-6"><a href="mlchapter.html#cb982-6" aria-hidden="true" tabindex="-1"></a>                          <span class="at">type =</span> <span class="fu">as.numeric</span>(<span class="fu">predict</span>(m, contour_data)))</span>
<span id="cb982-7"><a href="mlchapter.html#cb982-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb982-8"><a href="mlchapter.html#cb982-8" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb982-9"><a href="mlchapter.html#cb982-9" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(wine, <span class="fu">aes</span>(pH, alcohol, <span class="at">colour =</span> type)) <span class="sc">+</span></span>
<span id="cb982-10"><a href="mlchapter.html#cb982-10" aria-hidden="true" tabindex="-1"></a>      <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb982-11"><a href="mlchapter.html#cb982-11" aria-hidden="true" tabindex="-1"></a>      <span class="fu">stat_contour</span>(<span class="fu">aes</span>(<span class="at">x =</span> pH, <span class="at">y =</span> alcohol, <span class="at">z =</span> type),</span>
<span id="cb982-12"><a href="mlchapter.html#cb982-12" aria-hidden="true" tabindex="-1"></a>                   <span class="at">data =</span> predictions, <span class="at">colour =</span> <span class="st">&quot;black&quot;</span>)</span></code></pre></div>
<p>In this case, points to the left of the black line are classified as white, and points to the right of the line are classified as red. It is clear from the plot (both from the point clouds and from the decision boundaries) that the model won’t work very well, as many wines will be misclassified.</p>
</div>
</div>
<div id="ethical-issues-in-predictive-modelling" class="section level2 hasAnchor" number="9.2">
<h2><span class="header-section-number">9.2</span> Ethical issues in predictive modelling<a href="mlchapter.html#ethical-issues-in-predictive-modelling" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Even when they are used for the best of intents, predictive models can inadvertently create injustice and bias, and lead to discrimination. This is particularly so for models that, in one way or another, make predictions about people. Real-world examples include facial recognition systems that perform worse for people with darker skin (Buolamwini &amp; Gebru, 2018) and recruitment models that are biased against women (Dastin, 2018).</p>
<p>A common issue that can cause this type of problem is difficult-to-spot biases in the training data. If female applicants have been less likely to get a job at a company in the past, then a recruitment model built on data from that company will likely also become biased against women. It can be problematic to simply take data from the past and to consider it as the “ground-truth” when building models.</p>
<p>Similarly, predictive models can create situations where people are prevented from improving their circumstances, and for instance are stopped from getting out of poverty because they are poor. As an example, if people from a certain (poor) zip code historically often have defaulted on their loans, then a predictive model determining who should be granted a student loan may reject an applicant from that area solely on those grounds, even though they otherwise might be an ideal candidate for a loan (which would have allowed them to get an education and a better-paid job). Finally, in extreme cases, predictive models can be used by authoritarian governments to track and target dissidents in a bid to block democracy and human rights.</p>
<p>When working on a predictive model, you should always keep these risks in mind, and ask yourself some questions. How will your model be used, and by whom? Are there hidden biases in the training data? Are the predictions good enough, and if they aren’t, what could be the consequences for people who get erroneous predictions? Are the predictions good enough for all groups of people, or does the model have worse performance for some groups? Will the predictions improve fairness or cement structural unfairness that was implicitly incorporated in the training data?</p>
<p><span class="math display">\[\sim\]</span></p>
<div class="exercise">
<p><span id="exr:ch9ethics1" class="exercise"><strong>Exercise 9.6  </strong></span><em>Discuss the following.</em> You are working for a company that tracks the behaviour of online users using cookies. The users have all agreed to be tracked by clicking on an “Accept all cookies” button, but most can be expected not to have read the terms and conditions involved. You analyse information from the cookies, consisting of data about more or less all parts of the users’ digital lives, to serve targeted ads to the users. Is this acceptable? Does the accuracy of your targeting models affect your answer? What if the ads are relevant to the user 99 % of the time? What if they only are relevant 1 % of the time?</p>
</div>
<p><br></p>
<div class="exercise">
<p><span id="exr:ch9ethics2" class="exercise"><strong>Exercise 9.7  </strong></span><em>Discuss the following.</em> You work for a company that has developed a facial recognition system. In a final trial before releasing your product, you discover that your system performs poorly for people over the age of 70 (the accuracy is 99 % for people below 70 and 65 % for people above 70). Should you release your system without making any changes to it? Does your answer depend on how it will be used? What if it is used instead of keycards to access offices? What if it is used to unlock smartphones? What if it is used for ID controls at voting stations? What if it is used for payments?</p>
</div>
<p><br></p>
<div class="exercise">
<p><span id="exr:ch9ethics3" class="exercise"><strong>Exercise 9.8  </strong></span><em>Discuss the following.</em> Imagine a model that predicts how likely it is that a suspect committed a crime that they are accused of, and that said model is used in courts of law. The model is described as being faster, fairer, and more impartial than human judges. It is a highly complex black-box machine learning model built on data from previous trials. It uses hundreds of variables, and so it isn’t possible to explain why it gives a particular prediction for a specific individual. The model makes correct predictions 99 % of the time. Is using such a model in the judicial system acceptable? What if an innocent person is predicted by the model to be guilty, without an explanation of why it found them to be guilty? What if the model makes correct predictions 90 % or 99.99 % of the time? Are there things that the model shouldn’t be allowed to take into account, such as skin colour or income? If so, how can you make sure that such variables aren’t implicitly incorporated into the training data?</p>
</div>
</div>
<div id="modellingchallenges" class="section level2 hasAnchor" number="9.3">
<h2><span class="header-section-number">9.3</span> Challenges in predictive modelling<a href="mlchapter.html#modellingchallenges" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There are a number of challenges that often come up in predictive modelling projects. In this section we’ll briefly discuss some of them.</p>
<div id="imbalanced" class="section level3 hasAnchor" number="9.3.1">
<h3><span class="header-section-number">9.3.1</span> Handling class imbalance<a href="mlchapter.html#imbalanced" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Imbalanced data, where the proportions of different classes differ a lot, are common in practice. In some areas, such as the study of rare diseases, such datasets are inherent to the field. Class imbalance can cause problems for many classifiers, as they tend to become prone to classify too many observations as belonging to the more common class.</p>
<p>One way to mitigate this problem is to use <em>down-sampling</em> and <em>up-sampling</em> when fitting the model. In down-sampling, only a (random) subset of the observations from the larger class are used for fitting the model, so that the number of cases from each class becomes balanced. In up-sampling the number of observations in the smaller class are artificially increased by resampling, also to achieve balance. These methods are only used when fitting the model, to avoid problems with the model overfitting to the class imbalance.</p>
<p>To illustrate the need and use for these methods, let’s create a more imbalanced version of the <code>wine</code> data:</p>
<div class="sourceCode" id="cb983"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb983-1"><a href="mlchapter.html#cb983-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create imbalanced wine data:</span></span>
<span id="cb983-2"><a href="mlchapter.html#cb983-2" aria-hidden="true" tabindex="-1"></a>wine_imb <span class="ot">&lt;-</span> wine[<span class="dv">1</span><span class="sc">:</span><span class="dv">5000</span>,]</span>
<span id="cb983-3"><a href="mlchapter.html#cb983-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb983-4"><a href="mlchapter.html#cb983-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Check class balance:</span></span>
<span id="cb983-5"><a href="mlchapter.html#cb983-5" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(wine_imb<span class="sc">$</span>type)</span></code></pre></div>
<p>Next, we fit three logistic models - one the usual way, one with down-sampling and one with up-sampling. We’ll use 10-fold cross-validation to evaluate their performance.</p>
<div class="sourceCode" id="cb984"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb984-1"><a href="mlchapter.html#cb984-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb984-2"><a href="mlchapter.html#cb984-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb984-3"><a href="mlchapter.html#cb984-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a model the usual way:</span></span>
<span id="cb984-4"><a href="mlchapter.html#cb984-4" aria-hidden="true" tabindex="-1"></a>tc <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span> , <span class="at">number =</span> <span class="dv">10</span>,</span>
<span id="cb984-5"><a href="mlchapter.html#cb984-5" aria-hidden="true" tabindex="-1"></a>                   <span class="at">savePredictions =</span> <span class="cn">TRUE</span>,</span>
<span id="cb984-6"><a href="mlchapter.html#cb984-6" aria-hidden="true" tabindex="-1"></a>                   <span class="at">classProbs =</span> <span class="cn">TRUE</span>)</span>
<span id="cb984-7"><a href="mlchapter.html#cb984-7" aria-hidden="true" tabindex="-1"></a>m1 <span class="ot">&lt;-</span> <span class="fu">train</span>(type <span class="sc">~</span> pH <span class="sc">+</span> alcohol,</span>
<span id="cb984-8"><a href="mlchapter.html#cb984-8" aria-hidden="true" tabindex="-1"></a>            <span class="at">data =</span> wine_imb,</span>
<span id="cb984-9"><a href="mlchapter.html#cb984-9" aria-hidden="true" tabindex="-1"></a>            <span class="at">trControl =</span> tc,</span>
<span id="cb984-10"><a href="mlchapter.html#cb984-10" aria-hidden="true" tabindex="-1"></a>            <span class="at">method =</span> <span class="st">&quot;glm&quot;</span>,</span>
<span id="cb984-11"><a href="mlchapter.html#cb984-11" aria-hidden="true" tabindex="-1"></a>            <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb984-12"><a href="mlchapter.html#cb984-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb984-13"><a href="mlchapter.html#cb984-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit with down-sampling:</span></span>
<span id="cb984-14"><a href="mlchapter.html#cb984-14" aria-hidden="true" tabindex="-1"></a>tc <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span> , <span class="at">number =</span> <span class="dv">10</span>,</span>
<span id="cb984-15"><a href="mlchapter.html#cb984-15" aria-hidden="true" tabindex="-1"></a>                   <span class="at">savePredictions =</span> <span class="cn">TRUE</span>,</span>
<span id="cb984-16"><a href="mlchapter.html#cb984-16" aria-hidden="true" tabindex="-1"></a>                   <span class="at">classProbs =</span> <span class="cn">TRUE</span>,</span>
<span id="cb984-17"><a href="mlchapter.html#cb984-17" aria-hidden="true" tabindex="-1"></a>                   <span class="at">sampling =</span> <span class="st">&quot;down&quot;</span>)</span>
<span id="cb984-18"><a href="mlchapter.html#cb984-18" aria-hidden="true" tabindex="-1"></a>m2 <span class="ot">&lt;-</span> <span class="fu">train</span>(type <span class="sc">~</span> pH <span class="sc">+</span> alcohol,</span>
<span id="cb984-19"><a href="mlchapter.html#cb984-19" aria-hidden="true" tabindex="-1"></a>            <span class="at">data =</span> wine_imb,</span>
<span id="cb984-20"><a href="mlchapter.html#cb984-20" aria-hidden="true" tabindex="-1"></a>            <span class="at">trControl =</span> tc,</span>
<span id="cb984-21"><a href="mlchapter.html#cb984-21" aria-hidden="true" tabindex="-1"></a>            <span class="at">method =</span> <span class="st">&quot;glm&quot;</span>,</span>
<span id="cb984-22"><a href="mlchapter.html#cb984-22" aria-hidden="true" tabindex="-1"></a>            <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb984-23"><a href="mlchapter.html#cb984-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb984-24"><a href="mlchapter.html#cb984-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit with up-sampling:</span></span>
<span id="cb984-25"><a href="mlchapter.html#cb984-25" aria-hidden="true" tabindex="-1"></a>tc <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span> , <span class="at">number =</span> <span class="dv">10</span>,</span>
<span id="cb984-26"><a href="mlchapter.html#cb984-26" aria-hidden="true" tabindex="-1"></a>                   <span class="at">savePredictions =</span> <span class="cn">TRUE</span>,</span>
<span id="cb984-27"><a href="mlchapter.html#cb984-27" aria-hidden="true" tabindex="-1"></a>                   <span class="at">classProbs =</span> <span class="cn">TRUE</span>,</span>
<span id="cb984-28"><a href="mlchapter.html#cb984-28" aria-hidden="true" tabindex="-1"></a>                   <span class="at">sampling =</span> <span class="st">&quot;up&quot;</span>)</span>
<span id="cb984-29"><a href="mlchapter.html#cb984-29" aria-hidden="true" tabindex="-1"></a>m3 <span class="ot">&lt;-</span> <span class="fu">train</span>(type <span class="sc">~</span> pH <span class="sc">+</span> alcohol,</span>
<span id="cb984-30"><a href="mlchapter.html#cb984-30" aria-hidden="true" tabindex="-1"></a>            <span class="at">data =</span> wine_imb,</span>
<span id="cb984-31"><a href="mlchapter.html#cb984-31" aria-hidden="true" tabindex="-1"></a>            <span class="at">trControl =</span> tc,</span>
<span id="cb984-32"><a href="mlchapter.html#cb984-32" aria-hidden="true" tabindex="-1"></a>            <span class="at">method =</span> <span class="st">&quot;glm&quot;</span>,</span>
<span id="cb984-33"><a href="mlchapter.html#cb984-33" aria-hidden="true" tabindex="-1"></a>            <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>)</span></code></pre></div>
<p>Looking at the accuracy of the three models, <code>m1</code> seems to be the winner:</p>
<div class="sourceCode" id="cb985"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb985-1"><a href="mlchapter.html#cb985-1" aria-hidden="true" tabindex="-1"></a>m1<span class="sc">$</span>results</span>
<span id="cb985-2"><a href="mlchapter.html#cb985-2" aria-hidden="true" tabindex="-1"></a>m2<span class="sc">$</span>results</span>
<span id="cb985-3"><a href="mlchapter.html#cb985-3" aria-hidden="true" tabindex="-1"></a>m3<span class="sc">$</span>results</span></code></pre></div>
<p>Bear in mind though, that the accuracy can be very high when you have imbalanced classes, even if your model has overfitted to the data and always predicts that all observations belong to the same class. Perhaps ROC curves will paint a different picture?</p>
<div class="sourceCode" id="cb986"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb986-1"><a href="mlchapter.html#cb986-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MLeval)</span>
<span id="cb986-2"><a href="mlchapter.html#cb986-2" aria-hidden="true" tabindex="-1"></a>plots <span class="ot">&lt;-</span> <span class="fu">evalm</span>(<span class="fu">list</span>(m1, m2, m3),</span>
<span id="cb986-3"><a href="mlchapter.html#cb986-3" aria-hidden="true" tabindex="-1"></a>               <span class="at">gnames =</span> <span class="fu">c</span>(<span class="st">&quot;Imbalanced data&quot;</span>,</span>
<span id="cb986-4"><a href="mlchapter.html#cb986-4" aria-hidden="true" tabindex="-1"></a>                          <span class="st">&quot;Down-sampling&quot;</span>,</span>
<span id="cb986-5"><a href="mlchapter.html#cb986-5" aria-hidden="true" tabindex="-1"></a>                          <span class="st">&quot;Up-sampling&quot;</span>))</span></code></pre></div>
<p>The three models have virtually identical performance in terms of AUC, so thus far there doesn’t seem to be an advantage to using down-sampling or up-sampling.</p>
<p>Now, let’s make predictions for all the red wines that the models haven’t seen in the training data. What are the predicted probabilities of them being red, for each model?</p>
<div class="sourceCode" id="cb987"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb987-1"><a href="mlchapter.html#cb987-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of red wines:</span></span>
<span id="cb987-2"><a href="mlchapter.html#cb987-2" aria-hidden="true" tabindex="-1"></a>size <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="dv">5001</span><span class="sc">:</span><span class="fu">nrow</span>(wine))</span>
<span id="cb987-3"><a href="mlchapter.html#cb987-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb987-4"><a href="mlchapter.html#cb987-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Collect the predicted probabilities in a data frame:</span></span>
<span id="cb987-5"><a href="mlchapter.html#cb987-5" aria-hidden="true" tabindex="-1"></a>red_preds <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">pred =</span> <span class="fu">c</span>(</span>
<span id="cb987-6"><a href="mlchapter.html#cb987-6" aria-hidden="true" tabindex="-1"></a>            <span class="fu">predict</span>(m1, wine[<span class="dv">5001</span><span class="sc">:</span><span class="fu">nrow</span>(wine),], <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>)[, <span class="dv">1</span>],</span>
<span id="cb987-7"><a href="mlchapter.html#cb987-7" aria-hidden="true" tabindex="-1"></a>            <span class="fu">predict</span>(m2, wine[<span class="dv">5001</span><span class="sc">:</span><span class="fu">nrow</span>(wine),], <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>)[, <span class="dv">1</span>],</span>
<span id="cb987-8"><a href="mlchapter.html#cb987-8" aria-hidden="true" tabindex="-1"></a>            <span class="fu">predict</span>(m3, wine[<span class="dv">5001</span><span class="sc">:</span><span class="fu">nrow</span>(wine),], <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>)[, <span class="dv">1</span>]),</span>
<span id="cb987-9"><a href="mlchapter.html#cb987-9" aria-hidden="true" tabindex="-1"></a>            <span class="at">method =</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">&quot;Standard&quot;</span>,</span>
<span id="cb987-10"><a href="mlchapter.html#cb987-10" aria-hidden="true" tabindex="-1"></a>                           <span class="st">&quot;Down-sampling&quot;</span>,</span>
<span id="cb987-11"><a href="mlchapter.html#cb987-11" aria-hidden="true" tabindex="-1"></a>                           <span class="st">&quot;Up-sampling&quot;</span>),</span>
<span id="cb987-12"><a href="mlchapter.html#cb987-12" aria-hidden="true" tabindex="-1"></a>                         <span class="fu">c</span>(size, size, size)))</span>
<span id="cb987-13"><a href="mlchapter.html#cb987-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb987-14"><a href="mlchapter.html#cb987-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the distributions of the predicted probabilities:</span></span>
<span id="cb987-15"><a href="mlchapter.html#cb987-15" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb987-16"><a href="mlchapter.html#cb987-16" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(red_preds, <span class="fu">aes</span>(pred, <span class="at">colour =</span> method)) <span class="sc">+</span></span>
<span id="cb987-17"><a href="mlchapter.html#cb987-17" aria-hidden="true" tabindex="-1"></a>      <span class="fu">geom_density</span>()</span></code></pre></div>
<p>When the model is fitted using the standard methods, almost all red wines get very low predicted probabilities of being red. This isn’t the case for the models that used down-sampling and up-sampling, meaning that <code>m2</code> and <code>m3</code> are much better at correctly classifying red wines. Note that we couldn’t see any differences between the models in the ROC curves, but that there are huge differences between them when they are applied to new data. Problems related to class imbalance can be difficult to detect, so always be careful when working with imbalanced data.</p>
</div>
<div id="varimportance" class="section level3 hasAnchor" number="9.3.2">
<h3><span class="header-section-number">9.3.2</span> Assessing variable importance<a href="mlchapter.html#varimportance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><code>caret</code> contains a function called <code>varImp</code> that can be used to assess the relative importance of different variables in a model. <code>dotPlot</code> can then be used to plot the results:</p>
<div class="sourceCode" id="cb988"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb988-1"><a href="mlchapter.html#cb988-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb988-2"><a href="mlchapter.html#cb988-2" aria-hidden="true" tabindex="-1"></a>tc <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;LOOCV&quot;</span>)</span>
<span id="cb988-3"><a href="mlchapter.html#cb988-3" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">train</span>(mpg <span class="sc">~</span> .,</span>
<span id="cb988-4"><a href="mlchapter.html#cb988-4" aria-hidden="true" tabindex="-1"></a>           <span class="at">data =</span> mtcars,</span>
<span id="cb988-5"><a href="mlchapter.html#cb988-5" aria-hidden="true" tabindex="-1"></a>           <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>,</span>
<span id="cb988-6"><a href="mlchapter.html#cb988-6" aria-hidden="true" tabindex="-1"></a>           <span class="at">trControl =</span> tc)</span>
<span id="cb988-7"><a href="mlchapter.html#cb988-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb988-8"><a href="mlchapter.html#cb988-8" aria-hidden="true" tabindex="-1"></a><span class="fu">varImp</span>(m)          <span class="co"># Numeric summary</span></span>
<span id="cb988-9"><a href="mlchapter.html#cb988-9" aria-hidden="true" tabindex="-1"></a><span class="fu">dotPlot</span>(<span class="fu">varImp</span>(m)) <span class="co"># Graphical summary</span></span></code></pre></div>
<p>Getting a measure of variable importance sounds really good - it can be useful to know which variables that influence the model the most. Unfortunately, <code>varImp</code> uses a nonsensical importance measure: the <span class="math inline">\(t\)</span>-statistics of the coefficients of the linear model. In essence, this means that variables with a lower p-value are assigned higher importance. But the p-value is <em>not</em> a measure of effect size, nor the predictive importance of a variable (see e.g. Wasserstein &amp; Lazar (2016)). I strongly advise against using <code>varImp</code> for linear models.</p>
<p>There are other options for computing variable importance for linear and generalised linear models, for instance in the <code>relaimpo</code> package, but mostly these rely on in-sample metrics like <span class="math inline">\(R^2\)</span>. Since our interest is in the predictive performance of our model, we are chiefly interested in how much the different variables <em>affect the predictions</em>. In Section <a href="mlchapter.html#randomforests">9.5.2</a> we will see an example of such an evaluation, for another type of model.</p>
</div>
<div id="extrapolation" class="section level3 hasAnchor" number="9.3.3">
<h3><span class="header-section-number">9.3.3</span> Extrapolation<a href="mlchapter.html#extrapolation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>It is always dangerous to use a predictive model with data that comes from outside the range of the variables in the training data. We’ll use <code>bacteria.csv</code> as an example of that - <a href="http://www.modernstatisticswithr.com/data.zip">download that file from the books’ web page</a> and set <code>file_path</code> to its path. The data has two variables, <code>Time</code> and <code>OD</code>. The first describes the time of a measurement, and the second describes the optical density (OD) of a well containing bacteria. The more the bacteria grow, the greater the OD. First, let’s load and plot the data:</p>
<div class="sourceCode" id="cb989"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb989-1"><a href="mlchapter.html#cb989-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Read and format data:</span></span>
<span id="cb989-2"><a href="mlchapter.html#cb989-2" aria-hidden="true" tabindex="-1"></a>bacteria <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(file_path)</span>
<span id="cb989-3"><a href="mlchapter.html#cb989-3" aria-hidden="true" tabindex="-1"></a>bacteria<span class="sc">$</span>Time <span class="ot">&lt;-</span> <span class="fu">as.POSIXct</span>(bacteria<span class="sc">$</span>Time, <span class="at">format =</span> <span class="st">&quot;%H:%M:%S&quot;</span>)</span>
<span id="cb989-4"><a href="mlchapter.html#cb989-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb989-5"><a href="mlchapter.html#cb989-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the bacterial growth:</span></span>
<span id="cb989-6"><a href="mlchapter.html#cb989-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb989-7"><a href="mlchapter.html#cb989-7" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(bacteria, <span class="fu">aes</span>(Time, OD)) <span class="sc">+</span></span>
<span id="cb989-8"><a href="mlchapter.html#cb989-8" aria-hidden="true" tabindex="-1"></a>      <span class="fu">geom_line</span>()</span></code></pre></div>
<p>Now, let’s fit a linear model to data from hours 3-6, during which the bacteria are in their exponential phase, where they grow faster:</p>
<div class="sourceCode" id="cb990"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb990-1"><a href="mlchapter.html#cb990-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit model:</span></span>
<span id="cb990-2"><a href="mlchapter.html#cb990-2" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">lm</span>(OD <span class="sc">~</span> Time, <span class="at">data =</span> bacteria[<span class="dv">45</span><span class="sc">:</span><span class="dv">90</span>,])</span>
<span id="cb990-3"><a href="mlchapter.html#cb990-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb990-4"><a href="mlchapter.html#cb990-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot fitted model:</span></span>
<span id="cb990-5"><a href="mlchapter.html#cb990-5" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(bacteria, <span class="fu">aes</span>(Time, OD)) <span class="sc">+</span></span>
<span id="cb990-6"><a href="mlchapter.html#cb990-6" aria-hidden="true" tabindex="-1"></a>      <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb990-7"><a href="mlchapter.html#cb990-7" aria-hidden="true" tabindex="-1"></a>      <span class="fu">geom_abline</span>(<span class="fu">aes</span>(<span class="at">intercept =</span> <span class="fu">coef</span>(m)[<span class="dv">1</span>], <span class="at">slope =</span> <span class="fu">coef</span>(m)[<span class="dv">2</span>]),</span>
<span id="cb990-8"><a href="mlchapter.html#cb990-8" aria-hidden="true" tabindex="-1"></a>                <span class="at">colour =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p>The model fits the data that it’s been fitted to extremely well - but does very poorly outside this interval. It overestimates the future growth and underestimates the previous OD.</p>
<p>In this example, we had access to data from outside the range used for fitting the model, which allowed us to see that the model performs poorly outside the original data range. In most cases however, we do not have access to such data. When extrapolating outside the range of the training data, there is always a risk that the patterns governing the phenomenons we are studying are completely different, and it is important to be aware of this.</p>
</div>
<div id="missing-data-and-imputation" class="section level3 hasAnchor" number="9.3.4">
<h3><span class="header-section-number">9.3.4</span> Missing data and imputation<a href="mlchapter.html#missing-data-and-imputation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <code>estates.xlsx</code> data that you studied in Exercise <a href="mlchapter.html#exr:ch8exc2">9.2</a> contained a lot of missing data, and as a consequence, you had to remove a lot of rows from the dataset. Another option is to use <em>imputation</em>, i.e. to add artificially generated observations in place of the missing values. This allows you to use the entire dataset - even those observations where some variables are missing. <code>caret</code> has functions for doing this, using methods that are based on some of the machine learning models that we will look at in Section <a href="mlchapter.html#mlmethods">9.5</a>.</p>
<p>To see an example of imputation, let’s create some missing values in <code>mtcars</code>:</p>
<div class="sourceCode" id="cb991"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb991-1"><a href="mlchapter.html#cb991-1" aria-hidden="true" tabindex="-1"></a>mtcars_missing <span class="ot">&lt;-</span> mtcars</span>
<span id="cb991-2"><a href="mlchapter.html#cb991-2" aria-hidden="true" tabindex="-1"></a>rows <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(mtcars), <span class="dv">5</span>)</span>
<span id="cb991-3"><a href="mlchapter.html#cb991-3" aria-hidden="true" tabindex="-1"></a>cols <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(mtcars), <span class="dv">2</span>)</span>
<span id="cb991-4"><a href="mlchapter.html#cb991-4" aria-hidden="true" tabindex="-1"></a>mtcars_missing[rows, cols] <span class="ot">&lt;-</span> <span class="cn">NA</span></span>
<span id="cb991-5"><a href="mlchapter.html#cb991-5" aria-hidden="true" tabindex="-1"></a>mtcars_missing</span></code></pre></div>
<p>If we try to fit a model to this data, we’ll get an error message about <code>NA</code> values:</p>
<div class="sourceCode" id="cb992"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb992-1"><a href="mlchapter.html#cb992-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb992-2"><a href="mlchapter.html#cb992-2" aria-hidden="true" tabindex="-1"></a>tc <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;repeatedcv&quot;</span>,</span>
<span id="cb992-3"><a href="mlchapter.html#cb992-3" aria-hidden="true" tabindex="-1"></a>                   <span class="at">number =</span> <span class="dv">10</span>, <span class="at">repeats =</span> <span class="dv">100</span>)</span>
<span id="cb992-4"><a href="mlchapter.html#cb992-4" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">train</span>(mpg <span class="sc">~</span> .,</span>
<span id="cb992-5"><a href="mlchapter.html#cb992-5" aria-hidden="true" tabindex="-1"></a>           <span class="at">data =</span> mtcars_missing,</span>
<span id="cb992-6"><a href="mlchapter.html#cb992-6" aria-hidden="true" tabindex="-1"></a>           <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>,</span>
<span id="cb992-7"><a href="mlchapter.html#cb992-7" aria-hidden="true" tabindex="-1"></a>           <span class="at">trControl =</span> tc)</span></code></pre></div>
<p>By adding <code>preProcess = "knnImpute"</code> and <code>na.action = na.pass</code> to <code>train</code> we can use the observations that are the most similar to those with missing values to impute data:</p>
<div class="sourceCode" id="cb993"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb993-1"><a href="mlchapter.html#cb993-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb993-2"><a href="mlchapter.html#cb993-2" aria-hidden="true" tabindex="-1"></a>tc <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;repeatedcv&quot;</span>,</span>
<span id="cb993-3"><a href="mlchapter.html#cb993-3" aria-hidden="true" tabindex="-1"></a>                   <span class="at">number =</span> <span class="dv">10</span>, <span class="at">repeats =</span> <span class="dv">100</span>)</span>
<span id="cb993-4"><a href="mlchapter.html#cb993-4" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">train</span>(mpg <span class="sc">~</span> .,</span>
<span id="cb993-5"><a href="mlchapter.html#cb993-5" aria-hidden="true" tabindex="-1"></a>           <span class="at">data =</span> mtcars_missing,</span>
<span id="cb993-6"><a href="mlchapter.html#cb993-6" aria-hidden="true" tabindex="-1"></a>           <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>,</span>
<span id="cb993-7"><a href="mlchapter.html#cb993-7" aria-hidden="true" tabindex="-1"></a>           <span class="at">trControl =</span> tc,</span>
<span id="cb993-8"><a href="mlchapter.html#cb993-8" aria-hidden="true" tabindex="-1"></a>           <span class="at">preProcess =</span> <span class="st">&quot;knnImpute&quot;</span>,</span>
<span id="cb993-9"><a href="mlchapter.html#cb993-9" aria-hidden="true" tabindex="-1"></a>           <span class="at">na.action =</span> na.pass)</span>
<span id="cb993-10"><a href="mlchapter.html#cb993-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb993-11"><a href="mlchapter.html#cb993-11" aria-hidden="true" tabindex="-1"></a>m<span class="sc">$</span>results</span></code></pre></div>
<p>You can compare the results obtained for this model to does obtained using the complete dataset:</p>
<div class="sourceCode" id="cb994"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb994-1"><a href="mlchapter.html#cb994-1" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">train</span>(mpg <span class="sc">~</span> .,</span>
<span id="cb994-2"><a href="mlchapter.html#cb994-2" aria-hidden="true" tabindex="-1"></a>           <span class="at">data =</span> mtcars,</span>
<span id="cb994-3"><a href="mlchapter.html#cb994-3" aria-hidden="true" tabindex="-1"></a>           <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>,</span>
<span id="cb994-4"><a href="mlchapter.html#cb994-4" aria-hidden="true" tabindex="-1"></a>           <span class="at">trControl =</span> tc)</span>
<span id="cb994-5"><a href="mlchapter.html#cb994-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb994-6"><a href="mlchapter.html#cb994-6" aria-hidden="true" tabindex="-1"></a>m<span class="sc">$</span>results</span></code></pre></div>
<p>Here, these are probably pretty close (we didn’t have a lot of missing data, after all), but not identical.</p>
</div>
<div id="endless-waiting" class="section level3 hasAnchor" number="9.3.5">
<h3><span class="header-section-number">9.3.5</span> Endless waiting<a href="mlchapter.html#endless-waiting" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Comparing many different models can take a lot of time, especially if you are working with large datasets. Waiting for the results can seem to take forever. Fortuitously, modern computers have processing units, CPU’s, that can perform multiple computations in parallel using different <em>cores</em> or <em>threads</em>. This can significantly speed up model fitting, as it for instance allows us to fit the same model to different subsets in a cross-validation in parallel, i.e. at the same time.</p>
<p>In Section <a href="advancedchapter.html#parallel">10.2</a> you’ll learn how to perform any type of computation in parallel. However, <code>caret</code> is so simple to run in parallel that we’ll have a quick lock at that right away. We’ll use the <code>foreach</code>, <code>parallel</code>, and <code>doParallel</code> packages, so let’s install them:</p>
<div class="sourceCode" id="cb995"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb995-1"><a href="mlchapter.html#cb995-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="fu">c</span>(<span class="st">&quot;foreach&quot;</span>, <span class="st">&quot;parallel&quot;</span>, <span class="st">&quot;doParallel&quot;</span>))</span></code></pre></div>
<p>The number of cores available on your machine determines how many processes can be run in parallel. To see how many you have, use <code>detectCores</code>:</p>
<div class="sourceCode" id="cb996"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb996-1"><a href="mlchapter.html#cb996-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(parallel)</span>
<span id="cb996-2"><a href="mlchapter.html#cb996-2" aria-hidden="true" tabindex="-1"></a><span class="fu">detectCores</span>()</span></code></pre></div>
<p>You should avoid the temptation of using all available cores for your parallel computation - you’ll always need to reserve at least one for running RStudio and other applications.</p>
<p>To enable parallel computations, we use <code>registerDoParallel</code> to <em>register</em> the parallel backend to be used. Here is an example where we create 3 workers (and so use 3 cores in parallel<a href="#fn59" class="footnote-ref" id="fnref59"><sup>59</sup></a>):</p>
<div class="sourceCode" id="cb997"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb997-1"><a href="mlchapter.html#cb997-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(doParallel)</span>
<span id="cb997-2"><a href="mlchapter.html#cb997-2" aria-hidden="true" tabindex="-1"></a><span class="fu">registerDoParallel</span>(<span class="dv">3</span>)</span></code></pre></div>
<p>After this, it will likely take less time to fit your <code>caret</code> models, as model fitting now will be performed using parallel computations on 3 cores. That means that you’ll spend less time waiting and more time modelling. Hurrah! One word of warning though: parallel computations require more memory, so you may run into problems with RAM if you are working on very large datasets.</p>
</div>
<div id="overfittingtestset" class="section level3 hasAnchor" number="9.3.6">
<h3><span class="header-section-number">9.3.6</span> Overfitting to the test set<a href="mlchapter.html#overfittingtestset" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Although out-of-sample evaluations are better than in-sample evaluations of predictive models, they are not without risks. Many practitioners like to fit several different models to the same dataset, and then compare their performance (indeed, we ourselves have done and will continue to do so!). When doing this, there is a risk that we overfit our models to the data used for the evaluation. The risk is greater when using test-training splits, but is not non-existent for cross-validation and bootstrapping. An interesting example of this phenomenon is presented by Recht et al. (2019), who show that the celebrated image classifiers trained on a dataset known as ImageNet perform significantly worse when used on new data.</p>
<p>When building predictive models that will be used in a real setting, it is a good practice to collect an additional <em>evaluation set</em> that is used to verify that the model still works well when faced with new data, that wasn’t part of the model fitting or the model testing. If your model performs worse than expected on the evaluation set, it is a sign that you’ve overfitted your model to the test set.</p>
<p>Apart from testing so many models that one happens to perform well on the test data (thus overfitting), there are several mistakes that can lead to overfitting. One example is data leakage, where part of the test data “leaks” into the training set. This can happen in several ways: maybe you include an explanatory variable that is a function of the response variable (e.g. price per square meter when trying to predict housing prices), or maybe you have twinned or duplicate observations in your data. Another example is to not include all steps of the modelling in the evaluation, for instance by first using the entire dataset to select which variables to include, and then use cross-validation to assess the performance of the model. If you use the data for variable selection, then that needs to be a part of your cross-validation as well.</p>
<p>In contrast to much of traditional statistics, out-of-sample evaluations are example-based. We must be aware that what worked at one point won’t necessarily work in the future. It is entirely possible that the phenomenon that we are modelling is non-stationary, meaning that the patterns in the training data differ from the patterns in future data. In that case, our model can be overfitted in the sense that it describes patterns that no longer are valid. It is therefore important to not only validate a predictive model once, but to return to it at a later point to check that it still performs as expected. Model evaluation is a task that lasts as long as the model is in use.</p>
</div>
</div>
<div id="regularised" class="section level2 hasAnchor" number="9.4">
<h2><span class="header-section-number">9.4</span> Regularised regression models<a href="mlchapter.html#regularised" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The standard method used for fitting linear models, ordinary least squares or OLS, can be shown to yield the best unbiased estimator of the regression coefficients (under certain assumptions). But what if we are willing to use estimators that are <em>biased</em>? A common way of measuring the performance of an estimator is the mean squared error, <span class="math inline">\(MSE\)</span>. If <span class="math inline">\(\hat{\theta}\)</span> is an estimator of a parameter <span class="math inline">\(\theta\)</span>, then</p>
<p><span class="math display">\[MSE(\theta) = E((\hat{\theta}-\theta)^2) = Bias^2(\hat{\theta})+Var(\hat{\theta}),\]</span>
which is known as the <em>bias-variance decomposition</em> of the <span class="math inline">\(MSE\)</span>. This means that if increasing the bias allows us to decrease the variance, it is possible to obtain an estimator with a lower <span class="math inline">\(MSE\)</span> than what is possible for unbiased estimators.</p>
<p>Regularised regression models are linear or generalised linear models in which a small (typically) bias is introduced in the model fitting. Often this can lead to models with better predictive performance. Moreover, it turns out that this also allows us to fit models in situations where it wouldn’t be possible to fit ordinary (generalised) linear models, for example when the number of variables is greater than the sample size.</p>
<p>To introduce the bias, we add a <em>penalty</em> term to the <em>loss function</em> used to fit the regression model. In the case of linear regression, the usual loss function is the squared <span class="math inline">\(\ell_2\)</span> norm, meaning that we seek the estimates <span class="math inline">\(\beta_i\)</span> that minimise</p>
<p><span class="math display">\[\sum_{i=1}^n(y_i-\beta_0-\beta_1 x_{i1}-\beta_2 x_{i2}-\cdots-\beta_p x_{ip})^2.\]</span></p>
<p>When fitting a regularised regression model, we instead seek the <span class="math inline">\(\beta=(\beta_1,\ldots,\beta_p)\)</span> that minimise</p>
<p><span class="math display">\[\sum_{i=1}^n(y_i-\beta_0-\beta_1 x_{i1}-\beta_2 x_{i2}-\cdots-\beta_p x_{ip})^2 + p(\beta,\lambda),\]</span></p>
<p>for some penalty function <span class="math inline">\(p(\beta, \lambda)\)</span>. The penalty function increases the “cost” of having large <span class="math inline">\(\beta_i\)</span>, which causes the estimates to “shrink” towards 0. <span class="math inline">\(\lambda\)</span> is a <em>shrinkage parameter</em> used to control the strength of the shrinkage - the larger <span class="math inline">\(\lambda\)</span> is, the greater the shrinkage. It is usually chosen using cross-validation.</p>
<p>Regularised regression models are not invariant under linear rescalings of the explanatory variables, meaning that if a variable is multiplied by some number <span class="math inline">\(a\)</span>, then this can change the fit of the entire model in an arbitrary way. For that reason, it is widely agreed that the explanatory variables should be standardised to have mean 0 and variance 1 before fitting a regularised regression model. Fortunately, the functions that we will use for fitting these models does that for us, so that we don’t have to worry about it. Moreover, they then rescale the model coefficients to be on the original scale, to facilitate interpretation of the model. We can therefore interpret the regression coefficients in the same way as we would for any other regression model.</p>
<p>In this section, we’ll look at how to use regularised regression in practice. Further mathematical details are deferred to Section <a href="mathschap.html#regreg">12.5</a>. We will make use of model-fitting functions from the <code>glmnet</code> package, so let’s start by installing that:</p>
<div class="sourceCode" id="cb998"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb998-1"><a href="mlchapter.html#cb998-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;glmnet&quot;</span>)</span></code></pre></div>
<p>We will use the <code>mtcars</code> data to illustrate regularised regression. We’ll begin by once again fitting an ordinary linear regression model to the data:</p>
<div class="sourceCode" id="cb999"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb999-1"><a href="mlchapter.html#cb999-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb999-2"><a href="mlchapter.html#cb999-2" aria-hidden="true" tabindex="-1"></a>tc <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;LOOCV&quot;</span>)</span>
<span id="cb999-3"><a href="mlchapter.html#cb999-3" aria-hidden="true" tabindex="-1"></a>m1 <span class="ot">&lt;-</span> <span class="fu">train</span>(mpg <span class="sc">~</span> .,</span>
<span id="cb999-4"><a href="mlchapter.html#cb999-4" aria-hidden="true" tabindex="-1"></a>           <span class="at">data =</span> mtcars,</span>
<span id="cb999-5"><a href="mlchapter.html#cb999-5" aria-hidden="true" tabindex="-1"></a>           <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>,</span>
<span id="cb999-6"><a href="mlchapter.html#cb999-6" aria-hidden="true" tabindex="-1"></a>           <span class="at">trControl =</span> tc)</span>
<span id="cb999-7"><a href="mlchapter.html#cb999-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb999-8"><a href="mlchapter.html#cb999-8" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m1)</span></code></pre></div>
<div id="ridge-regression" class="section level3 hasAnchor" number="9.4.1">
<h3><span class="header-section-number">9.4.1</span> Ridge regression<a href="mlchapter.html#ridge-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The first regularised model that we will consider is <em>ridge regression</em> (Hoerl &amp; Kennard, 1970), for which the penalty function is <span class="math inline">\(p(\beta,\lambda)=\lambda\sum_{j=1}^{p}\beta_i^2\)</span>. We will fit such a model to the <code>mtcars</code> data using <code>train</code>. LOOCV will be used, both for evaluating the model and for finding the best choice of the shrinkage parameter <span class="math inline">\(\lambda\)</span>. This process is often called hyperparameter<a href="#fn60" class="footnote-ref" id="fnref60"><sup>60</sup></a> <em>tuning</em> - we tune the hyperparameter <span class="math inline">\(\lambda\)</span> until we get a good model.</p>
<div class="sourceCode" id="cb1000"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1000-1"><a href="mlchapter.html#cb1000-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb1000-2"><a href="mlchapter.html#cb1000-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit ridge regression:</span></span>
<span id="cb1000-3"><a href="mlchapter.html#cb1000-3" aria-hidden="true" tabindex="-1"></a>tc <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;LOOCV&quot;</span>)</span>
<span id="cb1000-4"><a href="mlchapter.html#cb1000-4" aria-hidden="true" tabindex="-1"></a>m2 <span class="ot">&lt;-</span> <span class="fu">train</span>(mpg <span class="sc">~</span> .,</span>
<span id="cb1000-5"><a href="mlchapter.html#cb1000-5" aria-hidden="true" tabindex="-1"></a>           <span class="at">data =</span> mtcars,</span>
<span id="cb1000-6"><a href="mlchapter.html#cb1000-6" aria-hidden="true" tabindex="-1"></a>           <span class="at">method =</span> <span class="st">&quot;glmnet&quot;</span>, </span>
<span id="cb1000-7"><a href="mlchapter.html#cb1000-7" aria-hidden="true" tabindex="-1"></a>           <span class="at">tuneGrid =</span> <span class="fu">expand.grid</span>(<span class="at">alpha =</span> <span class="dv">0</span>,</span>
<span id="cb1000-8"><a href="mlchapter.html#cb1000-8" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">lambda =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="fl">0.1</span>)),</span>
<span id="cb1000-9"><a href="mlchapter.html#cb1000-9" aria-hidden="true" tabindex="-1"></a>           <span class="at">metric =</span>  <span class="st">&quot;RMSE&quot;</span>,</span>
<span id="cb1000-10"><a href="mlchapter.html#cb1000-10" aria-hidden="true" tabindex="-1"></a>           <span class="at">trControl =</span> tc) </span></code></pre></div>
<p>In the <code>tuneGrid</code> setting of <code>train</code> we specified that values of <span class="math inline">\(\lambda\)</span> in the interval <span class="math inline">\(\lbrack 0,10\rbrack\)</span> should be evaluated. When we print the <code>m</code> object, we will see <span class="math inline">\(RMSE\)</span> and <span class="math inline">\(MAE\)</span> of the models for different values of <span class="math inline">\(\lambda\)</span> (with <span class="math inline">\(\lambda=0\)</span> being ordinary non-regularised linear regression):</p>
<div class="sourceCode" id="cb1001"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1001-1"><a href="mlchapter.html#cb1001-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the results:</span></span>
<span id="cb1001-2"><a href="mlchapter.html#cb1001-2" aria-hidden="true" tabindex="-1"></a>m2</span>
<span id="cb1001-3"><a href="mlchapter.html#cb1001-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1001-4"><a href="mlchapter.html#cb1001-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the results:</span></span>
<span id="cb1001-5"><a href="mlchapter.html#cb1001-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb1001-6"><a href="mlchapter.html#cb1001-6" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(m2, <span class="at">metric =</span> <span class="st">&quot;RMSE&quot;</span>)</span>
<span id="cb1001-7"><a href="mlchapter.html#cb1001-7" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(m2, <span class="at">metric =</span> <span class="st">&quot;MAE&quot;</span>)</span></code></pre></div>
<p>To only print the results for the best model, we can use:</p>
<div class="sourceCode" id="cb1002"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1002-1"><a href="mlchapter.html#cb1002-1" aria-hidden="true" tabindex="-1"></a>m2<span class="sc">$</span>results[<span class="fu">which</span>(m2<span class="sc">$</span>results<span class="sc">$</span>lambda <span class="sc">==</span> m2<span class="sc">$</span>finalModel<span class="sc">$</span>lambdaOpt),]</span></code></pre></div>
<p>Note that the <span class="math inline">\(RMSE\)</span> is substantially lower than that for the ordinary linear regression (<code>m1</code>).</p>
<p>In the <code>metric</code> setting of <code>train</code>, we said that we wanted <span class="math inline">\(RMSE\)</span> to be used to determine which value of <span class="math inline">\(\lambda\)</span> gives the best model. To get the coefficients of the model with the best choice of <span class="math inline">\(\lambda\)</span>, we use <code>coef</code> as follows:</p>
<div class="sourceCode" id="cb1003"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1003-1"><a href="mlchapter.html#cb1003-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check the coefficients of the best model:</span></span>
<span id="cb1003-2"><a href="mlchapter.html#cb1003-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(m2<span class="sc">$</span>finalModel, m2<span class="sc">$</span>finalModel<span class="sc">$</span>lambdaOpt)</span></code></pre></div>
<p>Comparing these coefficients to those from the ordinary linear regression (<code>summary(m1)</code>), we see that the coefficients of the two models actually differ quite a lot.</p>
<p>If we want to use our ridge regression model for prediction, it is straightforward to do so using <code>predict(m)</code>, as <code>predict</code> automatically uses the best model for prediction.</p>
<p>It is also possible to choose <span class="math inline">\(\lambda\)</span> without specifying the region in which to search for the best <span class="math inline">\(\lambda\)</span>, i.e. without providing a <code>tuneGrid</code> argument. In this case, some (arbitrarily chosen) default values will be used instead:</p>
<div class="sourceCode" id="cb1004"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1004-1"><a href="mlchapter.html#cb1004-1" aria-hidden="true" tabindex="-1"></a>m2 <span class="ot">&lt;-</span> <span class="fu">train</span>(mpg <span class="sc">~</span> .,</span>
<span id="cb1004-2"><a href="mlchapter.html#cb1004-2" aria-hidden="true" tabindex="-1"></a>           <span class="at">data =</span> mtcars,</span>
<span id="cb1004-3"><a href="mlchapter.html#cb1004-3" aria-hidden="true" tabindex="-1"></a>           <span class="at">method =</span> <span class="st">&quot;glmnet&quot;</span>, </span>
<span id="cb1004-4"><a href="mlchapter.html#cb1004-4" aria-hidden="true" tabindex="-1"></a>           <span class="at">metric =</span>  <span class="st">&quot;RMSE&quot;</span>,</span>
<span id="cb1004-5"><a href="mlchapter.html#cb1004-5" aria-hidden="true" tabindex="-1"></a>           <span class="at">trControl =</span> tc) </span>
<span id="cb1004-6"><a href="mlchapter.html#cb1004-6" aria-hidden="true" tabindex="-1"></a>m2</span></code></pre></div>
<p><span class="math display">\[\sim\]</span></p>
<div class="exercise">
<p><span id="exr:ch8exc6" class="exercise"><strong>Exercise 9.9  </strong></span>Return to the <code>estates.xlsx</code> data from Exercise <a href="mlchapter.html#exr:ch8exc2">9.2</a>. Refit your linear model, but this time use ridge regression instead. Does the <span class="math inline">\(RMSE\)</span> and <span class="math inline">\(MAE\)</span> improve?</p>
</div>
<p><a href="solutions.html#ch8solutions6">(Click here to go to the solution.)</a></p>
<p><br></p>
<div class="exercise">
<p><span id="exr:ch8exc6b" class="exercise"><strong>Exercise 9.10  </strong></span>Return to the <code>wine</code> data from Exercise <a href="mlchapter.html#exr:ch8exc5">9.5</a>. Fitting the models below will take a few minutes, so be prepared to wait for a little while.</p>
<ol style="list-style-type: decimal">
<li><p>Fit a logistic ridge regression model to the data (make sure to add <code>family = "binomial"</code> so that you actually fit a logistic model and not a linear model), using all variables in the dataset (except <code>type</code>) as explanatory variables. Use 5-fold cross-validation for choosing <span class="math inline">\(\lambda\)</span> and evaluating the model (other options are too computer-intensive). What metric is used when finding the optimal <span class="math inline">\(\lambda\)</span>?</p></li>
<li><p>Set <code>summaryFunction = twoClassSummary</code> in <code>trainControl</code> and <code>metric = "ROC"</code> in <code>train</code> and refit the model using <span class="math inline">\(AUC\)</span> to find the optimal <span class="math inline">\(\lambda\)</span>. Does the choice of <span class="math inline">\(\lambda\)</span> change, for this particular dataset?</p></li>
</ol>
</div>
<p><a href="solutions.html#ch8solutions6b">(Click here to go to the solution.)</a></p>
</div>
<div id="lasso" class="section level3 hasAnchor" number="9.4.2">
<h3><span class="header-section-number">9.4.2</span> The lasso<a href="mlchapter.html#lasso" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The next regularised regression model that we will consider is <em>the lasso</em> (Tibshirani, 1996), for which <span class="math inline">\(p(\beta,\lambda)=\lambda\sum_{j=1}^{p}|\beta_i|\)</span>. This is an interesting model because it simultaneously performs estimation and <em>variable selection</em>, by completely removing some variables from the model. This is particularly useful if we have a large number of variables, in which case the lasso may create a simpler model while maintaining high predictive accuracy. Let’s fit a lasso model to our data, using <span class="math inline">\(MAE\)</span> to select the best <span class="math inline">\(\lambda\)</span>:</p>
<div class="sourceCode" id="cb1005"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1005-1"><a href="mlchapter.html#cb1005-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb1005-2"><a href="mlchapter.html#cb1005-2" aria-hidden="true" tabindex="-1"></a>tc <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;LOOCV&quot;</span>)</span>
<span id="cb1005-3"><a href="mlchapter.html#cb1005-3" aria-hidden="true" tabindex="-1"></a>m3 <span class="ot">&lt;-</span> <span class="fu">train</span>(mpg <span class="sc">~</span> .,</span>
<span id="cb1005-4"><a href="mlchapter.html#cb1005-4" aria-hidden="true" tabindex="-1"></a>           <span class="at">data =</span> mtcars,</span>
<span id="cb1005-5"><a href="mlchapter.html#cb1005-5" aria-hidden="true" tabindex="-1"></a>           <span class="at">method =</span> <span class="st">&quot;glmnet&quot;</span>, </span>
<span id="cb1005-6"><a href="mlchapter.html#cb1005-6" aria-hidden="true" tabindex="-1"></a>           <span class="at">tuneGrid =</span> <span class="fu">expand.grid</span>(<span class="at">alpha =</span> <span class="dv">1</span>,</span>
<span id="cb1005-7"><a href="mlchapter.html#cb1005-7" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">lambda =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="fl">0.1</span>)),</span>
<span id="cb1005-8"><a href="mlchapter.html#cb1005-8" aria-hidden="true" tabindex="-1"></a>           <span class="at">metric =</span> <span class="st">&quot;MAE&quot;</span>,</span>
<span id="cb1005-9"><a href="mlchapter.html#cb1005-9" aria-hidden="true" tabindex="-1"></a>           <span class="at">trControl =</span> tc) </span>
<span id="cb1005-10"><a href="mlchapter.html#cb1005-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1005-11"><a href="mlchapter.html#cb1005-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the results:</span></span>
<span id="cb1005-12"><a href="mlchapter.html#cb1005-12" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb1005-13"><a href="mlchapter.html#cb1005-13" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(m3, <span class="at">metric =</span> <span class="st">&quot;RMSE&quot;</span>)</span>
<span id="cb1005-14"><a href="mlchapter.html#cb1005-14" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(m3, <span class="at">metric =</span> <span class="st">&quot;MAE&quot;</span>)</span>
<span id="cb1005-15"><a href="mlchapter.html#cb1005-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1005-16"><a href="mlchapter.html#cb1005-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Results for the best model:</span></span>
<span id="cb1005-17"><a href="mlchapter.html#cb1005-17" aria-hidden="true" tabindex="-1"></a>m3<span class="sc">$</span>results[<span class="fu">which</span>(m3<span class="sc">$</span>results<span class="sc">$</span>lambda <span class="sc">==</span> m3<span class="sc">$</span>finalModel<span class="sc">$</span>lambdaOpt),]</span>
<span id="cb1005-18"><a href="mlchapter.html#cb1005-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1005-19"><a href="mlchapter.html#cb1005-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Coefficients for the best model:</span></span>
<span id="cb1005-20"><a href="mlchapter.html#cb1005-20" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(m3<span class="sc">$</span>finalModel, m3<span class="sc">$</span>finalModel<span class="sc">$</span>lambdaOpt)</span></code></pre></div>
<p>The variables that were removed from the model are marked by points (<code>.</code>) in the list of coefficients. The <span class="math inline">\(RMSE\)</span> is comparable to that from the ridge regression - and is better than that for the ordinary linear regression, but the number of variables used is fewer. The lasso model is more parsimonious, and therefore easier to interpret (and present to your boss/client/supervisor/colleagues!).</p>
<p>If you only wish to extract the names of the variables with non-zero coefficients from the lasso model (i.e. a list of the variables retained in the variable selection), you can do so using the code below. This can be useful if you have a large number of variables and quickly want to check which have non-zero coefficients:</p>
<div class="sourceCode" id="cb1006"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1006-1"><a href="mlchapter.html#cb1006-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(<span class="fu">coef</span>(m3<span class="sc">$</span>finalModel, m3<span class="sc">$</span>finalModel<span class="sc">$</span>lambdaOpt))[</span>
<span id="cb1006-2"><a href="mlchapter.html#cb1006-2" aria-hidden="true" tabindex="-1"></a>         <span class="fu">coef</span>(m3<span class="sc">$</span>finalModel, m3<span class="sc">$</span>finalModel<span class="sc">$</span>lambdaOpt)[,<span class="dv">1</span>]<span class="sc">!=</span> <span class="dv">0</span>]</span></code></pre></div>
<p><span class="math display">\[\sim\]</span></p>
<div class="exercise">
<p><span id="exr:ch8exc7" class="exercise"><strong>Exercise 9.11  </strong></span>Return to the <code>estates.xlsx</code> data from Exercise <a href="mlchapter.html#exr:ch8exc2">9.2</a>. Refit your linear model, but this time use the lasso instead. Does the <span class="math inline">\(RMSE\)</span> and <span class="math inline">\(MAE\)</span> improve?</p>
</div>
<p><a href="solutions.html#ch8solutions7">(Click here to go to the solution.)</a></p>
<p><br></p>
<div class="exercise">
<p><span id="exr:ch8exc7b" class="exercise"><strong>Exercise 9.12  </strong></span>To see how the lasso handles variable selection, simulate a dataset where only the first 5 out of 200 explanatory variables are correlated with the response variable:</p>
</div>
<div class="sourceCode" id="cb1007"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1007-1"><a href="mlchapter.html#cb1007-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span> <span class="co"># Number of observations</span></span>
<span id="cb1007-2"><a href="mlchapter.html#cb1007-2" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">200</span> <span class="co"># Number of variables</span></span>
<span id="cb1007-3"><a href="mlchapter.html#cb1007-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate explanatory variables:</span></span>
<span id="cb1007-4"><a href="mlchapter.html#cb1007-4" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n<span class="sc">*</span>p), n, p) </span>
<span id="cb1007-5"><a href="mlchapter.html#cb1007-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate response variable:</span></span>
<span id="cb1007-6"><a href="mlchapter.html#cb1007-6" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span>x[,<span class="dv">1</span>] <span class="sc">+</span> x[,<span class="dv">2</span>] <span class="sc">-</span> <span class="dv">3</span><span class="sc">*</span>x[,<span class="dv">3</span>] <span class="sc">+</span> <span class="fl">0.5</span><span class="sc">*</span>x[,<span class="dv">4</span>] <span class="sc">+</span> <span class="fl">0.25</span><span class="sc">*</span>x[,<span class="dv">5</span>] <span class="sc">+</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb1007-7"><a href="mlchapter.html#cb1007-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Collect the simulated data in a data frame:</span></span>
<span id="cb1007-8"><a href="mlchapter.html#cb1007-8" aria-hidden="true" tabindex="-1"></a>simulated_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(y, x)</span></code></pre></div>
<ol style="list-style-type: decimal">
<li><p>Fit a linear model to the data (using the model formula <code>y ~ .</code>). What happens?</p></li>
<li><p>Fit a lasso model to this data. Does it select the correct variables? What if you repeat the simulation several times, or change the values of <code>n</code> and <code>p</code>?</p></li>
</ol>
<p><a href="solutions.html#ch8solutions7b">(Click here to go to the solution.)</a></p>
</div>
<div id="elastic-net" class="section level3 hasAnchor" number="9.4.3">
<h3><span class="header-section-number">9.4.3</span> Elastic net<a href="mlchapter.html#elastic-net" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A third option is the <em>elastic net</em> (Zou &amp; Hastie, 2005), which essentially is a compromise between ridge regression and the lasso. Its penalty function is <span class="math inline">\(p(\beta,\lambda,\alpha)=\lambda\Big(\alpha\sum_{j=1}^{p}|\beta_i|+(1-\alpha)\sum_{j=1}^{p}\beta_i^2\Big)\)</span>, with <span class="math inline">\(0\leq\alpha\leq1\)</span>. <span class="math inline">\(\alpha=0\)</span> yields the ridge estimator, <span class="math inline">\(\alpha=1\)</span> yields the lasso, and <span class="math inline">\(\alpha\)</span> between 0 and 1 yields a combination of the both. When fitting an elastic net model, we search for an optimal choice of <span class="math inline">\(\alpha\)</span>, along with the choice of <span class="math inline">\(\lambda_i\)</span>. To fit such a model, we can run the following:</p>
<div class="sourceCode" id="cb1008"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1008-1"><a href="mlchapter.html#cb1008-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb1008-2"><a href="mlchapter.html#cb1008-2" aria-hidden="true" tabindex="-1"></a>tc <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;LOOCV&quot;</span>)</span>
<span id="cb1008-3"><a href="mlchapter.html#cb1008-3" aria-hidden="true" tabindex="-1"></a>m4 <span class="ot">&lt;-</span> <span class="fu">train</span>(mpg <span class="sc">~</span> .,</span>
<span id="cb1008-4"><a href="mlchapter.html#cb1008-4" aria-hidden="true" tabindex="-1"></a>           <span class="at">data =</span> mtcars,</span>
<span id="cb1008-5"><a href="mlchapter.html#cb1008-5" aria-hidden="true" tabindex="-1"></a>           <span class="at">method =</span> <span class="st">&quot;glmnet&quot;</span>, </span>
<span id="cb1008-6"><a href="mlchapter.html#cb1008-6" aria-hidden="true" tabindex="-1"></a>           <span class="at">tuneGrid =</span> <span class="fu">expand.grid</span>(<span class="at">alpha =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.1</span>),</span>
<span id="cb1008-7"><a href="mlchapter.html#cb1008-7" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">lambda =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="fl">0.1</span>)),</span>
<span id="cb1008-8"><a href="mlchapter.html#cb1008-8" aria-hidden="true" tabindex="-1"></a>           <span class="at">metric =</span> <span class="st">&quot;RMSE&quot;</span>,</span>
<span id="cb1008-9"><a href="mlchapter.html#cb1008-9" aria-hidden="true" tabindex="-1"></a>           <span class="at">trControl =</span> tc) </span>
<span id="cb1008-10"><a href="mlchapter.html#cb1008-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1008-11"><a href="mlchapter.html#cb1008-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Print best choices of alpha and lambda:</span></span>
<span id="cb1008-12"><a href="mlchapter.html#cb1008-12" aria-hidden="true" tabindex="-1"></a>m4<span class="sc">$</span>bestTune</span>
<span id="cb1008-13"><a href="mlchapter.html#cb1008-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1008-14"><a href="mlchapter.html#cb1008-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the RMSE and MAE for the best model:</span></span>
<span id="cb1008-15"><a href="mlchapter.html#cb1008-15" aria-hidden="true" tabindex="-1"></a>m4<span class="sc">$</span>results[<span class="fu">which</span>(<span class="fu">rownames</span>(m4<span class="sc">$</span>results) <span class="sc">==</span> <span class="fu">rownames</span>(m4<span class="sc">$</span>bestTune)),]</span>
<span id="cb1008-16"><a href="mlchapter.html#cb1008-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1008-17"><a href="mlchapter.html#cb1008-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the coefficients of the best model:</span></span>
<span id="cb1008-18"><a href="mlchapter.html#cb1008-18" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(m4<span class="sc">$</span>finalModel, m4<span class="sc">$</span>bestTune<span class="sc">$</span>lambda, m4<span class="sc">$</span>bestTune<span class="sc">$</span>alpha)</span></code></pre></div>
<p>In this example, the ridge regression happened to yield the best fit, in terms of the cross-validation <span class="math inline">\(RMSE\)</span>.</p>
<p><span class="math display">\[\sim\]</span></p>
<div class="exercise">
<p><span id="exr:ch8exc8" class="exercise"><strong>Exercise 9.13  </strong></span>Return to the <code>estates.xlsx</code> data from Exercise <a href="mlchapter.html#exr:ch8exc2">9.2</a>. Refit your linear model, but this time use the elastic net instead. Does the <span class="math inline">\(RMSE\)</span> and <span class="math inline">\(MAE\)</span> improve?</p>
</div>
<p><a href="solutions.html#ch8solutions8">(Click here to go to the solution.)</a></p>
</div>
<div id="choosing-the-best-model" class="section level3 hasAnchor" number="9.4.4">
<h3><span class="header-section-number">9.4.4</span> Choosing the best model<a href="mlchapter.html#choosing-the-best-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>So far, we have used the values of <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\alpha\)</span> that give the best results according to a performance metric, such as <span class="math inline">\(RMSE\)</span> or <span class="math inline">\(AUC\)</span>. However, it is often the case that we can find a more parsimonious, i.e. simpler, model with almost as good performance. Such models can sometimes be preferable, because of their relative simplicity. Using those models can also reduce the risk of overfitting. <code>caret</code> has two functions that can be used for this:</p>
<ul>
<li><code>oneSE</code>, which follows a rule-of-thumb from Breiman et al. (1984), which states that the simplest model within one standard error of the model with the best performance should be chosen,</li>
<li><code>tolerance</code>, which chooses the simplest model that has a performance within (by default) 1.5 % of the model with the best performance.</li>
</ul>
<p>Neither of these can be used with LOOCV, but work for other cross-validation schemes and the bootstrap.</p>
<p>We can set the rule for selecting the “best” model using the argument <code>selectionFunction</code> in <code>trainControl</code>. By default, it uses a function called <code>best</code> that simply extracts the model with the best performance. Here are some examples for the lasso:</p>
<div class="sourceCode" id="cb1009"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1009-1"><a href="mlchapter.html#cb1009-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb1009-2"><a href="mlchapter.html#cb1009-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Choose the best model (this is the default!):</span></span>
<span id="cb1009-3"><a href="mlchapter.html#cb1009-3" aria-hidden="true" tabindex="-1"></a>  tc <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;repeatedcv&quot;</span>,</span>
<span id="cb1009-4"><a href="mlchapter.html#cb1009-4" aria-hidden="true" tabindex="-1"></a>                     <span class="at">number =</span> <span class="dv">10</span>, <span class="at">repeats =</span> <span class="dv">100</span>)</span>
<span id="cb1009-5"><a href="mlchapter.html#cb1009-5" aria-hidden="true" tabindex="-1"></a>  m3 <span class="ot">&lt;-</span> <span class="fu">train</span>(mpg <span class="sc">~</span> .,</span>
<span id="cb1009-6"><a href="mlchapter.html#cb1009-6" aria-hidden="true" tabindex="-1"></a>             <span class="at">data =</span> mtcars,</span>
<span id="cb1009-7"><a href="mlchapter.html#cb1009-7" aria-hidden="true" tabindex="-1"></a>             <span class="at">method =</span> <span class="st">&quot;glmnet&quot;</span>, </span>
<span id="cb1009-8"><a href="mlchapter.html#cb1009-8" aria-hidden="true" tabindex="-1"></a>             <span class="at">tuneGrid =</span> <span class="fu">expand.grid</span>(<span class="at">alpha =</span> <span class="dv">1</span>,</span>
<span id="cb1009-9"><a href="mlchapter.html#cb1009-9" aria-hidden="true" tabindex="-1"></a>                                    <span class="at">lambda =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="fl">0.1</span>)),</span>
<span id="cb1009-10"><a href="mlchapter.html#cb1009-10" aria-hidden="true" tabindex="-1"></a>             <span class="at">metric =</span> <span class="st">&quot;RMSE&quot;</span>,</span>
<span id="cb1009-11"><a href="mlchapter.html#cb1009-11" aria-hidden="true" tabindex="-1"></a>             <span class="at">trControl =</span> tc) </span>
<span id="cb1009-12"><a href="mlchapter.html#cb1009-12" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1009-13"><a href="mlchapter.html#cb1009-13" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Print the best model:</span></span>
<span id="cb1009-14"><a href="mlchapter.html#cb1009-14" aria-hidden="true" tabindex="-1"></a>  m3<span class="sc">$</span>bestTune</span>
<span id="cb1009-15"><a href="mlchapter.html#cb1009-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coef</span>(m3<span class="sc">$</span>finalModel, m3<span class="sc">$</span>finalModel<span class="sc">$</span>lambdaOpt)</span>
<span id="cb1009-16"><a href="mlchapter.html#cb1009-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1009-17"><a href="mlchapter.html#cb1009-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Choose model using oneSE:</span></span>
<span id="cb1009-18"><a href="mlchapter.html#cb1009-18" aria-hidden="true" tabindex="-1"></a>  tc <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;repeatedcv&quot;</span>,</span>
<span id="cb1009-19"><a href="mlchapter.html#cb1009-19" aria-hidden="true" tabindex="-1"></a>                     <span class="at">number =</span> <span class="dv">10</span>, <span class="at">repeats =</span> <span class="dv">100</span>,</span>
<span id="cb1009-20"><a href="mlchapter.html#cb1009-20" aria-hidden="true" tabindex="-1"></a>                     <span class="at">selectionFunction =</span> <span class="st">&quot;oneSE&quot;</span>)</span>
<span id="cb1009-21"><a href="mlchapter.html#cb1009-21" aria-hidden="true" tabindex="-1"></a>  m3 <span class="ot">&lt;-</span> <span class="fu">train</span>(mpg <span class="sc">~</span> .,</span>
<span id="cb1009-22"><a href="mlchapter.html#cb1009-22" aria-hidden="true" tabindex="-1"></a>             <span class="at">data =</span> mtcars,</span>
<span id="cb1009-23"><a href="mlchapter.html#cb1009-23" aria-hidden="true" tabindex="-1"></a>             <span class="at">method =</span> <span class="st">&quot;glmnet&quot;</span>, </span>
<span id="cb1009-24"><a href="mlchapter.html#cb1009-24" aria-hidden="true" tabindex="-1"></a>             <span class="at">tuneGrid =</span> <span class="fu">expand.grid</span>(<span class="at">alpha =</span> <span class="dv">1</span>,</span>
<span id="cb1009-25"><a href="mlchapter.html#cb1009-25" aria-hidden="true" tabindex="-1"></a>                                    <span class="at">lambda =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="fl">0.1</span>)),</span>
<span id="cb1009-26"><a href="mlchapter.html#cb1009-26" aria-hidden="true" tabindex="-1"></a>             <span class="at">trControl =</span> tc) </span>
<span id="cb1009-27"><a href="mlchapter.html#cb1009-27" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1009-28"><a href="mlchapter.html#cb1009-28" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Print the &quot;best&quot; model (according to the oneSE rule):</span></span>
<span id="cb1009-29"><a href="mlchapter.html#cb1009-29" aria-hidden="true" tabindex="-1"></a>  m3<span class="sc">$</span>bestTune</span>
<span id="cb1009-30"><a href="mlchapter.html#cb1009-30" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coef</span>(m3<span class="sc">$</span>finalModel, m3<span class="sc">$</span>finalModel<span class="sc">$</span>lambdaOpt)</span></code></pre></div>
<p>In this example, the difference between the models is small - and it usually is. In some cases, using <code>oneSE</code> or <code>tolerance</code> leads to a model that has better performance on new data, but in other cases the model that has the best performance in the evaluation also has the best performance for new data.</p>
</div>
<div id="regularised-mixed-models" class="section level3 hasAnchor" number="9.4.5">
<h3><span class="header-section-number">9.4.5</span> Regularised mixed models<a href="mlchapter.html#regularised-mixed-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><code>caret</code> does not handle regularisation of (generalised) linear mixed models. If you want to work with such models, you’ll therefore need a package that provides functions for this:</p>
<div class="sourceCode" id="cb1010"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1010-1"><a href="mlchapter.html#cb1010-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;glmmLasso&quot;</span>)</span></code></pre></div>
<p>Regularised mixed models are strange birds. Mixed models are primarily used for inference about the fixed effects, whereas regularisation primarily is used for predictive purposes. The two don’t really seem to match. They can however be very useful if our main interest is <em>estimation</em> rather than prediction or hypothesis testing, where regularisation can help decrease overfitting. Similarly, it is not uncommon for linear mixed models to be numerically unstable, with the model fitting sometimes failing to converge. In such situations, a regularised LMM will often work better. Let’s study an example concerning football (soccer) teams, from Groll &amp; Tutz (2014), that shows how to incorporate random effects and the lasso in the same model:</p>
<div class="sourceCode" id="cb1011"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1011-1"><a href="mlchapter.html#cb1011-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmmLasso)</span>
<span id="cb1011-2"><a href="mlchapter.html#cb1011-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1011-3"><a href="mlchapter.html#cb1011-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(soccer)</span>
<span id="cb1011-4"><a href="mlchapter.html#cb1011-4" aria-hidden="true" tabindex="-1"></a>?soccer</span>
<span id="cb1011-5"><a href="mlchapter.html#cb1011-5" aria-hidden="true" tabindex="-1"></a><span class="fu">View</span>(soccer)</span></code></pre></div>
<p>We want to model the points totals for these football teams. We suspect that variables like <code>transfer.spendings</code> can affect the performance of a team:</p>
<div class="sourceCode" id="cb1012"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1012-1"><a href="mlchapter.html#cb1012-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(soccer, <span class="fu">aes</span>(transfer.spendings, points, <span class="at">colour =</span> team)) <span class="sc">+</span></span>
<span id="cb1012-2"><a href="mlchapter.html#cb1012-2" aria-hidden="true" tabindex="-1"></a>      <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb1012-3"><a href="mlchapter.html#cb1012-3" aria-hidden="true" tabindex="-1"></a>      <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">colour =</span> <span class="st">&quot;black&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p>Moreover, it also seems likely that other non-quantitative variables also affect the performance, which could cause the teams to all have different intercepts. Let’s plot them side-by-side:</p>
<div class="sourceCode" id="cb1013"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1013-1"><a href="mlchapter.html#cb1013-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb1013-2"><a href="mlchapter.html#cb1013-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(soccer, <span class="fu">aes</span>(transfer.spendings, points, <span class="at">colour =</span> team)) <span class="sc">+</span></span>
<span id="cb1013-3"><a href="mlchapter.html#cb1013-3" aria-hidden="true" tabindex="-1"></a>      <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb1013-4"><a href="mlchapter.html#cb1013-4" aria-hidden="true" tabindex="-1"></a>      <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">&quot;none&quot;</span>) <span class="sc">+</span></span>
<span id="cb1013-5"><a href="mlchapter.html#cb1013-5" aria-hidden="true" tabindex="-1"></a>      <span class="fu">facet_wrap</span>(<span class="sc">~</span> team, <span class="at">nrow =</span> <span class="dv">3</span>)</span></code></pre></div>
<p>When we model the points totals, it seems reasonable to include a random intercept for <code>team</code>. We’ll also include other fixed effects describing the crowd capacity of the teams’ stadiums, and their playing style (e.g. ball possession and number of yellow cards).</p>
<p>The <code>glmmLasso</code> functions won’t automatically centre and scale the data for us, which you’ll recall is recommended to do before fitting a regularised regression model. We’ll create a copy of the data with centred and scaled numeric explanatory variables:</p>
<div class="sourceCode" id="cb1014"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1014-1"><a href="mlchapter.html#cb1014-1" aria-hidden="true" tabindex="-1"></a>soccer_scaled <span class="ot">&lt;-</span> soccer</span>
<span id="cb1014-2"><a href="mlchapter.html#cb1014-2" aria-hidden="true" tabindex="-1"></a>soccer_scaled[, <span class="fu">c</span>(<span class="dv">4</span><span class="sc">:</span><span class="dv">16</span>)] <span class="ot">&lt;-</span> <span class="fu">scale</span>(soccer_scaled[, <span class="fu">c</span>(<span class="dv">4</span><span class="sc">:</span><span class="dv">16</span>)],</span>
<span id="cb1014-3"><a href="mlchapter.html#cb1014-3" aria-hidden="true" tabindex="-1"></a>                           <span class="at">center =</span> <span class="cn">TRUE</span>,</span>
<span id="cb1014-4"><a href="mlchapter.html#cb1014-4" aria-hidden="true" tabindex="-1"></a>                           <span class="at">scale =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p>Next, we’ll run a <code>for</code> loop to find the best <span class="math inline">\(\lambda\)</span>. Because we are interested in fitting a model to this particular dataset rather than making predictions, we will use an in-sample measure of model fit, <span class="math inline">\(BIC\)</span>, to compare the different values of <span class="math inline">\(\lambda\)</span>. The code below is partially adapted from <code>demo("glmmLasso-soccer")</code>:</p>
<div class="sourceCode" id="cb1015"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1015-1"><a href="mlchapter.html#cb1015-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of effects used in model:</span></span>
<span id="cb1015-2"><a href="mlchapter.html#cb1015-2" aria-hidden="true" tabindex="-1"></a>params <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb1015-3"><a href="mlchapter.html#cb1015-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1015-4"><a href="mlchapter.html#cb1015-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Set parameters for optimisation: </span></span>
<span id="cb1015-5"><a href="mlchapter.html#cb1015-5" aria-hidden="true" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">500</span>, <span class="dv">0</span>, <span class="at">by =</span> <span class="sc">-</span><span class="dv">5</span>)</span>
<span id="cb1015-6"><a href="mlchapter.html#cb1015-6" aria-hidden="true" tabindex="-1"></a>BIC_vec <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">Inf</span>, <span class="fu">length</span>(lambda))</span>
<span id="cb1015-7"><a href="mlchapter.html#cb1015-7" aria-hidden="true" tabindex="-1"></a>m_list <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb1015-8"><a href="mlchapter.html#cb1015-8" aria-hidden="true" tabindex="-1"></a>Delta_start <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">t</span>(<span class="fu">rep</span>(<span class="dv">0</span>, params <span class="sc">+</span> <span class="dv">23</span>)))</span>
<span id="cb1015-9"><a href="mlchapter.html#cb1015-9" aria-hidden="true" tabindex="-1"></a>Q_start <span class="ot">&lt;-</span> <span class="fl">0.1</span>  </span>
<span id="cb1015-10"><a href="mlchapter.html#cb1015-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1015-11"><a href="mlchapter.html#cb1015-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Search for optimal lambda:</span></span>
<span id="cb1015-12"><a href="mlchapter.html#cb1015-12" aria-hidden="true" tabindex="-1"></a>pbar <span class="ot">&lt;-</span> <span class="fu">txtProgressBar</span>(<span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> <span class="fu">length</span>(lambda), <span class="at">style =</span> <span class="dv">3</span>)</span>
<span id="cb1015-13"><a href="mlchapter.html#cb1015-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(lambda))</span>
<span id="cb1015-14"><a href="mlchapter.html#cb1015-14" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb1015-15"><a href="mlchapter.html#cb1015-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">setTxtProgressBar</span>(pbar, j)</span>
<span id="cb1015-16"><a href="mlchapter.html#cb1015-16" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1015-17"><a href="mlchapter.html#cb1015-17" aria-hidden="true" tabindex="-1"></a>  m <span class="ot">&lt;-</span> <span class="fu">glmmLasso</span>(points <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> transfer.spendings <span class="sc">+</span></span>
<span id="cb1015-18"><a href="mlchapter.html#cb1015-18" aria-hidden="true" tabindex="-1"></a>                    transfer.receits <span class="sc">+</span></span>
<span id="cb1015-19"><a href="mlchapter.html#cb1015-19" aria-hidden="true" tabindex="-1"></a>                    ave.unfair.score <span class="sc">+</span></span>
<span id="cb1015-20"><a href="mlchapter.html#cb1015-20" aria-hidden="true" tabindex="-1"></a>                    tackles  <span class="sc">+</span></span>
<span id="cb1015-21"><a href="mlchapter.html#cb1015-21" aria-hidden="true" tabindex="-1"></a>                    yellow.card <span class="sc">+</span></span>
<span id="cb1015-22"><a href="mlchapter.html#cb1015-22" aria-hidden="true" tabindex="-1"></a>                    sold.out <span class="sc">+</span></span>
<span id="cb1015-23"><a href="mlchapter.html#cb1015-23" aria-hidden="true" tabindex="-1"></a>                    ball.possession <span class="sc">+</span></span>
<span id="cb1015-24"><a href="mlchapter.html#cb1015-24" aria-hidden="true" tabindex="-1"></a>                    capacity <span class="sc">+</span></span>
<span id="cb1015-25"><a href="mlchapter.html#cb1015-25" aria-hidden="true" tabindex="-1"></a>                    ave.attend,</span>
<span id="cb1015-26"><a href="mlchapter.html#cb1015-26" aria-hidden="true" tabindex="-1"></a>                    <span class="at">rnd =</span> <span class="fu">list</span>(<span class="at">team =</span><span class="sc">~</span> <span class="dv">1</span>),  </span>
<span id="cb1015-27"><a href="mlchapter.html#cb1015-27" aria-hidden="true" tabindex="-1"></a>                    <span class="at">family =</span> <span class="fu">poisson</span>(<span class="at">link =</span> log),</span>
<span id="cb1015-28"><a href="mlchapter.html#cb1015-28" aria-hidden="true" tabindex="-1"></a>                    <span class="at">data =</span> soccer_scaled, </span>
<span id="cb1015-29"><a href="mlchapter.html#cb1015-29" aria-hidden="true" tabindex="-1"></a>                    <span class="at">lambda =</span> lambda[j],</span>
<span id="cb1015-30"><a href="mlchapter.html#cb1015-30" aria-hidden="true" tabindex="-1"></a>                    <span class="at">switch.NR =</span> <span class="cn">FALSE</span>,</span>
<span id="cb1015-31"><a href="mlchapter.html#cb1015-31" aria-hidden="true" tabindex="-1"></a>                    <span class="at">final.re =</span> <span class="cn">TRUE</span>,</span>
<span id="cb1015-32"><a href="mlchapter.html#cb1015-32" aria-hidden="true" tabindex="-1"></a>                    <span class="at">control =</span> <span class="fu">list</span>(<span class="at">start =</span> Delta_start[j,],</span>
<span id="cb1015-33"><a href="mlchapter.html#cb1015-33" aria-hidden="true" tabindex="-1"></a>                                   <span class="at">q_start =</span> Q_start[j]))    </span>
<span id="cb1015-34"><a href="mlchapter.html#cb1015-34" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1015-35"><a href="mlchapter.html#cb1015-35" aria-hidden="true" tabindex="-1"></a>  BIC_vec[j] <span class="ot">&lt;-</span> m<span class="sc">$</span>bic</span>
<span id="cb1015-36"><a href="mlchapter.html#cb1015-36" aria-hidden="true" tabindex="-1"></a>  Delta_start <span class="ot">&lt;-</span> <span class="fu">rbind</span>(Delta_start, m<span class="sc">$</span>Deltamatrix[m<span class="sc">$</span>conv.step,])</span>
<span id="cb1015-37"><a href="mlchapter.html#cb1015-37" aria-hidden="true" tabindex="-1"></a>  Q_start <span class="ot">&lt;-</span> <span class="fu">c</span>(Q_start,m<span class="sc">$</span>Q_long[[m<span class="sc">$</span>conv.step <span class="sc">+</span> <span class="dv">1</span>]])</span>
<span id="cb1015-38"><a href="mlchapter.html#cb1015-38" aria-hidden="true" tabindex="-1"></a>  m_list[[j]] <span class="ot">&lt;-</span> m</span>
<span id="cb1015-39"><a href="mlchapter.html#cb1015-39" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1015-40"><a href="mlchapter.html#cb1015-40" aria-hidden="true" tabindex="-1"></a><span class="fu">close</span>(pbar)</span>
<span id="cb1015-41"><a href="mlchapter.html#cb1015-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1015-42"><a href="mlchapter.html#cb1015-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the optimal model:</span></span>
<span id="cb1015-43"><a href="mlchapter.html#cb1015-43" aria-hidden="true" tabindex="-1"></a>opt_m <span class="ot">&lt;-</span> m_list[[<span class="fu">which.min</span>(BIC_vec)]]</span>
<span id="cb1015-44"><a href="mlchapter.html#cb1015-44" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(opt_m)</span></code></pre></div>
<p>Don’t pay any attention to the p-values in the summary table. Variable selection can affect p-values in all sorts of strange ways, and because we’ve used the lasso to select what variables to include, the p-values presented here are no longer valid.</p>
<p>Note that the coefficients printed by the code above are on the scale of the standardised data. To make them possible to interpret, let’s finish by transforming them back to the original scale of the variables:</p>
<div class="sourceCode" id="cb1016"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1016-1"><a href="mlchapter.html#cb1016-1" aria-hidden="true" tabindex="-1"></a>sds <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">diag</span>(<span class="fu">cov</span>(soccer[, <span class="fu">c</span>(<span class="dv">4</span><span class="sc">:</span><span class="dv">16</span>)])))</span>
<span id="cb1016-2"><a href="mlchapter.html#cb1016-2" aria-hidden="true" tabindex="-1"></a>sd_table <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="dv">1</span><span class="sc">/</span>sds)</span>
<span id="cb1016-3"><a href="mlchapter.html#cb1016-3" aria-hidden="true" tabindex="-1"></a>sd_table[<span class="st">&quot;(Intercept)&quot;</span>,] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb1016-4"><a href="mlchapter.html#cb1016-4" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(opt_m) <span class="sc">*</span> sd_table[<span class="fu">names</span>(<span class="fu">coef</span>(opt_m)),]</span></code></pre></div>
</div>
</div>
<div id="mlmethods" class="section level2 hasAnchor" number="9.5">
<h2><span class="header-section-number">9.5</span> Machine learning models<a href="mlchapter.html#mlmethods" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this section we will have a look at the smorgasbord of machine learning models that can be used for predictive modelling. Some of these models differ from more traditional regression models in that they are black-box models, meaning that we don’t always know what’s going on inside the fitted model. This is in contrast to e.g. linear regression, where we can look at and try to interpret the <span class="math inline">\(\beta\)</span> coefficients. Another difference is that these models have been developed solely for prediction, and so often lack some of the tools that we associate with traditional regression models, like confidence intervals and p-values.</p>
<p>Because we use <code>caret</code> for the model fitting, fitting a new type of model mostly amounts to changing the <code>method</code> argument in <code>train</code>. But please note that I wrote <em>mostly</em> - there are a few other differences e.g. in the preprocessing of the data to which you need to pay attention. We’ll point these out as we go.</p>
<div id="decisiontrees" class="section level3 hasAnchor" number="9.5.1">
<h3><span class="header-section-number">9.5.1</span> Decision trees<a href="mlchapter.html#decisiontrees" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Decision trees are a class of models that can be used for both classification and regression. Their use is perhaps best illustrated by an example, so let’s fit a decision tree to the <code>estates</code> data from Exercise <a href="mlchapter.html#exr:ch8exc2">9.2</a>. We set <code>file_path</code> to the path to <code>estates.xlsx</code> and import and clean the data as before:</p>
<div class="sourceCode" id="cb1017"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1017-1"><a href="mlchapter.html#cb1017-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(openxlsx)</span>
<span id="cb1017-2"><a href="mlchapter.html#cb1017-2" aria-hidden="true" tabindex="-1"></a>estates <span class="ot">&lt;-</span> <span class="fu">read.xlsx</span>(file_path)</span>
<span id="cb1017-3"><a href="mlchapter.html#cb1017-3" aria-hidden="true" tabindex="-1"></a>estates <span class="ot">&lt;-</span> <span class="fu">na.omit</span>(estates)</span></code></pre></div>
<p>Next, we fit a decision tree by setting <code>method = "rpart"</code><a href="#fn61" class="footnote-ref" id="fnref61"><sup>61</sup></a>, which uses functions from the <code>rpart</code> package to fit the tree:</p>
<div class="sourceCode" id="cb1018"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1018-1"><a href="mlchapter.html#cb1018-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb1018-2"><a href="mlchapter.html#cb1018-2" aria-hidden="true" tabindex="-1"></a>tc <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;LOOCV&quot;</span>)</span>
<span id="cb1018-3"><a href="mlchapter.html#cb1018-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1018-4"><a href="mlchapter.html#cb1018-4" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">train</span>(selling_price <span class="sc">~</span> .,</span>
<span id="cb1018-5"><a href="mlchapter.html#cb1018-5" aria-hidden="true" tabindex="-1"></a>           <span class="at">data =</span> estates,</span>
<span id="cb1018-6"><a href="mlchapter.html#cb1018-6" aria-hidden="true" tabindex="-1"></a>           <span class="at">trControl =</span> tc,</span>
<span id="cb1018-7"><a href="mlchapter.html#cb1018-7" aria-hidden="true" tabindex="-1"></a>           <span class="at">method =</span> <span class="st">&quot;rpart&quot;</span>,</span>
<span id="cb1018-8"><a href="mlchapter.html#cb1018-8" aria-hidden="true" tabindex="-1"></a>           <span class="at">tuneGrid =</span> <span class="fu">expand.grid</span>(<span class="at">cp =</span> <span class="dv">0</span>))</span>
<span id="cb1018-9"><a href="mlchapter.html#cb1018-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1018-10"><a href="mlchapter.html#cb1018-10" aria-hidden="true" tabindex="-1"></a>m</span></code></pre></div>
<p>So, what is this? We can plot the resulting decision tree using the <code>rpart.plot</code> package, so let’s install and use that:</p>
<div class="sourceCode" id="cb1019"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1019-1"><a href="mlchapter.html#cb1019-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;rpart.plot&quot;</span>)</span>
<span id="cb1019-2"><a href="mlchapter.html#cb1019-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1019-3"><a href="mlchapter.html#cb1019-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart.plot)</span>
<span id="cb1019-4"><a href="mlchapter.html#cb1019-4" aria-hidden="true" tabindex="-1"></a><span class="fu">prp</span>(m<span class="sc">$</span>finalModel)</span></code></pre></div>
<p>What we see here <em>is</em> our machine learning model - our decision tree. When it is used for prediction, the new observation is fed to the top of the tree, where a question about the new observation is asked: “is <code>tax_value</code> &lt; 1610”? If the answer is <em>yes</em>, the observation continues down the line to the left, to the next question. If the answer is <em>no</em>, it continues down the line to the right, to the question “is <code>tax_value</code> &lt; 2720`, and so on. After a number of questions, the observation reaches a circle - a so-called <em>leaf node</em>, with a number in it. This number is the predicted selling price of the house, which is based on observations in the training data that belong to the same leaf. When the tree is used for classification, the predicted probability of class A is the proportion of observations from the training data in the leaf that belong to class A.</p>
<p><code>prp</code> has a number of parameters that lets us control what our tree plot looks like. <code>box.palette</code>, <code>shadow.col</code>, <code>nn</code>, <code>type</code>, <code>extra</code>, and <code>cex</code> are all useful - read the documentation for <code>prp</code> to see how they affect the plot:</p>
<div class="sourceCode" id="cb1020"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1020-1"><a href="mlchapter.html#cb1020-1" aria-hidden="true" tabindex="-1"></a><span class="fu">prp</span>(m<span class="sc">$</span>finalModel,</span>
<span id="cb1020-2"><a href="mlchapter.html#cb1020-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">box.palette =</span> <span class="st">&quot;RdBu&quot;</span>,</span>
<span id="cb1020-3"><a href="mlchapter.html#cb1020-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">shadow.col =</span> <span class="st">&quot;gray&quot;</span>,</span>
<span id="cb1020-4"><a href="mlchapter.html#cb1020-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">nn =</span> <span class="cn">TRUE</span>,</span>
<span id="cb1020-5"><a href="mlchapter.html#cb1020-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">type =</span> <span class="dv">3</span>,</span>
<span id="cb1020-6"><a href="mlchapter.html#cb1020-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">extra =</span> <span class="dv">1</span>,</span>
<span id="cb1020-7"><a href="mlchapter.html#cb1020-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">cex =</span> <span class="fl">0.75</span>)</span></code></pre></div>
<p>When fitting the model, <code>rpart</code> builds the tree from the top down. At each split, it tries to find a question that will separate subgroups in the data as much as possible. There is no need to standardise the data (in fact, this won’t change the shape of the tree at all).</p>
<p><span class="math display">\[\sim\]</span></p>
<div class="exercise">
<p><span id="exr:ch8exc10" class="exercise"><strong>Exercise 9.14  </strong></span>Fit a classification tree model to the <code>wine</code> data, using <code>pH</code>, <code>alcohol</code>, <code>fixed.acidity</code>, and <code>residual.sugar</code> as explanatory variables. Evaluate its <span class="math inline">\(AUC\)</span> using repeated 10-fold cross-validation.</p>
<ol style="list-style-type: decimal">
<li><p>Plot the resulting decision tree. It is too large to be easily understandable, and needs to be <em>pruned</em>. This is done using the parameter <code>cp</code>. Try increasing the value of <code>cp</code> in <code>tuneGrid = expand.grid(cp = 0)</code> to different values between 0 and 1. What happens with the tree?</p></li>
<li><p>Use <code>tuneGrid = expand.grid(cp = seq(0, 0.01, 0.001))</code> to find an optimal choice of <code>cp</code>. What is the result?</p></li>
</ol>
</div>
<p><a href="solutions.html#ch8solutions10">(Click here to go to the solution.)</a></p>
<p><br></p>
<div class="exercise">
<p><span id="exr:ch8exc10b" class="exercise"><strong>Exercise 9.15  </strong></span>Fit a regression tree model to the <code>bacteria.csv</code> data to see how <code>OD</code> changes with <code>Time</code>, using the data from observations 45 to 90 of the data frame, as in the example in Section <a href="mlchapter.html#extrapolation">9.3.3</a>. Then make predictions for all observations in the dataset. Plot the actual OD values along with your predictions. Does the model extrapolate well?</p>
</div>
<p><a href="solutions.html#ch8solutions10b">(Click here to go to the solution.)</a></p>
<p><br></p>
<div class="exercise">
<p><span id="exr:ch8exc10c" class="exercise"><strong>Exercise 9.16  </strong></span>Fit a classification tree model to the <code>seeds</code> data from Section <a href="eda.html#pca">4.9</a>, using <code>Variety</code> as the response variable and <code>Kernel_length</code> and <code>Compactness</code> as explanatory variables. Plot the resulting decision boundaries, as in Section <a href="mlchapter.html#decisionboundaries">9.1.8</a>. Do they seem reasonable to you?</p>
</div>
<p><a href="solutions.html#ch8solutions10c">(Click here to go to the solution.)</a></p>
</div>
<div id="randomforests" class="section level3 hasAnchor" number="9.5.2">
<h3><span class="header-section-number">9.5.2</span> Random forests<a href="mlchapter.html#randomforests" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Random forest (Breiman, 2001) is an <em>ensemble method</em>, which means that it is based on combining multiple predictive models. In this case, it is a combination of multiple decision trees, that have been built using different subsets of the data. Each tree is fitted to a bootstrap sample of the data (a procedure known as <em>bagging</em>), and at each split only a random subset of the explanatory variables are used. The predictions from these trees are then averaged to obtain a single prediction. While the individual trees in the forest tend to have rather poor performance, the random forest itself often performs better than a single decision tree fitted to all of the data using all variables.</p>
<p>To fit a random forest to the <code>estates</code> data (loaded in the same way as in Section <a href="mlchapter.html#decisiontrees">9.5.1</a>), we set <code>method = "rf"</code>, which will let us do the fitting using functions from the <code>randomForest</code> package. The random forest has a parameter called <code>mtry</code> that determines the number of randomly selected explanatory variables. As a rule-of-thumb, <code>mtry</code> close to <span class="math inline">\(\sqrt{p}\)</span>, where <span class="math inline">\(p\)</span> is the number of explanatory variables in your data, is usually a good choice. When trying to find the best choice for <code>mtry</code> I recommend trying some values close to that.</p>
<p>For the <code>estates</code> data we have 11 explanatory variables, and so a value of <code>mtry</code> close to <span class="math inline">\(\sqrt{11}\approx 3\)</span> could be a good choice. Let’s try a few different values with a 10-fold cross-validation:</p>
<div class="sourceCode" id="cb1021"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1021-1"><a href="mlchapter.html#cb1021-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb1021-2"><a href="mlchapter.html#cb1021-2" aria-hidden="true" tabindex="-1"></a>tc <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>,</span>
<span id="cb1021-3"><a href="mlchapter.html#cb1021-3" aria-hidden="true" tabindex="-1"></a>                   <span class="at">number =</span> <span class="dv">10</span>)</span>
<span id="cb1021-4"><a href="mlchapter.html#cb1021-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1021-5"><a href="mlchapter.html#cb1021-5" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">train</span>(selling_price <span class="sc">~</span> .,</span>
<span id="cb1021-6"><a href="mlchapter.html#cb1021-6" aria-hidden="true" tabindex="-1"></a>           <span class="at">data =</span> estates,</span>
<span id="cb1021-7"><a href="mlchapter.html#cb1021-7" aria-hidden="true" tabindex="-1"></a>           <span class="at">trControl =</span> tc,</span>
<span id="cb1021-8"><a href="mlchapter.html#cb1021-8" aria-hidden="true" tabindex="-1"></a>           <span class="at">method =</span> <span class="st">&quot;rf&quot;</span>,</span>
<span id="cb1021-9"><a href="mlchapter.html#cb1021-9" aria-hidden="true" tabindex="-1"></a>           <span class="at">tuneGrid =</span> <span class="fu">expand.grid</span>(<span class="at">mtry =</span> <span class="dv">2</span><span class="sc">:</span><span class="dv">4</span>))</span>
<span id="cb1021-10"><a href="mlchapter.html#cb1021-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1021-11"><a href="mlchapter.html#cb1021-11" aria-hidden="true" tabindex="-1"></a>m</span></code></pre></div>
<p>In my run, an <code>mtry</code> equal to 4 gave the best results. Let’s try larger values as well, just to see if that gives a better model:</p>
<div class="sourceCode" id="cb1022"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1022-1"><a href="mlchapter.html#cb1022-1" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">train</span>(selling_price <span class="sc">~</span> .,</span>
<span id="cb1022-2"><a href="mlchapter.html#cb1022-2" aria-hidden="true" tabindex="-1"></a>           <span class="at">data =</span> estates,</span>
<span id="cb1022-3"><a href="mlchapter.html#cb1022-3" aria-hidden="true" tabindex="-1"></a>           <span class="at">trControl =</span> tc,</span>
<span id="cb1022-4"><a href="mlchapter.html#cb1022-4" aria-hidden="true" tabindex="-1"></a>           <span class="at">method =</span> <span class="st">&quot;rf&quot;</span>,</span>
<span id="cb1022-5"><a href="mlchapter.html#cb1022-5" aria-hidden="true" tabindex="-1"></a>           <span class="at">tuneGrid =</span> <span class="fu">expand.grid</span>(<span class="at">mtry =</span> <span class="dv">4</span><span class="sc">:</span><span class="dv">10</span>))</span>
<span id="cb1022-6"><a href="mlchapter.html#cb1022-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1022-7"><a href="mlchapter.html#cb1022-7" aria-hidden="true" tabindex="-1"></a>m</span></code></pre></div>
<p>We can visually inspect the impact of <code>mtry</code> by plotting <code>m</code>:</p>
<div class="sourceCode" id="cb1023"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1023-1"><a href="mlchapter.html#cb1023-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(m)</span></code></pre></div>
<p>For this data, a value of <code>mtry</code> that is a little larger than what usually is recommended seems to give the best results. It was a good thing that we didn’t just blindly go with the rule-of-thumb, but instead tried a few different values.</p>
<p>Random forests have a built-in variable importance measure, which is based on measuring how much worse the model fares when the values of each variable are permuted. This is a much more sensible measure of variable importance than that presented in Section <a href="mlchapter.html#varimportance">9.3.2</a>. The importance values are reported on a relative scale, with the value for the most important variable always being 100. Let’s have a look:</p>
<div class="sourceCode" id="cb1024"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1024-1"><a href="mlchapter.html#cb1024-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dotPlot</span>(<span class="fu">varImp</span>(m))</span></code></pre></div>
<p><span class="math display">\[\sim\]</span></p>
<div class="exercise">
<p><span id="exr:ch8exc11" class="exercise"><strong>Exercise 9.17  </strong></span>Fit a decision tree model and a random forest to the <code>wine</code> data, using all variables (except <code>type</code>) as explanatory variables. Evaluate their performance using 10-fold cross-validation. Which model has the best performance?</p>
</div>
<p><a href="solutions.html#ch8solutions11">(Click here to go to the solution.)</a></p>
<p><br></p>
<div class="exercise">
<p><span id="exr:ch8exc11b" class="exercise"><strong>Exercise 9.18  </strong></span>Fit a random forest to the <code>bacteria.csv</code> data to see how <code>OD</code> changes with <code>Time</code>, using the data from observations 45 to 90 of the data frame, as in the example in Section <a href="mlchapter.html#extrapolation">9.3.3</a>. Then make predictions for all observations in the dataset. Plot the actual OD values along with your predictions. Does the model extrapolate well?</p>
</div>
<p><a href="solutions.html#ch8solutions11b">(Click here to go to the solution.)</a></p>
<p><br></p>
<div class="exercise">
<p><span id="exr:ch8exc11c" class="exercise"><strong>Exercise 9.19  </strong></span>Fit a random forest model to the <code>seeds</code> data from Section <a href="eda.html#pca">4.9</a>, using <code>Variety</code> as the response variable and <code>Kernel_length</code> and <code>Compactness</code> as explanatory variables. Plot the resulting decision boundaries, as in Section <a href="mlchapter.html#decisionboundaries">9.1.8</a>. Do they seem reasonable to you?</p>
</div>
<p><a href="solutions.html#ch8solutions11c">(Click here to go to the solution.)</a></p>
</div>
<div id="boosted-trees" class="section level3 hasAnchor" number="9.5.3">
<h3><span class="header-section-number">9.5.3</span> Boosted trees<a href="mlchapter.html#boosted-trees" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Another useful class of ensemble method that relies on combining decision trees are <em>boosted trees</em>. Several different versions are available - we’ll use a version called Stochastic Gradient Boosting (Friedman, 2002), which is available through the <code>gbm</code> package. Let’s start by installing that:</p>
<div class="sourceCode" id="cb1025"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1025-1"><a href="mlchapter.html#cb1025-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;gbm&quot;</span>)</span></code></pre></div>
<p>The decision trees in the ensemble are built sequentially, with each new tree giving more weight to observations for which the previous trees performed poorly. This process is known as <em>boosting</em>.</p>
<p>When fitting a boosted trees model in <code>caret</code>, we set <code>method = "gbm"</code>. There are four parameters that we can use to find a better fit. The two most important are <code>interaction.depth</code>, which determines the maximum tree depth (values greater than <span class="math inline">\(\sqrt{p}\)</span>, where <span class="math inline">\(p\)</span> is the number of explanatory variables in your data, are discouraged) and <code>n.trees</code>, which specifies the number of trees to fit (also known as the number of boosting iterations). Both these can have a large impact on the model fit. Let’s try a few values with the <code>estates</code> data (loaded in the same way as in Section <a href="mlchapter.html#decisiontrees">9.5.1</a>):</p>
<div class="sourceCode" id="cb1026"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1026-1"><a href="mlchapter.html#cb1026-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb1026-2"><a href="mlchapter.html#cb1026-2" aria-hidden="true" tabindex="-1"></a>tc <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>,</span>
<span id="cb1026-3"><a href="mlchapter.html#cb1026-3" aria-hidden="true" tabindex="-1"></a>                   <span class="at">number =</span> <span class="dv">10</span>)</span>
<span id="cb1026-4"><a href="mlchapter.html#cb1026-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1026-5"><a href="mlchapter.html#cb1026-5" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">train</span>(selling_price <span class="sc">~</span> .,</span>
<span id="cb1026-6"><a href="mlchapter.html#cb1026-6" aria-hidden="true" tabindex="-1"></a>           <span class="at">data =</span> estates,</span>
<span id="cb1026-7"><a href="mlchapter.html#cb1026-7" aria-hidden="true" tabindex="-1"></a>           <span class="at">trControl =</span> tc,</span>
<span id="cb1026-8"><a href="mlchapter.html#cb1026-8" aria-hidden="true" tabindex="-1"></a>           <span class="at">method =</span> <span class="st">&quot;gbm&quot;</span>,</span>
<span id="cb1026-9"><a href="mlchapter.html#cb1026-9" aria-hidden="true" tabindex="-1"></a>           <span class="at">tuneGrid =</span> <span class="fu">expand.grid</span>(</span>
<span id="cb1026-10"><a href="mlchapter.html#cb1026-10" aria-hidden="true" tabindex="-1"></a>                 <span class="at">interaction.depth =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>,</span>
<span id="cb1026-11"><a href="mlchapter.html#cb1026-11" aria-hidden="true" tabindex="-1"></a>                 <span class="at">n.trees =</span> <span class="fu">seq</span>(<span class="dv">20</span>, <span class="dv">200</span>, <span class="dv">10</span>),</span>
<span id="cb1026-12"><a href="mlchapter.html#cb1026-12" aria-hidden="true" tabindex="-1"></a>                 <span class="at">shrinkage =</span> <span class="fl">0.1</span>,</span>
<span id="cb1026-13"><a href="mlchapter.html#cb1026-13" aria-hidden="true" tabindex="-1"></a>                 <span class="at">n.minobsinnode =</span> <span class="dv">10</span>),</span>
<span id="cb1026-14"><a href="mlchapter.html#cb1026-14" aria-hidden="true" tabindex="-1"></a>           <span class="at">verbose =</span> <span class="cn">FALSE</span>)</span>
<span id="cb1026-15"><a href="mlchapter.html#cb1026-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1026-16"><a href="mlchapter.html#cb1026-16" aria-hidden="true" tabindex="-1"></a>m</span></code></pre></div>
<p>The setting <code>verbose = FALSE</code> is added used to stop <code>gbm</code> from printing details about each fitted tree.</p>
<p>We can plot the model performance for different settings:</p>
<div class="sourceCode" id="cb1027"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1027-1"><a href="mlchapter.html#cb1027-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(m)</span></code></pre></div>
<p>As you can see, using more trees (a higher number of boosting iterations) seems to lead to a better model. However, if we use too many trees, the model usually overfits, leading to a worse performance in the evaluation:</p>
<div class="sourceCode" id="cb1028"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1028-1"><a href="mlchapter.html#cb1028-1" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">train</span>(selling_price <span class="sc">~</span> .,</span>
<span id="cb1028-2"><a href="mlchapter.html#cb1028-2" aria-hidden="true" tabindex="-1"></a>           <span class="at">data =</span> estates,</span>
<span id="cb1028-3"><a href="mlchapter.html#cb1028-3" aria-hidden="true" tabindex="-1"></a>           <span class="at">trControl =</span> tc,</span>
<span id="cb1028-4"><a href="mlchapter.html#cb1028-4" aria-hidden="true" tabindex="-1"></a>           <span class="at">method =</span> <span class="st">&quot;gbm&quot;</span>,</span>
<span id="cb1028-5"><a href="mlchapter.html#cb1028-5" aria-hidden="true" tabindex="-1"></a>           <span class="at">tuneGrid =</span> <span class="fu">expand.grid</span>(</span>
<span id="cb1028-6"><a href="mlchapter.html#cb1028-6" aria-hidden="true" tabindex="-1"></a>                 <span class="at">interaction.depth =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>,</span>
<span id="cb1028-7"><a href="mlchapter.html#cb1028-7" aria-hidden="true" tabindex="-1"></a>                 <span class="at">n.trees =</span> <span class="fu">seq</span>(<span class="dv">25</span>, <span class="dv">500</span>, <span class="dv">25</span>),</span>
<span id="cb1028-8"><a href="mlchapter.html#cb1028-8" aria-hidden="true" tabindex="-1"></a>                 <span class="at">shrinkage =</span> <span class="fl">0.1</span>,</span>
<span id="cb1028-9"><a href="mlchapter.html#cb1028-9" aria-hidden="true" tabindex="-1"></a>                 <span class="at">n.minobsinnode =</span> <span class="dv">10</span>),</span>
<span id="cb1028-10"><a href="mlchapter.html#cb1028-10" aria-hidden="true" tabindex="-1"></a>           <span class="at">verbose =</span> <span class="cn">FALSE</span>)</span>
<span id="cb1028-11"><a href="mlchapter.html#cb1028-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1028-12"><a href="mlchapter.html#cb1028-12" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(m)</span></code></pre></div>
<p>A table and plot of variable importance is given by <code>summary</code>:</p>
<div class="sourceCode" id="cb1029"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1029-1"><a href="mlchapter.html#cb1029-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code></pre></div>
<p>In many problems, boosted trees are among the best-performing models. They do however require a lot of tuning, which can be time-consuming, both in terms of how long it takes to run the tuning and in terms of how much time you have to spend fiddling with the different parameters. Several different implementations of boosted trees are available in <code>caret</code>. A good alternative to <code>gbm</code> is <code>xgbTree</code> from the <code>xgboost</code> package. I’ve chosen not to use that for the examples here, as it often is slower to train due to having a larger number of hyperparameters (which in return makes it even more flexible!).</p>
<p><span class="math display">\[\sim\]</span></p>
<div class="exercise">
<p><span id="exr:ch8exc12" class="exercise"><strong>Exercise 9.20  </strong></span>Fit a boosted trees model to the <code>wine</code> data, using all variables (except <code>type</code>) as explanatory variables. Evaluate its performance using repeated 10-fold cross-validation. What is the best <span class="math inline">\(AUC\)</span> that you can get by tuning the model parameters?</p>
</div>
<p><a href="solutions.html#ch8solutions12">(Click here to go to the solution.)</a></p>
<p><br></p>
<div class="exercise">
<p><span id="exr:ch8exc12a" class="exercise"><strong>Exercise 9.21  </strong></span>Fit a boosted trees regression model to the <code>bacteria.csv</code> data to see how <code>OD</code> changes with <code>Time</code>, using the data from observations 45 to 90 of the data frame, as in the example in Section <a href="mlchapter.html#extrapolation">9.3.3</a>. Then make predictions for all observations in the dataset. Plot the actual OD values along with your predictions. Does the model extrapolate well?</p>
</div>
<p><a href="solutions.html#ch8solutions12a">(Click here to go to the solution.)</a></p>
<p><br></p>
<div class="exercise">
<p><span id="exr:ch8exc12c" class="exercise"><strong>Exercise 9.22  </strong></span>Fit a boosted trees model to the <code>seeds</code> data from Section <a href="eda.html#pca">4.9</a>, using <code>Variety</code> as the response variable and <code>Kernel_length</code> and <code>Compactness</code> as explanatory variables. Plot the resulting decision boundaries, as in Section <a href="mlchapter.html#decisionboundaries">9.1.8</a>. Do they seem reasonable to you?</p>
</div>
<p><a href="solutions.html#ch8solutions12c">(Click here to go to the solution.)</a></p>
</div>
<div id="model-trees" class="section level3 hasAnchor" number="9.5.4">
<h3><span class="header-section-number">9.5.4</span> Model trees<a href="mlchapter.html#model-trees" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A downside to all the tree-based models that we’ve seen so far is their inability to extrapolate when the explanatory variables of a new observation are outside the range in the training data. You’ve seen this e.g. in Exercise <a href="mlchapter.html#exr:ch8exc10b">9.15</a>. Methods based on <em>model trees</em> solve this problem by fitting e.g. a linear model in each leaf node of the decision tree. Ordinary decision trees fit regression models that are piecewise constant, while model trees utilising linear regression fit regression models that are piecewise linear.</p>
<p>The model trees that we’ll now have a look at aren’t available in <code>caret</code>, meaning that we can’t use its functions for evaluating models using cross-validations. We can however still perform cross-validation using a <code>for</code> loop, as we did in the beginning of Section <a href="mlchapter.html#loocv">9.1.3</a>. Model trees are available through the <code>partykit</code> package, which we’ll install next. We’ll also install <code>ggparty</code>, which contains tools for creating good-looking plots of model trees:</p>
<div class="sourceCode" id="cb1030"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1030-1"><a href="mlchapter.html#cb1030-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="fu">c</span>(<span class="st">&quot;partykit&quot;</span>, <span class="st">&quot;ggparty&quot;</span>))</span></code></pre></div>
<p>The model trees in <code>partykit</code> differ from classical decision tress not only in how the nodes are treated, but also in how the splits are determined; see Zeileis et al. (2008) for details. To illustrate their use, we’ll return to the <code>estates</code> data. The model formula for model trees has two parts. The first specifies the response variable and what variables to use for the linear models in the nodes, and the second part specifies what variables to use for the splits. In our example, we’ll use <code>living_area</code> as the sole explanatory variable in our linear models, and <code>location</code>, <code>build_year</code>, <code>tax_value</code>, and <code>plot_area</code> for the splits (in this particular example, there is no overlap between the variables used for the linear models and the variables used for the splits, but its perfectly fine to have an overlap if you like!).</p>
<p>As in Section <a href="mlchapter.html#decisiontrees">9.5.1</a>, we set <code>file_path</code> to the path to <code>estates.xlsx</code> and import and clean the data. We can then fit a model tree with linear regressions in the nodes using <code>lmtree</code>:</p>
<div class="sourceCode" id="cb1031"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1031-1"><a href="mlchapter.html#cb1031-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(openxlsx)</span>
<span id="cb1031-2"><a href="mlchapter.html#cb1031-2" aria-hidden="true" tabindex="-1"></a>estates <span class="ot">&lt;-</span> <span class="fu">read.xlsx</span>(file_path)</span>
<span id="cb1031-3"><a href="mlchapter.html#cb1031-3" aria-hidden="true" tabindex="-1"></a>estates <span class="ot">&lt;-</span> <span class="fu">na.omit</span>(estates)</span>
<span id="cb1031-4"><a href="mlchapter.html#cb1031-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1031-5"><a href="mlchapter.html#cb1031-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Make location a factor variable:</span></span>
<span id="cb1031-6"><a href="mlchapter.html#cb1031-6" aria-hidden="true" tabindex="-1"></a>estates<span class="sc">$</span>location <span class="ot">&lt;-</span> <span class="fu">factor</span>(estates<span class="sc">$</span>location)</span>
<span id="cb1031-7"><a href="mlchapter.html#cb1031-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1031-8"><a href="mlchapter.html#cb1031-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit model tree:</span></span>
<span id="cb1031-9"><a href="mlchapter.html#cb1031-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(partykit)</span>
<span id="cb1031-10"><a href="mlchapter.html#cb1031-10" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">lmtree</span>(selling_price <span class="sc">~</span> living_area <span class="sc">|</span> location <span class="sc">+</span> build_year <span class="sc">+</span></span>
<span id="cb1031-11"><a href="mlchapter.html#cb1031-11" aria-hidden="true" tabindex="-1"></a>                                           tax_value <span class="sc">+</span> plot_area,</span>
<span id="cb1031-12"><a href="mlchapter.html#cb1031-12" aria-hidden="true" tabindex="-1"></a>            <span class="at">data =</span> estates)</span></code></pre></div>
<p>Next, we plot the resulting tree - make sure that you enlarge your Plot panel so that you can see the linear models fitted in each node:</p>
<div class="sourceCode" id="cb1032"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1032-1"><a href="mlchapter.html#cb1032-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggparty)</span>
<span id="cb1032-2"><a href="mlchapter.html#cb1032-2" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(m)</span></code></pre></div>
<p>By adding additional arguments to <code>lmtree</code>, we can control e.g. the amount of pruning. You can find a list of all the available arguments by having a look at <code>?mob_control</code>. To do automated likelihood-based pruning, we can use <code>prune = "AIC"</code> or <code>prune = "BIC"</code>, which yields a slightly shorter tree:</p>
<div class="sourceCode" id="cb1033"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1033-1"><a href="mlchapter.html#cb1033-1" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">lmtree</span>(selling_price <span class="sc">~</span> living_area <span class="sc">|</span> location <span class="sc">+</span> build_year <span class="sc">+</span></span>
<span id="cb1033-2"><a href="mlchapter.html#cb1033-2" aria-hidden="true" tabindex="-1"></a>                                           tax_value <span class="sc">+</span> plot_area,</span>
<span id="cb1033-3"><a href="mlchapter.html#cb1033-3" aria-hidden="true" tabindex="-1"></a>            <span class="at">data =</span> estates,</span>
<span id="cb1033-4"><a href="mlchapter.html#cb1033-4" aria-hidden="true" tabindex="-1"></a>            <span class="at">prune =</span> <span class="st">&quot;BIC&quot;</span>)</span>
<span id="cb1033-5"><a href="mlchapter.html#cb1033-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1033-6"><a href="mlchapter.html#cb1033-6" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(m)</span></code></pre></div>
<p>As per usual, we can use <code>predict</code> to make predictions from our model. Similarly to how we used <code>lmtree</code> above, we can use <code>glmtree</code> to fit a logistic regression in each node, which can be useful for classification problems. We can also fit Poisson regressions in the nodes using <code>glmtree</code>, creating more flexible Poisson regression models. For more information on how you can control how model trees are plotted using <code>ggparty</code>, have a look at <code>vignette("ggparty-graphic-partying")</code>.</p>
<p><span class="math display">\[\sim\]</span></p>
<div class="exercise">
<p><span id="exr:ch8exc12ab" class="exercise"><strong>Exercise 9.23  </strong></span>In this exercise, you will fit model trees to the <code>bacteria.csv</code> data to see how <code>OD</code> changes with <code>Time</code>.</p>
<ol style="list-style-type: decimal">
<li><p>Fit a model tree and a decision tree, using the data from observations 45 to 90 of the data frame, as in the example in Section <a href="mlchapter.html#extrapolation">9.3.3</a>. Then make predictions for all observations in the dataset. Plot the actual OD values along with your predictions. Do the models extrapolate well?</p></li>
<li><p>Now, fit a model tree and a decision tree using the data from observations 20 to 120 of the data frame. Then make predictions for all observations in the dataset. Does this improve the models’ ability to extrapolate?</p></li>
</ol>
</div>
<p><a href="solutions.html#ch8solutions12ab">(Click here to go to the solution.)</a></p>
</div>
<div id="discriminant-analysis" class="section level3 hasAnchor" number="9.5.5">
<h3><span class="header-section-number">9.5.5</span> Discriminant analysis<a href="mlchapter.html#discriminant-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In <em>linear discriminant analysis</em> (LDA), prior knowledge about how common different classes are is used to classify new observations using <em>Bayes’ theorem</em>. It relies on the assumption that the data from each class is generated by a multivariate normal distribution, and that all classes share a common covariance matrix. The resulting decision boundary is a hyperplane.</p>
<p>As part of fitting the model, LDA creates linear combinations of the explanatory variables, which are used for separating different classes. These can be used both for classification and as a supervised alternative to principal components analysis (PCA, Section <a href="eda.html#pca">4.9</a>).</p>
<p>LDA does not require any tuning. It does however allow you to specify prior class probabilities if you like, using the <code>prior</code> argument, allowing for Bayesian classification. If you don’t provide a prior, the class proportions in the training data will be used instead. Here is an example using the <code>wine</code> data from Section <a href="mlchapter.html#classifieraccuracy">9.1.7</a>:</p>
<div class="sourceCode" id="cb1034"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1034-1"><a href="mlchapter.html#cb1034-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb1034-2"><a href="mlchapter.html#cb1034-2" aria-hidden="true" tabindex="-1"></a>tc <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;repeatedcv&quot;</span>,</span>
<span id="cb1034-3"><a href="mlchapter.html#cb1034-3" aria-hidden="true" tabindex="-1"></a>                   <span class="at">number =</span> <span class="dv">10</span>, <span class="at">repeats =</span> <span class="dv">100</span>,</span>
<span id="cb1034-4"><a href="mlchapter.html#cb1034-4" aria-hidden="true" tabindex="-1"></a>                   <span class="at">summaryFunction =</span> twoClassSummary,</span>
<span id="cb1034-5"><a href="mlchapter.html#cb1034-5" aria-hidden="true" tabindex="-1"></a>                   <span class="at">savePredictions =</span> <span class="cn">TRUE</span>,</span>
<span id="cb1034-6"><a href="mlchapter.html#cb1034-6" aria-hidden="true" tabindex="-1"></a>                   <span class="at">classProbs =</span> <span class="cn">TRUE</span>)</span>
<span id="cb1034-7"><a href="mlchapter.html#cb1034-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1034-8"><a href="mlchapter.html#cb1034-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Without the use of a prior:</span></span>
<span id="cb1034-9"><a href="mlchapter.html#cb1034-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Prior probability of a red wine is 0.25 (i.e. the</span></span>
<span id="cb1034-10"><a href="mlchapter.html#cb1034-10" aria-hidden="true" tabindex="-1"></a><span class="co"># proportion of red wines in the dataset).</span></span>
<span id="cb1034-11"><a href="mlchapter.html#cb1034-11" aria-hidden="true" tabindex="-1"></a>m_no_prior <span class="ot">&lt;-</span> <span class="fu">train</span>(type <span class="sc">~</span>  pH <span class="sc">+</span> alcohol <span class="sc">+</span> fixed.acidity <span class="sc">+</span></span>
<span id="cb1034-12"><a href="mlchapter.html#cb1034-12" aria-hidden="true" tabindex="-1"></a>                      residual.sugar,</span>
<span id="cb1034-13"><a href="mlchapter.html#cb1034-13" aria-hidden="true" tabindex="-1"></a>           <span class="at">data =</span> wine,</span>
<span id="cb1034-14"><a href="mlchapter.html#cb1034-14" aria-hidden="true" tabindex="-1"></a>           <span class="at">trControl =</span> tc,</span>
<span id="cb1034-15"><a href="mlchapter.html#cb1034-15" aria-hidden="true" tabindex="-1"></a>           <span class="at">method =</span> <span class="st">&quot;lda&quot;</span>,</span>
<span id="cb1034-16"><a href="mlchapter.html#cb1034-16" aria-hidden="true" tabindex="-1"></a>           <span class="at">metric =</span> <span class="st">&quot;ROC&quot;</span>)</span>
<span id="cb1034-17"><a href="mlchapter.html#cb1034-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1034-18"><a href="mlchapter.html#cb1034-18" aria-hidden="true" tabindex="-1"></a><span class="co"># With a prior:</span></span>
<span id="cb1034-19"><a href="mlchapter.html#cb1034-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Prior probability of a red wine is set to be 0.5.</span></span>
<span id="cb1034-20"><a href="mlchapter.html#cb1034-20" aria-hidden="true" tabindex="-1"></a>m_with_prior <span class="ot">&lt;-</span> <span class="fu">train</span>(type <span class="sc">~</span>  pH <span class="sc">+</span> alcohol <span class="sc">+</span> fixed.acidity <span class="sc">+</span></span>
<span id="cb1034-21"><a href="mlchapter.html#cb1034-21" aria-hidden="true" tabindex="-1"></a>                        residual.sugar,</span>
<span id="cb1034-22"><a href="mlchapter.html#cb1034-22" aria-hidden="true" tabindex="-1"></a>           <span class="at">data =</span> wine,</span>
<span id="cb1034-23"><a href="mlchapter.html#cb1034-23" aria-hidden="true" tabindex="-1"></a>           <span class="at">trControl =</span> tc,</span>
<span id="cb1034-24"><a href="mlchapter.html#cb1034-24" aria-hidden="true" tabindex="-1"></a>           <span class="at">method =</span> <span class="st">&quot;lda&quot;</span>,</span>
<span id="cb1034-25"><a href="mlchapter.html#cb1034-25" aria-hidden="true" tabindex="-1"></a>           <span class="at">metric =</span> <span class="st">&quot;ROC&quot;</span>,</span>
<span id="cb1034-26"><a href="mlchapter.html#cb1034-26" aria-hidden="true" tabindex="-1"></a>           <span class="at">prior =</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>))</span>
<span id="cb1034-27"><a href="mlchapter.html#cb1034-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1034-28"><a href="mlchapter.html#cb1034-28" aria-hidden="true" tabindex="-1"></a>m_no_prior</span>
<span id="cb1034-29"><a href="mlchapter.html#cb1034-29" aria-hidden="true" tabindex="-1"></a>m_with_prior</span></code></pre></div>
<p>As I mentioned, LDA can also be used as an alternative to PCA, which we studied in Section <a href="eda.html#pca">4.9</a>. Let’s have a look at the <code>seeds</code> data that the we used in that section:</p>
<div class="sourceCode" id="cb1035"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1035-1"><a href="mlchapter.html#cb1035-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The data is downloaded from the UCI Machine Learning Repository:</span></span>
<span id="cb1035-2"><a href="mlchapter.html#cb1035-2" aria-hidden="true" tabindex="-1"></a><span class="co"># http://archive.ics.uci.edu/ml/datasets/seeds</span></span>
<span id="cb1035-3"><a href="mlchapter.html#cb1035-3" aria-hidden="true" tabindex="-1"></a>seeds <span class="ot">&lt;-</span> <span class="fu">read.table</span>(<span class="st">&quot;https://tinyurl.com/seedsdata&quot;</span>,</span>
<span id="cb1035-4"><a href="mlchapter.html#cb1035-4" aria-hidden="true" tabindex="-1"></a>        <span class="at">col.names =</span> <span class="fu">c</span>(<span class="st">&quot;Area&quot;</span>, <span class="st">&quot;Perimeter&quot;</span>, <span class="st">&quot;Compactness&quot;</span>,</span>
<span id="cb1035-5"><a href="mlchapter.html#cb1035-5" aria-hidden="true" tabindex="-1"></a>         <span class="st">&quot;Kernel_length&quot;</span>, <span class="st">&quot;Kernel_width&quot;</span>, <span class="st">&quot;Asymmetry&quot;</span>,</span>
<span id="cb1035-6"><a href="mlchapter.html#cb1035-6" aria-hidden="true" tabindex="-1"></a>         <span class="st">&quot;Groove_length&quot;</span>, <span class="st">&quot;Variety&quot;</span>))</span>
<span id="cb1035-7"><a href="mlchapter.html#cb1035-7" aria-hidden="true" tabindex="-1"></a>seeds<span class="sc">$</span>Variety <span class="ot">&lt;-</span> <span class="fu">factor</span>(seeds<span class="sc">$</span>Variety)</span></code></pre></div>
<p>When <code>caret</code> fits an LDA, it uses the <code>lda</code> function from the <code>MASS</code> package, which uses the same syntax as <code>lm</code>. If we use <code>lda</code> directly, without involving <code>caret</code>, we can extract the scores (linear combinations of variables) for all observations. We can then plot these, to get something similar to a plot of the first two principal components. There is a difference though - PCA seeks to create new variables that summarise as much as possible of the variation in the data, whereas LDA seeks to create new variables that can be used to discriminate between pre-specified groups.</p>
<div class="sourceCode" id="cb1036"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1036-1"><a href="mlchapter.html#cb1036-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Run an LDA:</span></span>
<span id="cb1036-2"><a href="mlchapter.html#cb1036-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb1036-3"><a href="mlchapter.html#cb1036-3" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">lda</span>(Variety <span class="sc">~</span> ., <span class="at">data =</span> seeds)</span>
<span id="cb1036-4"><a href="mlchapter.html#cb1036-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1036-5"><a href="mlchapter.html#cb1036-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the LDA scores:</span></span>
<span id="cb1036-6"><a href="mlchapter.html#cb1036-6" aria-hidden="true" tabindex="-1"></a>lda_preds <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Type =</span> seeds<span class="sc">$</span>Variety,</span>
<span id="cb1036-7"><a href="mlchapter.html#cb1036-7" aria-hidden="true" tabindex="-1"></a>                        <span class="at">Score =</span> <span class="fu">predict</span>(m)<span class="sc">$</span>x)</span>
<span id="cb1036-8"><a href="mlchapter.html#cb1036-8" aria-hidden="true" tabindex="-1"></a><span class="fu">View</span>(lda_preds)</span>
<span id="cb1036-9"><a href="mlchapter.html#cb1036-9" aria-hidden="true" tabindex="-1"></a><span class="co"># There are 3 varieties of seeds. LDA creates 1 less new variable</span></span>
<span id="cb1036-10"><a href="mlchapter.html#cb1036-10" aria-hidden="true" tabindex="-1"></a><span class="co"># than the number of categories - so 2 in this case. We can</span></span>
<span id="cb1036-11"><a href="mlchapter.html#cb1036-11" aria-hidden="true" tabindex="-1"></a><span class="co"># therefore visualise these using a simple scatterplot.</span></span>
<span id="cb1036-12"><a href="mlchapter.html#cb1036-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1036-13"><a href="mlchapter.html#cb1036-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the two LDA scores for each observation to get a visual</span></span>
<span id="cb1036-14"><a href="mlchapter.html#cb1036-14" aria-hidden="true" tabindex="-1"></a><span class="co"># representation of the data:</span></span>
<span id="cb1036-15"><a href="mlchapter.html#cb1036-15" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb1036-16"><a href="mlchapter.html#cb1036-16" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(lda_preds, <span class="fu">aes</span>(Score.LD1, Score.LD2, <span class="at">colour =</span> Type)) <span class="sc">+</span></span>
<span id="cb1036-17"><a href="mlchapter.html#cb1036-17" aria-hidden="true" tabindex="-1"></a>      <span class="fu">geom_point</span>()</span></code></pre></div>
<p><span class="math display">\[\sim\]</span></p>
<div class="exercise">
<p><span id="exr:ch8exc12b" class="exercise"><strong>Exercise 9.24  </strong></span>An alternative to linear discriminant analysis is <em>quadratic discriminant analysis</em> (QDA). This is closely related to LDA, the difference being that we no longer assume that the classes have equal covariance matrices. The resulting decision boundaries are quadratic (i.e. non-linear). Run a QDA on the <code>wine</code> data, by using <code>method = "qda"</code> in <code>train</code>.</p>
</div>
<p><a href="solutions.html#ch8solutions12b">(Click here to go to the solution.)</a></p>
<p><br></p>
<div class="exercise">
<p><span id="exr:ch8exc12ad" class="exercise"><strong>Exercise 9.25  </strong></span>Fit an LDA classifier and a QDA classifier to the <code>seeds</code> data from Section <a href="eda.html#pca">4.9</a>, using <code>Variety</code> as the response variable and <code>Kernel_length</code> and <code>Compactness</code> as explanatory variables. Plot the resulting decision boundaries, as in Section <a href="mlchapter.html#decisionboundaries">9.1.8</a>. Do they seem reasonable to you?</p>
</div>
<p><a href="solutions.html#ch8solutions12ad">(Click here to go to the solution.)</a></p>
<p><br></p>
<div class="exercise">
<p><span id="exr:ch8exc12ae" class="exercise"><strong>Exercise 9.26  </strong></span>An even more flexible version of discriminant analysis is MDA, <em>mixture discriminant analysis</em>, which uses normal mixture distributions for classification. That way, we no longer have to rely on the assumption of normality. It is available through the <code>mda</code> package, and can be used in <code>train</code> with `<code>method = "mda"</code>. Fit an MDA classifier to the <code>seeds</code> data from Section <a href="eda.html#pca">4.9</a>, using <code>Variety</code> as the response variable and <code>Kernel_length</code> and <code>Compactness</code> as explanatory variables. Plot the resulting decision boundaries, as in Section <a href="mlchapter.html#decisionboundaries">9.1.8</a>. Do they seem reasonable to you?</p>
</div>
<p><a href="solutions.html#ch8solutions12ae">(Click here to go to the solution.)</a></p>
</div>
<div id="support-vector-machines" class="section level3 hasAnchor" number="9.5.6">
<h3><span class="header-section-number">9.5.6</span> Support vector machines<a href="mlchapter.html#support-vector-machines" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Support vector machines, SVM, is a flexible class of methods for classification and regression. Like LDA, they rely on hyperplanes to separate classes. Unlike LDA however, more weight is put to points close to the border between classes. Moreover, the data is projected into a higher-dimensional space, with the intention of creating a projection that yields a good separation between classes. Several different projection methods can be used, typically represented by <em>kernels</em> - functions that measure the inner product in these high-dimensional spaces.</p>
<p>Despite the fancy mathematics, using SVM’s is not that difficult. With <code>caret</code>, we can fit many SVM’s with many different types of kernels using the <code>kernlab</code> package. Let’s install it:</p>
<div class="sourceCode" id="cb1037"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1037-1"><a href="mlchapter.html#cb1037-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;kernlab&quot;</span>)</span></code></pre></div>
<p>The simplest SVM uses a linear kernel, creating a linear classification that is reminiscent of LDA. Let’s look at an example using the <code>wine</code> data from Section <a href="mlchapter.html#classifieraccuracy">9.1.7</a>. The parameter <span class="math inline">\(C\)</span> is a regularisation parameter:</p>
<div class="sourceCode" id="cb1038"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1038-1"><a href="mlchapter.html#cb1038-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb1038-2"><a href="mlchapter.html#cb1038-2" aria-hidden="true" tabindex="-1"></a>tc <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>,</span>
<span id="cb1038-3"><a href="mlchapter.html#cb1038-3" aria-hidden="true" tabindex="-1"></a>                   <span class="at">number =</span> <span class="dv">10</span>, </span>
<span id="cb1038-4"><a href="mlchapter.html#cb1038-4" aria-hidden="true" tabindex="-1"></a>                   <span class="at">summaryFunction =</span> twoClassSummary,</span>
<span id="cb1038-5"><a href="mlchapter.html#cb1038-5" aria-hidden="true" tabindex="-1"></a>                   <span class="at">savePredictions =</span> <span class="cn">TRUE</span>,</span>
<span id="cb1038-6"><a href="mlchapter.html#cb1038-6" aria-hidden="true" tabindex="-1"></a>                   <span class="at">classProbs =</span> <span class="cn">TRUE</span>)</span>
<span id="cb1038-7"><a href="mlchapter.html#cb1038-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1038-8"><a href="mlchapter.html#cb1038-8" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">train</span>(type <span class="sc">~</span>  pH <span class="sc">+</span> alcohol <span class="sc">+</span> fixed.acidity <span class="sc">+</span> residual.sugar,</span>
<span id="cb1038-9"><a href="mlchapter.html#cb1038-9" aria-hidden="true" tabindex="-1"></a>           <span class="at">data =</span> wine,</span>
<span id="cb1038-10"><a href="mlchapter.html#cb1038-10" aria-hidden="true" tabindex="-1"></a>           <span class="at">trControl =</span> tc,</span>
<span id="cb1038-11"><a href="mlchapter.html#cb1038-11" aria-hidden="true" tabindex="-1"></a>           <span class="at">method =</span> <span class="st">&quot;svmLinear&quot;</span>,</span>
<span id="cb1038-12"><a href="mlchapter.html#cb1038-12" aria-hidden="true" tabindex="-1"></a>           <span class="at">tuneGrid =</span> <span class="fu">expand.grid</span>(<span class="at">C =</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">2</span>)),</span>
<span id="cb1038-13"><a href="mlchapter.html#cb1038-13" aria-hidden="true" tabindex="-1"></a>           <span class="at">metric =</span> <span class="st">&quot;ROC&quot;</span>)</span></code></pre></div>
<p>There are a number of other nonlinear kernels that can be used, with different hyperparameters that can be tuned. Without going into details about the different kernels, some important examples are:</p>
<ul>
<li><code>method = "svmPoly</code>: polynomial kernel. The tuning parameters are <code>degree</code> (the polynomial degree, e.g. 3 for a cubic polynomial), <code>scale</code> (scale) and <code>C</code> (regularisation).</li>
<li><code>method = "svmRadialCost</code>: radial basis/Gaussian kernel. The only tuning parameter is <code>C</code> (regularisation).</li>
<li><code>method = "svmRadialSigma</code>: radial basis/Gaussian kernel with tuning of <span class="math inline">\(\sigma\)</span>. The tuning parameters are <code>C</code> (regularisation) and <code>sigma</code> (<span class="math inline">\(\sigma\)</span>).</li>
<li><code>method = "svmSpectrumString</code>: spectrum string kernel. The tuning parameters are <code>C</code> (regularisation) and <code>length</code> (length).</li>
</ul>
<p><span class="math display">\[\sim\]</span></p>
<div class="exercise">
<p><span id="exr:ch8exc12cc" class="exercise"><strong>Exercise 9.27  </strong></span>Fit an SVM to the <code>wine</code> data, using all variables (except <code>type</code>) as explanatory variables, using a kernel of your choice. Evaluate its performance using repeated 10-fold cross-validation. What is the best <span class="math inline">\(AUC\)</span> that you can get by tuning the model parameters?</p>
</div>
<p><a href="solutions.html#ch8solutions12cc">(Click here to go to the solution.)</a></p>
<p><br></p>
<div class="exercise">
<p><span id="exr:ch8exc12d" class="exercise"><strong>Exercise 9.28  </strong></span>In this exercise, you will SVM regression models to the <code>bacteria.csv</code> data to see how <code>OD</code> changes with <code>Time</code>.</p>
<ol style="list-style-type: decimal">
<li><p>Fit an SVM, using the data from observations 45 to 90 of the data frame, as in the example in Section <a href="mlchapter.html#extrapolation">9.3.3</a>. Then make predictions for all observations in the dataset. Plot the actual OD values along with your predictions. Does the model extrapolate well?</p></li>
<li><p>Now, fit an SVM using the data from observations 20 to 120 of the data frame. Then make predictions for all observations in the dataset. Does this improve the model’s ability to extrapolate?</p></li>
</ol>
</div>
<p><a href="solutions.html#ch8solutions12d">(Click here to go to the solution.)</a></p>
<p><br></p>
<div class="exercise">
<p><span id="exr:ch8exc12af" class="exercise"><strong>Exercise 9.29  </strong></span>Fit SVM classifiers with different kernels to the <code>seeds</code> data from Section <a href="eda.html#pca">4.9</a>, using <code>Variety</code> as the response variable and <code>Kernel_length</code> and <code>Compactness</code> as explanatory variables. Plot the resulting decision boundaries, as in Section <a href="mlchapter.html#decisionboundaries">9.1.8</a>. Do they seem reasonable to you?</p>
</div>
<p><a href="solutions.html#ch8solutions12af">(Click here to go to the solution.)</a></p>
</div>
<div id="nearest-neighbours-classifiers" class="section level3 hasAnchor" number="9.5.7">
<h3><span class="header-section-number">9.5.7</span> Nearest neighbours classifiers<a href="mlchapter.html#nearest-neighbours-classifiers" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In classification problems with numeric explanatory variables, a natural approach to finding the class of a new observation is to look at the classes of neighbouring observations, i.e. of observations that are “close” to it in some sense. This requires a distance measure, to measure how close observations are. A kNN classifier classifies the new observations by letting the <span class="math inline">\(k\)</span> Nearest Neighbours – the <span class="math inline">\(k\)</span> points that are the closest to the observation – ”vote” about the class of the new observation. As an example, if <span class="math inline">\(k=3\)</span>, two of the three closest neighbours belong to class A, and one of the three closest neighbours belongs to class B, then the new observation will be classified as A. If we like, we can also use the proportion of different classes among the nearest neighbours to get predicted probabilities of the classes (in our example: 2/3 for A, 1/3 for B).</p>
<p>What makes kNN appealing is that it doesn’t require a complicated model - instead, we simply compare observations to each other. A major downside is that we have to compute the distance between each new observations and all observations in the training data, which can be time-consuming if you have large datasets. Moreover, we consequently have to store the training data indefinitely, as it is used each time we use the model for prediction. This can cause problems e.g. if the data are of a kind that falls under the European GDPR regulation, which limits how long data can be stored, and for what purpose.</p>
<p>A common choice of distance measure, which is the default when we set <code>method = "knn"</code> in <code>train</code>, is the common Euclidean distance. We need to take care to standardise our variables before using it, as variables with a high variance otherwise automatically will contribute more to the Euclidean distance. Unlike in regularised regression, <code>caret</code> does <em>not</em> do this for us. Instead, we must provide the argument <code>preProcess = c("center", "scale")</code> to <code>train</code>.</p>
<p>An important choice in kNN is what value to use for the parameter <span class="math inline">\(k\)</span>. If <span class="math inline">\(k\)</span> is too small, we use too little information, and if <span class="math inline">\(k\)</span> is to large, the classifier will become prone to classify all observations as belonging to the most common class in the training data. <span class="math inline">\(k\)</span> is usually chosen using cross-validation or bootstrapping. To have <code>caret</code> find a good choice of <span class="math inline">\(k\)</span> for us (like we did with <span class="math inline">\(\lambda\)</span> in regularised regression models), we use the argument <code>tuneLength</code> in train, e.g. <code>tuneLength = 15</code> to try 15 different values of <span class="math inline">\(k\)</span>.</p>
<p>By now, I think you’ve seen enough examples of how to fit models in <code>caret</code> that you can figure out how to fit a model with <code>knn</code> on your own (using the information above, of course). In the next exercise, you will give kNN a go, using the <code>wine</code> data.</p>
<p><span class="math display">\[\sim\]</span></p>
<div class="exercise">
<p><span id="exr:ch8exc9" class="exercise"><strong>Exercise 9.30  </strong></span>Fit a kNN classification model to the <code>wine</code> data, using <code>pH</code>, <code>alcohol</code>, <code>fixed.acidity</code>, and <code>residual.sugar</code> as explanatory variables. Evaluate its performance using 10-fold cross-validation, using <span class="math inline">\(AUC\)</span> to choose the best <span class="math inline">\(k\)</span>. Is it better than the logistic regression models that you fitted in Exercise <a href="mlchapter.html#exr:ch8exc5">9.5</a>?</p>
</div>
<p><a href="solutions.html#ch8solutions9">(Click here to go to the solution.)</a></p>
<p><br></p>
<div class="exercise">
<p><span id="exr:ch8exc9f" class="exercise"><strong>Exercise 9.31  </strong></span>Fit a kNN classifier to the <code>seeds</code> data from Section <a href="eda.html#pca">4.9</a>, using <code>Variety</code> as the response variable and <code>Kernel_length</code> and <code>Compactness</code> as explanatory variables. Plot the resulting decision boundaries, as in Section <a href="mlchapter.html#decisionboundaries">9.1.8</a>. Do they seem reasonable to you?</p>
</div>
<p><a href="solutions.html#ch8solutions9f">(Click here to go to the solution.)</a></p>
</div>
</div>
<div id="tsforecast" class="section level2 hasAnchor" number="9.6">
<h2><span class="header-section-number">9.6</span> Forecasting time series<a href="mlchapter.html#tsforecast" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A time series, like those we studied in Section <a href="eda.html#tsplots">4.6</a>, is a series of observations sorted in time order. The goal of time series analysis is to model temporal patterns in data. This allows us to take correlations between observations into account (today’s stock prices are correlated to yesterday’s), to capture seasonal patterns (ice cream sales always increase during the summer), and to incorporate those into predictions, or forecasts, for the future. This section acts as a brief introduction to how this can be done.</p>
<div id="decomposition" class="section level3 hasAnchor" number="9.6.1">
<h3><span class="header-section-number">9.6.1</span> Decomposition<a href="mlchapter.html#decomposition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In Section <a href="eda.html#stlsec">4.6.5</a> we saw how time series can be <em>decomposed</em> into three components:</p>
<ul>
<li>A <em>seasonal</em> component, describing recurring seasonal patterns,</li>
<li>A <em>trend</em> component, describing a trend over time,</li>
<li>A <em>remainder</em> component, describing random variation.</li>
</ul>
<p>Let’s have a quick look at how to do this in R, using the <code>a10</code> data from <code>fpp2</code>:</p>
<div class="sourceCode" id="cb1039"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1039-1"><a href="mlchapter.html#cb1039-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(forecast)</span>
<span id="cb1039-2"><a href="mlchapter.html#cb1039-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb1039-3"><a href="mlchapter.html#cb1039-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(fpp2)</span>
<span id="cb1039-4"><a href="mlchapter.html#cb1039-4" aria-hidden="true" tabindex="-1"></a>?a10</span>
<span id="cb1039-5"><a href="mlchapter.html#cb1039-5" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(a10)</span></code></pre></div>
<p>The <code>stl</code> function uses repeated LOESS smoothing to decompose the series. The <code>s.window</code> parameter lets us set the length of the season in the data. We can set it to <code>"periodic"</code> to have <code>stl</code> find the periodicity of the data automatically:</p>
<div class="sourceCode" id="cb1040"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1040-1"><a href="mlchapter.html#cb1040-1" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(<span class="fu">stl</span>(a10, <span class="at">s.window =</span> <span class="st">&quot;periodic&quot;</span>))</span></code></pre></div>
<p>We can access the different parts of the decomposition as follows:</p>
<div class="sourceCode" id="cb1041"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1041-1"><a href="mlchapter.html#cb1041-1" aria-hidden="true" tabindex="-1"></a>a10_stl <span class="ot">&lt;-</span> <span class="fu">stl</span>(a10, <span class="at">s.window =</span> <span class="st">&quot;periodic&quot;</span>)</span>
<span id="cb1041-2"><a href="mlchapter.html#cb1041-2" aria-hidden="true" tabindex="-1"></a>a10_stl<span class="sc">$</span>time.series[,<span class="st">&quot;seasonal&quot;</span>]</span>
<span id="cb1041-3"><a href="mlchapter.html#cb1041-3" aria-hidden="true" tabindex="-1"></a>a10_stl<span class="sc">$</span>time.series[,<span class="st">&quot;trend&quot;</span>]</span>
<span id="cb1041-4"><a href="mlchapter.html#cb1041-4" aria-hidden="true" tabindex="-1"></a>a10_stl<span class="sc">$</span>time.series[,<span class="st">&quot;remainder&quot;</span>]</span></code></pre></div>
<p>When modelling time series data, we usually want to remove the seasonal component, as it makes the data structure too complicated. We can then add it back when we use the model for forecasting. We’ll see how to do that in the following sections.</p>
</div>
<div id="forecasting-using-arima-models" class="section level3 hasAnchor" number="9.6.2">
<h3><span class="header-section-number">9.6.2</span> Forecasting using ARIMA models<a href="mlchapter.html#forecasting-using-arima-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <code>forecast</code> package contains a large number of useful methods for fitting time series models. Among them is <code>auto.arima</code> which can be used to fit autoregressive integrated moving average (<em>ARIMA</em>) models to time series data. ARIMA models are a flexible class of models that can capture many different types of temporal correlations and patterns. <code>auto.arima</code> helps us select a model that seems appropriate based on historical data, using an in-sample criterion, a version of <span class="math inline">\(AIC\)</span>, for model selection.</p>
<p><code>stlm</code> can be used to fit a model after removing the seasonal component, and then automatically add it back again when using it for a forecast. The <code>modelfunction</code> argument lets us specify what model to fit. Let’s use <code>auto.arima</code> for model fitting through <code>stlm</code>:</p>
<div class="sourceCode" id="cb1042"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1042-1"><a href="mlchapter.html#cb1042-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(forecast)</span>
<span id="cb1042-2"><a href="mlchapter.html#cb1042-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(fpp2)</span>
<span id="cb1042-3"><a href="mlchapter.html#cb1042-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1042-4"><a href="mlchapter.html#cb1042-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model after removing the seasonal component:</span></span>
<span id="cb1042-5"><a href="mlchapter.html#cb1042-5" aria-hidden="true" tabindex="-1"></a>tsmod <span class="ot">&lt;-</span> <span class="fu">stlm</span>(a10, <span class="at">s.window =</span> <span class="st">&quot;periodic&quot;</span>, <span class="at">modelfunction =</span> auto.arima)</span></code></pre></div>
<p>For model diagnostics, we can use <code>checkresiduals</code> to check whether the residuals from the model look like white noise (i.e. look normal):</p>
<div class="sourceCode" id="cb1043"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1043-1"><a href="mlchapter.html#cb1043-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check model diagnostics:</span></span>
<span id="cb1043-2"><a href="mlchapter.html#cb1043-2" aria-hidden="true" tabindex="-1"></a><span class="fu">checkresiduals</span>(tsmod)</span></code></pre></div>
<p>In this case, the variance of the series seems to increase with time, which the model fails to capture. We therefore see more large residuals than what is expected under the model.</p>
<p>Nevertheless, let’s see how we can make a forecast for the next 24 months. The function for this is the aptly named <code>forecast</code>:</p>
<div class="sourceCode" id="cb1044"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1044-1"><a href="mlchapter.html#cb1044-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the forecast (with the seasonal component added back)</span></span>
<span id="cb1044-2"><a href="mlchapter.html#cb1044-2" aria-hidden="true" tabindex="-1"></a><span class="co"># for the next 24 months:</span></span>
<span id="cb1044-3"><a href="mlchapter.html#cb1044-3" aria-hidden="true" tabindex="-1"></a><span class="fu">forecast</span>(tsmod, <span class="at">h =</span> <span class="dv">24</span>)</span>
<span id="cb1044-4"><a href="mlchapter.html#cb1044-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1044-5"><a href="mlchapter.html#cb1044-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the forecast along with the original data:</span></span>
<span id="cb1044-6"><a href="mlchapter.html#cb1044-6" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(<span class="fu">forecast</span>(tsmod, <span class="at">h =</span> <span class="dv">24</span>))</span></code></pre></div>
<p>In addition to the forecasted curve, <code>forecast</code> also provides prediction intervals. By default, these are based on an asymptotic approximation. To obtain bootstrap prediction intervals instead, we can add <code>bootstrap = TRUE</code> to <code>forecast</code>:</p>
<div class="sourceCode" id="cb1045"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1045-1"><a href="mlchapter.html#cb1045-1" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(<span class="fu">forecast</span>(tsmod, <span class="at">h =</span> <span class="dv">24</span>, <span class="at">bootstrap =</span> <span class="cn">TRUE</span>))</span></code></pre></div>
<p>The <code>forecast</code> package is designed to work well with pipes. To fit a model using <code>stlm</code> and <code>auto.arima</code> and then plot the forecast, we could have used:</p>
<div class="sourceCode" id="cb1046"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1046-1"><a href="mlchapter.html#cb1046-1" aria-hidden="true" tabindex="-1"></a>a10 <span class="sc">%&gt;%</span> <span class="fu">stlm</span>(<span class="at">s.window =</span> <span class="st">&quot;periodic&quot;</span>, <span class="at">modelfunction =</span> auto.arima) <span class="sc">%&gt;%</span> </span>
<span id="cb1046-2"><a href="mlchapter.html#cb1046-2" aria-hidden="true" tabindex="-1"></a>        <span class="fu">forecast</span>(<span class="at">h =</span> <span class="dv">24</span>, <span class="at">bootstrap =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span> <span class="fu">autoplot</span>()</span></code></pre></div>
<p>It is also possible to incorporate seasonal effects into ARIMA models by adding seasonal terms to the model. <code>auto.arima</code> will do this for us if we apply it directly to the data:</p>
<div class="sourceCode" id="cb1047"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1047-1"><a href="mlchapter.html#cb1047-1" aria-hidden="true" tabindex="-1"></a>a10 <span class="sc">%&gt;%</span> <span class="fu">auto.arima</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb1047-2"><a href="mlchapter.html#cb1047-2" aria-hidden="true" tabindex="-1"></a>        <span class="fu">forecast</span>(<span class="at">h =</span> <span class="dv">24</span>, <span class="at">bootstrap =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span> <span class="fu">autoplot</span>()</span></code></pre></div>
<p>For this data, the forecasts from the two approaches are very similar.</p>
<p>In Section <a href="mlchapter.html#modellingchallenges">9.3</a> we mentioned that a common reason for predictive models failing in practical applications is that many processes are non-stationary, so that their patterns change over time. ARIMA model are designed to handle some types of non-stationary, which can make them particularly useful for modelling such processes.</p>
<p><span class="math display">\[\sim\]</span></p>
<div class="exercise">
<p><span id="exr:ch8exc13" class="exercise"><strong>Exercise 9.32  </strong></span>Return to the <code>writing</code> dataset from the <code>fma</code> package, that we studied in Exercise <a href="eda.html#exr:ch4exc7">4.15</a>. Remove the seasonal component. Fit an ARIMA model to the data and use it plot a forecast for the next three years, with the seasonal component added back and with bootstrap prediction intervals.</p>
</div>
<p><a href="solutions.html#ch8solutions13">(Click here to go to the solution.)</a></p>
</div>
</div>
<div id="deploying-models" class="section level2 hasAnchor" number="9.7">
<h2><span class="header-section-number">9.7</span> Deploying models<a href="mlchapter.html#deploying-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The process of making a prediction model available to other users or systems, for instance by running them on a server, is known as <em>deployment</em>. In addition to the need for continuous model evaluation, mentioned in Section <a href="mlchapter.html#overfittingtestset">9.3.6</a>, you will also need to check that your R code works as intended in the environment in which you deploy your model. For instance, if you developed your model using R 4.1 and then run it on a server running R 3.6 with out-of-date versions of the packages you used, there is a risk that some of the functions that you use behave differently from what you expected. Maybe something that should be a <code>factor</code> variable becomes a <code>character</code> variable, which breaks that part of your code where you use <code>levels</code>. A lot of the time, small changes are enough to make the code work in the new environment (add a line that converts the variable to a <code>factor</code>), but sometimes large changes can be needed. Likewise, you must check that the model still works after the software is updated on the server.</p>
<div id="creating-apis-with-plumber" class="section level3 hasAnchor" number="9.7.1">
<h3><span class="header-section-number">9.7.1</span> Creating APIs with <code>plumber</code><a href="mlchapter.html#creating-apis-with-plumber" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>An Application Programming Interface (API) is an interface that lets other systems access your R code - which is exactly what you want when you’re ready to deploy your model. By using the <code>plumber</code> package to create an API (or a REST API, to be more specific), you can let other systems (a web page, a Java script, a Python script, and so on) access your model. Those systems can call your model, sending some input, and then receive its output in different formats, e.g. a JSON list, a csv file, or an image.</p>
<p>We’ll illustrate how this works with a simple example. First, let’s install <code>plumber</code>:</p>
<div class="sourceCode" id="cb1048"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1048-1"><a href="mlchapter.html#cb1048-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;plumber&quot;</span>)</span></code></pre></div>
<p>Next, assume that we’ve fitted a model (we’ll use the linear regression model for <code>mtcars</code> that we’ve used several times before). We can use this model to make predictions:</p>
<div class="sourceCode" id="cb1049"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1049-1"><a href="mlchapter.html#cb1049-1" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> hp <span class="sc">+</span> wt, <span class="at">data =</span> mtcars)</span>
<span id="cb1049-2"><a href="mlchapter.html#cb1049-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1049-3"><a href="mlchapter.html#cb1049-3" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(m, <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">hp =</span> <span class="dv">150</span>, <span class="at">wt =</span> <span class="dv">2</span>))</span></code></pre></div>
<p>We would like to make these predictions available to other systems. That is, we’d like to allow other systems to send values of <code>hp</code> and <code>wt</code> to our model, and get predictions in return. To do so, we start by writing a function for the predictions:</p>
<div class="sourceCode" id="cb1050"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1050-1"><a href="mlchapter.html#cb1050-1" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> hp <span class="sc">+</span> wt, <span class="at">data =</span> mtcars)</span>
<span id="cb1050-2"><a href="mlchapter.html#cb1050-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1050-3"><a href="mlchapter.html#cb1050-3" aria-hidden="true" tabindex="-1"></a>predictions <span class="ot">&lt;-</span> <span class="cf">function</span>(hp, wt)</span>
<span id="cb1050-4"><a href="mlchapter.html#cb1050-4" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb1050-5"><a href="mlchapter.html#cb1050-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">predict</span>(m, <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">hp =</span> hp, <span class="at">wt =</span> wt))</span>
<span id="cb1050-6"><a href="mlchapter.html#cb1050-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1050-7"><a href="mlchapter.html#cb1050-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1050-8"><a href="mlchapter.html#cb1050-8" aria-hidden="true" tabindex="-1"></a><span class="fu">predictions</span>(<span class="dv">150</span>, <span class="dv">2</span>)</span></code></pre></div>
<p>To make this accessible to other systems, we save this function in a script called <code>mtcarsAPI.R</code> (make sure to save it in your working directory), which looks as follows:</p>
<div class="sourceCode" id="cb1051"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1051-1"><a href="mlchapter.html#cb1051-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model:</span></span>
<span id="cb1051-2"><a href="mlchapter.html#cb1051-2" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> hp <span class="sc">+</span> wt, <span class="at">data =</span> mtcars)</span>
<span id="cb1051-3"><a href="mlchapter.html#cb1051-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1051-4"><a href="mlchapter.html#cb1051-4" aria-hidden="true" tabindex="-1"></a><span class="co">#* Return the prediction:</span></span>
<span id="cb1051-5"><a href="mlchapter.html#cb1051-5" aria-hidden="true" tabindex="-1"></a><span class="co">#* @param hp</span></span>
<span id="cb1051-6"><a href="mlchapter.html#cb1051-6" aria-hidden="true" tabindex="-1"></a><span class="co">#* @param wt</span></span>
<span id="cb1051-7"><a href="mlchapter.html#cb1051-7" aria-hidden="true" tabindex="-1"></a><span class="co">#* @post /predictions</span></span>
<span id="cb1051-8"><a href="mlchapter.html#cb1051-8" aria-hidden="true" tabindex="-1"></a><span class="cf">function</span>(hp, wt)</span>
<span id="cb1051-9"><a href="mlchapter.html#cb1051-9" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb1051-10"><a href="mlchapter.html#cb1051-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">predict</span>(m, <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">hp =</span> <span class="fu">as.numeric</span>(hp),</span>
<span id="cb1051-11"><a href="mlchapter.html#cb1051-11" aria-hidden="true" tabindex="-1"></a>                                    <span class="at">wt =</span> <span class="fu">as.numeric</span>(wt)))</span>
<span id="cb1051-12"><a href="mlchapter.html#cb1051-12" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>The only changes that we have made are some additional special comments (<code>#*</code>), which specify what input is expected (parameters <code>hp</code> and <code>wt</code>) and that the function is called <code>predictions</code>. <code>plumber</code> uses this information to create the API. The functions made available in an API are referred to as <em>endpoints</em>.</p>
<p>To make the function available to other systems, we run <code>pr</code> as follows:</p>
<div class="sourceCode" id="cb1052"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1052-1"><a href="mlchapter.html#cb1052-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(plumber)</span>
<span id="cb1052-2"><a href="mlchapter.html#cb1052-2" aria-hidden="true" tabindex="-1"></a><span class="fu">pr</span>(<span class="st">&quot;mtcarsAPI.R&quot;</span>) <span class="sc">%&gt;%</span> <span class="fu">pr_run</span>(<span class="at">port =</span> <span class="dv">8000</span>)</span></code></pre></div>
<p>The function will now be available on port <code>8000</code> of your computer. To access it, you can open your browser and go to the following URL:</p>
<ul>
<li><code>http://localhost:8000/predictions?hp=150&amp;wt=2</code></li>
</ul>
<p>Try changing the values of <code>hp</code> and <code>wt</code> and see how the returned value changes.</p>
<p>That’s it! As long as you leave your R session running with <code>plumber</code>, other systems will be able to access the model using the URL. Typically, you would run this on a server and not on your personal computer.</p>
</div>
<div id="different-types-of-output" class="section level3 hasAnchor" number="9.7.2">
<h3><span class="header-section-number">9.7.2</span> Different types of output<a href="mlchapter.html#different-types-of-output" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>You won’t always want to return a number. Maybe you want to use R to create a plot, send a file, or print some text. Here is an example of an R script, which we’ll save as <code>exampleAPI.R</code>, that returns different types of output - an image, a text, and a downloadable csv file:</p>
<div class="sourceCode" id="cb1053"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1053-1"><a href="mlchapter.html#cb1053-1" aria-hidden="true" tabindex="-1"></a><span class="co">#* Plot some random numbers</span></span>
<span id="cb1053-2"><a href="mlchapter.html#cb1053-2" aria-hidden="true" tabindex="-1"></a><span class="co">#* param n The number of points to plot</span></span>
<span id="cb1053-3"><a href="mlchapter.html#cb1053-3" aria-hidden="true" tabindex="-1"></a><span class="co">#* @serializer png</span></span>
<span id="cb1053-4"><a href="mlchapter.html#cb1053-4" aria-hidden="true" tabindex="-1"></a><span class="co">#* @get /plot</span></span>
<span id="cb1053-5"><a href="mlchapter.html#cb1053-5" aria-hidden="true" tabindex="-1"></a><span class="cf">function</span>(<span class="at">n =</span> <span class="dv">15</span>) {</span>
<span id="cb1053-6"><a href="mlchapter.html#cb1053-6" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="fu">as.numeric</span>(n))</span>
<span id="cb1053-7"><a href="mlchapter.html#cb1053-7" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="fu">as.numeric</span>(n))</span>
<span id="cb1053-8"><a href="mlchapter.html#cb1053-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(x, y, <span class="at">col =</span> <span class="dv">2</span>, <span class="at">pch =</span> <span class="dv">16</span>)</span>
<span id="cb1053-9"><a href="mlchapter.html#cb1053-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1053-10"><a href="mlchapter.html#cb1053-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1053-11"><a href="mlchapter.html#cb1053-11" aria-hidden="true" tabindex="-1"></a><span class="co">#* Print a message</span></span>
<span id="cb1053-12"><a href="mlchapter.html#cb1053-12" aria-hidden="true" tabindex="-1"></a><span class="co">#* @param name Your name</span></span>
<span id="cb1053-13"><a href="mlchapter.html#cb1053-13" aria-hidden="true" tabindex="-1"></a><span class="co">#* @get /message</span></span>
<span id="cb1053-14"><a href="mlchapter.html#cb1053-14" aria-hidden="true" tabindex="-1"></a><span class="cf">function</span>(<span class="at">name =</span> <span class="st">&quot;&quot;</span>) {</span>
<span id="cb1053-15"><a href="mlchapter.html#cb1053-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">message =</span> <span class="fu">paste</span>(<span class="st">&quot;Hello&quot;</span>, name, <span class="st">&quot;- I&#39;m happy to see you!&quot;</span>))</span>
<span id="cb1053-16"><a href="mlchapter.html#cb1053-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1053-17"><a href="mlchapter.html#cb1053-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1053-18"><a href="mlchapter.html#cb1053-18" aria-hidden="true" tabindex="-1"></a><span class="co">#* Download the mtcars data as a csv file</span></span>
<span id="cb1053-19"><a href="mlchapter.html#cb1053-19" aria-hidden="true" tabindex="-1"></a><span class="co">#* @serializer csv</span></span>
<span id="cb1053-20"><a href="mlchapter.html#cb1053-20" aria-hidden="true" tabindex="-1"></a><span class="co">#* @get /download</span></span>
<span id="cb1053-21"><a href="mlchapter.html#cb1053-21" aria-hidden="true" tabindex="-1"></a><span class="cf">function</span>() {</span>
<span id="cb1053-22"><a href="mlchapter.html#cb1053-22" aria-hidden="true" tabindex="-1"></a>  mtcars</span>
<span id="cb1053-23"><a href="mlchapter.html#cb1053-23" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>After you’ve saved the file in your working directory, run the following to create the API:</p>
<div class="sourceCode" id="cb1054"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1054-1"><a href="mlchapter.html#cb1054-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(plumber)</span>
<span id="cb1054-2"><a href="mlchapter.html#cb1054-2" aria-hidden="true" tabindex="-1"></a><span class="fu">pr</span>(<span class="st">&quot;mtcarsAPI.R&quot;</span>) <span class="sc">%&gt;%</span> <span class="fu">pr_run</span>(<span class="at">port =</span> <span class="dv">8000</span>)</span></code></pre></div>
<p>You can now try the different endpoints:</p>
<ul>
<li><code>http://localhost:8000/plot</code></li>
<li><code>http://localhost:8000/plot?n=50</code></li>
<li><code>http://localhost:8000/message?name=Oskar</code></li>
<li><code>http://localhost:8000/download</code></li>
</ul>
<p>We’ve only scratched the surface of <code>plumber</code>:s capabilities here. A more thorough guide can be found on the official <code>plumber</code> web page at <a href="https://www.rplumber.io/" class="uri">https://www.rplumber.io/</a></p>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="58">
<li id="fn58"><p>Many, but not all, classifiers also output predicted class probabilities. The distinction between regression models and classifiers is blurry at best.<a href="mlchapter.html#fnref58" class="footnote-back">↩︎</a></p></li>
<li id="fn59"><p>If your CPU has 3 or fewer cores, you should lower this number.<a href="mlchapter.html#fnref59" class="footnote-back">↩︎</a></p></li>
<li id="fn60"><p>Parameters like <span class="math inline">\(\lambda\)</span> that describe “settings” used for the method rather than parts of the model, are often referred to as <em>hyperparameters</em>.<a href="mlchapter.html#fnref60" class="footnote-back">↩︎</a></p></li>
<li id="fn61"><p>The name <code>rpart</code> may seem cryptic: it is an abbreviation for Recursive Partitioning and Regression Trees, which is a type of decision trees.<a href="mlchapter.html#fnref61" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="advancedchapter.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
